{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kw4t2EwxhAxQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self):    \n",
        "        # input \n",
        "        self.num_classes = 10\n",
        "        # training \n",
        "        self.batch_size = 128\n",
        "        self.epochs = 200\n",
        "        self.start_epoch = 0\n",
        "        self.momentum = 0.9\n",
        "        self.lr = 1e-1\n",
        "        self.weight_decay = 5e-4\n",
        "        self.label_smoothing = 0\n",
        "        self.model_name = 'resnet56_test2'\n",
        "\n",
        "        self.gpu = True\n",
        "        self.log_dir = '' + self.model_name\n",
        "config = Config()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5hyKMNef2aJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, config.num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o_Ck3oOHhrqw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "    \n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        \n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses += loss.item()\n",
        "        accs += prec1.item() \n",
        "        \n",
        "    losses /= len(train_loader)\n",
        "    accs /= len(train_loader)\n",
        "    print('[Epoch {epoch}] Average Loss : {loss:.3f}, Average Accuracy : {acc:.3f}'\n",
        "          .format(epoch = epoch , loss=losses, acc=accs))\n",
        "            \n",
        "    writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", accs, epoch)\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, writer):\n",
        "\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "    \n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses += loss.item()\n",
        "            accs += prec1.item()\n",
        "        \n",
        "        losses /= len(val_loader)\n",
        "        accs /= len(val_loader)\n",
        "        \n",
        "        writer.add_scalar(\"Loss/val\", losses, epoch)\n",
        "        writer.add_scalar(\"Accuracy/val\", accs, epoch)\n",
        "    \n",
        "    return accs\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0522ec8260b14df091e86856a3679cfb",
            "c9a9077c5af4478da373747a46ef0db7",
            "08eabf0bd60b49cea0230de641aa7bd9",
            "75ea2b9364524b9fae61c74a95b355bf",
            "348deaf4ccf7468fb4cf9fd832cb0529",
            "a44007ca099643ba8cbf81e2a6d693d1",
            "ae8b1c2903124bc488a06449b4eedf39",
            "51c6503da647432a80c00cad7eb59277",
            "f10e54310c494b0bb9cd7458a7d4a9e4",
            "8f303d9f20e6454cabbcfb1583b3ebfc",
            "0cae142e8de843d0a3193013084f5031"
          ]
        },
        "id": "jaRxjlK-sg4L",
        "outputId": "6d675cbb-a667-4fbd-914d-db129970c9cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total parameters :  853018\n",
            "Files already downloaded and verified\n",
            "current lr 1.00000e-01\n",
            "Epoch: [0][0/391]\tTime 9.480 (9.480)\tData 4.945 (4.945)\tLoss 5.6147 (5.6147)\tPrec@1 11.719 (11.719)\n",
            "Epoch: [0][50/391]\tTime 0.049 (0.234)\tData 0.000 (0.097)\tLoss 2.3709 (3.6014)\tPrec@1 7.031 (10.585)\n",
            "Epoch: [0][100/391]\tTime 0.046 (0.142)\tData 0.001 (0.049)\tLoss 2.2756 (2.9691)\tPrec@1 14.844 (11.255)\n",
            "Epoch: [0][150/391]\tTime 0.048 (0.112)\tData 0.000 (0.033)\tLoss 2.2613 (2.7420)\tPrec@1 13.281 (11.905)\n",
            "Epoch: [0][200/391]\tTime 0.048 (0.096)\tData 0.000 (0.025)\tLoss 2.2374 (2.6176)\tPrec@1 16.406 (12.675)\n",
            "Epoch: [0][250/391]\tTime 0.048 (0.087)\tData 0.000 (0.020)\tLoss 2.0639 (2.5094)\tPrec@1 24.219 (13.969)\n",
            "Epoch: [0][300/391]\tTime 0.049 (0.080)\tData 0.000 (0.017)\tLoss 1.7894 (2.4207)\tPrec@1 28.906 (15.480)\n",
            "Epoch: [0][350/391]\tTime 0.050 (0.076)\tData 0.000 (0.014)\tLoss 1.8703 (2.3534)\tPrec@1 27.344 (16.676)\n",
            "epoch 0 training time consumed: 28.85s\n",
            "Test: [0/79]\tTime 3.862 (3.862)\tLoss 1.9695 (1.9695)\tPrec@1 26.562 (26.562)\n",
            "Test: [50/79]\tTime 0.013 (0.093)\tLoss 1.8622 (1.9928)\tPrec@1 32.031 (28.110)\n",
            " * Prec@1 27.930\n",
            "current lr 9.99938e-02\n",
            "Epoch: [1][0/391]\tTime 5.249 (5.249)\tData 5.015 (5.015)\tLoss 1.8276 (1.8276)\tPrec@1 26.562 (26.562)\n",
            "Epoch: [1][50/391]\tTime 0.048 (0.153)\tData 0.000 (0.098)\tLoss 1.8750 (1.8685)\tPrec@1 28.906 (27.727)\n",
            "Epoch: [1][100/391]\tTime 0.050 (0.102)\tData 0.000 (0.050)\tLoss 1.8592 (1.8373)\tPrec@1 30.469 (29.007)\n",
            "Epoch: [1][150/391]\tTime 0.050 (0.084)\tData 0.000 (0.033)\tLoss 1.7015 (1.8096)\tPrec@1 33.594 (30.303)\n",
            "Epoch: [1][200/391]\tTime 0.049 (0.076)\tData 0.000 (0.025)\tLoss 1.6970 (1.7843)\tPrec@1 32.031 (31.507)\n",
            "Epoch: [1][250/391]\tTime 0.049 (0.071)\tData 0.000 (0.020)\tLoss 1.5908 (1.7646)\tPrec@1 38.281 (32.498)\n",
            "Epoch: [1][300/391]\tTime 0.049 (0.067)\tData 0.000 (0.017)\tLoss 1.5299 (1.7435)\tPrec@1 50.781 (33.467)\n",
            "Epoch: [1][350/391]\tTime 0.049 (0.065)\tData 0.000 (0.014)\tLoss 1.4711 (1.7211)\tPrec@1 46.875 (34.509)\n",
            "epoch 1 training time consumed: 24.80s\n",
            "Test: [0/79]\tTime 3.831 (3.831)\tLoss 1.7814 (1.7814)\tPrec@1 37.500 (37.500)\n",
            "Test: [50/79]\tTime 0.015 (0.091)\tLoss 1.7196 (1.7764)\tPrec@1 37.500 (36.259)\n",
            " * Prec@1 36.620\n",
            "current lr 9.99753e-02\n",
            "Epoch: [2][0/391]\tTime 5.154 (5.154)\tData 4.919 (4.919)\tLoss 1.4480 (1.4480)\tPrec@1 50.000 (50.000)\n",
            "Epoch: [2][50/391]\tTime 0.050 (0.150)\tData 0.001 (0.097)\tLoss 1.4734 (1.5041)\tPrec@1 42.188 (44.010)\n",
            "Epoch: [2][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 1.4744 (1.4653)\tPrec@1 46.094 (45.753)\n",
            "Epoch: [2][150/391]\tTime 0.049 (0.083)\tData 0.000 (0.033)\tLoss 1.1305 (1.4422)\tPrec@1 61.719 (46.885)\n",
            "Epoch: [2][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 1.3978 (1.4152)\tPrec@1 45.312 (47.959)\n",
            "Epoch: [2][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 1.2737 (1.3924)\tPrec@1 50.000 (48.957)\n",
            "Epoch: [2][300/391]\tTime 0.049 (0.066)\tData 0.000 (0.017)\tLoss 1.1525 (1.3719)\tPrec@1 60.156 (49.836)\n",
            "Epoch: [2][350/391]\tTime 0.048 (0.064)\tData 0.000 (0.014)\tLoss 1.1631 (1.3569)\tPrec@1 57.031 (50.503)\n",
            "epoch 2 training time consumed: 24.59s\n",
            "Test: [0/79]\tTime 3.926 (3.926)\tLoss 1.4948 (1.4948)\tPrec@1 46.094 (46.094)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 1.5532 (1.4766)\tPrec@1 54.688 (50.092)\n",
            " * Prec@1 49.550\n",
            "current lr 9.99445e-02\n",
            "Epoch: [3][0/391]\tTime 5.205 (5.205)\tData 4.970 (4.970)\tLoss 1.1826 (1.1826)\tPrec@1 56.250 (56.250)\n",
            "Epoch: [3][50/391]\tTime 0.049 (0.152)\tData 0.000 (0.098)\tLoss 1.0792 (1.1489)\tPrec@1 62.500 (58.517)\n",
            "Epoch: [3][100/391]\tTime 0.049 (0.101)\tData 0.000 (0.049)\tLoss 1.1480 (1.1387)\tPrec@1 60.938 (59.197)\n",
            "Epoch: [3][150/391]\tTime 0.049 (0.084)\tData 0.000 (0.033)\tLoss 1.0503 (1.1223)\tPrec@1 63.281 (59.701)\n",
            "Epoch: [3][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 1.0733 (1.1066)\tPrec@1 61.719 (60.261)\n",
            "Epoch: [3][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 1.0941 (1.0924)\tPrec@1 64.062 (60.853)\n",
            "Epoch: [3][300/391]\tTime 0.050 (0.067)\tData 0.000 (0.017)\tLoss 0.8922 (1.0795)\tPrec@1 69.531 (61.335)\n",
            "Epoch: [3][350/391]\tTime 0.049 (0.064)\tData 0.001 (0.014)\tLoss 1.0163 (1.0659)\tPrec@1 65.625 (61.910)\n",
            "epoch 3 training time consumed: 24.74s\n",
            "Test: [0/79]\tTime 3.850 (3.850)\tLoss 1.2183 (1.2183)\tPrec@1 59.375 (59.375)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 1.1719 (1.2593)\tPrec@1 67.969 (59.819)\n",
            " * Prec@1 59.910\n",
            "current lr 9.99013e-02\n",
            "Epoch: [4][0/391]\tTime 5.153 (5.153)\tData 4.944 (4.944)\tLoss 1.0085 (1.0085)\tPrec@1 63.281 (63.281)\n",
            "Epoch: [4][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.097)\tLoss 1.0328 (0.9432)\tPrec@1 61.719 (66.636)\n",
            "Epoch: [4][100/391]\tTime 0.048 (0.100)\tData 0.000 (0.049)\tLoss 0.9369 (0.9253)\tPrec@1 68.750 (67.435)\n",
            "Epoch: [4][150/391]\tTime 0.049 (0.083)\tData 0.000 (0.033)\tLoss 0.8680 (0.9175)\tPrec@1 69.531 (67.870)\n",
            "Epoch: [4][200/391]\tTime 0.048 (0.075)\tData 0.000 (0.025)\tLoss 0.8567 (0.9119)\tPrec@1 71.094 (68.070)\n",
            "Epoch: [4][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 0.9982 (0.9017)\tPrec@1 67.188 (68.582)\n",
            "Epoch: [4][300/391]\tTime 0.050 (0.066)\tData 0.001 (0.017)\tLoss 0.8405 (0.8974)\tPrec@1 69.531 (68.675)\n",
            "Epoch: [4][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 1.0140 (0.8893)\tPrec@1 63.281 (68.897)\n",
            "epoch 4 training time consumed: 24.63s\n",
            "Test: [0/79]\tTime 3.847 (3.847)\tLoss 0.9615 (0.9615)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.014 (0.093)\tLoss 1.0841 (1.0014)\tPrec@1 64.844 (67.984)\n",
            " * Prec@1 67.220\n",
            "current lr 9.98459e-02\n",
            "Epoch: [5][0/391]\tTime 5.191 (5.191)\tData 4.985 (4.985)\tLoss 0.8157 (0.8157)\tPrec@1 71.094 (71.094)\n",
            "Epoch: [5][50/391]\tTime 0.051 (0.151)\tData 0.000 (0.098)\tLoss 0.8461 (0.8229)\tPrec@1 71.875 (71.584)\n",
            "Epoch: [5][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.050)\tLoss 0.7311 (0.8143)\tPrec@1 75.781 (71.805)\n",
            "Epoch: [5][150/391]\tTime 0.049 (0.083)\tData 0.000 (0.033)\tLoss 0.7775 (0.8096)\tPrec@1 75.781 (71.740)\n",
            "Epoch: [5][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.8156 (0.8037)\tPrec@1 66.406 (72.042)\n",
            "Epoch: [5][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.7866 (0.7997)\tPrec@1 70.312 (72.249)\n",
            "Epoch: [5][300/391]\tTime 0.050 (0.066)\tData 0.000 (0.017)\tLoss 0.8041 (0.7956)\tPrec@1 76.562 (72.376)\n",
            "Epoch: [5][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.8752 (0.7928)\tPrec@1 66.406 (72.438)\n",
            "epoch 5 training time consumed: 24.60s\n",
            "Test: [0/79]\tTime 3.827 (3.827)\tLoss 0.8641 (0.8641)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 1.0393 (1.0359)\tPrec@1 65.625 (65.135)\n",
            " * Prec@1 65.260\n",
            "current lr 9.97781e-02\n",
            "Epoch: [6][0/391]\tTime 5.161 (5.161)\tData 4.938 (4.938)\tLoss 0.6782 (0.6782)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [6][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.097)\tLoss 0.7680 (0.7373)\tPrec@1 75.000 (74.050)\n",
            "Epoch: [6][100/391]\tTime 0.050 (0.100)\tData 0.000 (0.049)\tLoss 0.5817 (0.7237)\tPrec@1 79.688 (74.923)\n",
            "Epoch: [6][150/391]\tTime 0.048 (0.083)\tData 0.000 (0.033)\tLoss 0.8388 (0.7396)\tPrec@1 68.750 (74.327)\n",
            "Epoch: [6][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.7129 (0.7424)\tPrec@1 76.562 (74.250)\n",
            "Epoch: [6][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.8061 (0.7349)\tPrec@1 71.875 (74.474)\n",
            "Epoch: [6][300/391]\tTime 0.050 (0.066)\tData 0.001 (0.017)\tLoss 0.6965 (0.7383)\tPrec@1 75.000 (74.450)\n",
            "Epoch: [6][350/391]\tTime 0.049 (0.064)\tData 0.001 (0.014)\tLoss 0.7701 (0.7345)\tPrec@1 72.656 (74.506)\n",
            "epoch 6 training time consumed: 24.60s\n",
            "Test: [0/79]\tTime 3.829 (3.829)\tLoss 1.2417 (1.2417)\tPrec@1 61.719 (61.719)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 1.5617 (1.3531)\tPrec@1 51.562 (58.563)\n",
            " * Prec@1 58.640\n",
            "current lr 9.96980e-02\n",
            "Epoch: [7][0/391]\tTime 5.160 (5.160)\tData 4.915 (4.915)\tLoss 0.6448 (0.6448)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [7][50/391]\tTime 0.049 (0.149)\tData 0.001 (0.097)\tLoss 0.7032 (0.6499)\tPrec@1 76.562 (78.094)\n",
            "Epoch: [7][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.6582 (0.6726)\tPrec@1 78.906 (76.833)\n",
            "Epoch: [7][150/391]\tTime 0.051 (0.083)\tData 0.001 (0.033)\tLoss 0.7837 (0.6705)\tPrec@1 71.875 (76.744)\n",
            "Epoch: [7][200/391]\tTime 0.050 (0.075)\tData 0.000 (0.025)\tLoss 0.6298 (0.6719)\tPrec@1 79.688 (76.683)\n",
            "Epoch: [7][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 0.6616 (0.6719)\tPrec@1 73.438 (76.578)\n",
            "Epoch: [7][300/391]\tTime 0.048 (0.066)\tData 0.000 (0.017)\tLoss 0.7034 (0.6695)\tPrec@1 75.781 (76.674)\n",
            "Epoch: [7][350/391]\tTime 0.047 (0.065)\tData 0.000 (0.014)\tLoss 0.7922 (0.6682)\tPrec@1 75.781 (76.665)\n",
            "epoch 7 training time consumed: 25.12s\n",
            "Test: [0/79]\tTime 4.351 (4.351)\tLoss 0.8078 (0.8078)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.017 (0.100)\tLoss 0.9322 (0.8020)\tPrec@1 67.188 (72.947)\n",
            " * Prec@1 72.850\n",
            "current lr 9.96057e-02\n",
            "Epoch: [8][0/391]\tTime 7.364 (7.364)\tData 7.157 (7.157)\tLoss 0.5849 (0.5849)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [8][50/391]\tTime 0.048 (0.193)\tData 0.000 (0.140)\tLoss 0.5084 (0.6368)\tPrec@1 82.812 (77.987)\n",
            "Epoch: [8][100/391]\tTime 0.049 (0.122)\tData 0.000 (0.071)\tLoss 0.7259 (0.6374)\tPrec@1 72.656 (77.854)\n",
            "Epoch: [8][150/391]\tTime 0.049 (0.098)\tData 0.000 (0.048)\tLoss 0.8316 (0.6433)\tPrec@1 75.000 (77.784)\n",
            "Epoch: [8][200/391]\tTime 0.049 (0.086)\tData 0.000 (0.036)\tLoss 0.7441 (0.6461)\tPrec@1 72.656 (77.682)\n",
            "Epoch: [8][250/391]\tTime 0.051 (0.078)\tData 0.000 (0.029)\tLoss 0.5214 (0.6383)\tPrec@1 82.812 (77.938)\n",
            "Epoch: [8][300/391]\tTime 0.048 (0.074)\tData 0.000 (0.024)\tLoss 0.6503 (0.6382)\tPrec@1 76.562 (77.904)\n",
            "Epoch: [8][350/391]\tTime 0.049 (0.070)\tData 0.000 (0.021)\tLoss 0.6498 (0.6342)\tPrec@1 80.469 (78.080)\n",
            "epoch 8 training time consumed: 26.73s\n",
            "Test: [0/79]\tTime 3.926 (3.926)\tLoss 0.7351 (0.7351)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.014 (0.095)\tLoss 1.1291 (0.9297)\tPrec@1 62.500 (69.455)\n",
            " * Prec@1 69.260\n",
            "current lr 9.95012e-02\n",
            "Epoch: [9][0/391]\tTime 5.359 (5.359)\tData 5.086 (5.086)\tLoss 0.7602 (0.7602)\tPrec@1 73.438 (73.438)\n",
            "Epoch: [9][50/391]\tTime 0.050 (0.154)\tData 0.001 (0.100)\tLoss 0.6103 (0.6234)\tPrec@1 80.469 (78.554)\n",
            "Epoch: [9][100/391]\tTime 0.048 (0.102)\tData 0.000 (0.051)\tLoss 0.6254 (0.6056)\tPrec@1 75.000 (79.192)\n",
            "Epoch: [9][150/391]\tTime 0.049 (0.085)\tData 0.000 (0.034)\tLoss 0.8337 (0.6011)\tPrec@1 68.750 (79.227)\n",
            "Epoch: [9][200/391]\tTime 0.049 (0.076)\tData 0.000 (0.026)\tLoss 0.6083 (0.6113)\tPrec@1 78.125 (78.805)\n",
            "Epoch: [9][250/391]\tTime 0.049 (0.071)\tData 0.000 (0.020)\tLoss 0.4713 (0.6076)\tPrec@1 82.812 (79.000)\n",
            "Epoch: [9][300/391]\tTime 0.049 (0.067)\tData 0.000 (0.017)\tLoss 0.7450 (0.6085)\tPrec@1 68.750 (78.885)\n",
            "Epoch: [9][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.015)\tLoss 0.6505 (0.6134)\tPrec@1 80.469 (78.782)\n",
            "epoch 9 training time consumed: 24.74s\n",
            "Test: [0/79]\tTime 3.865 (3.865)\tLoss 0.6222 (0.6222)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.015 (0.093)\tLoss 0.8057 (0.7136)\tPrec@1 75.000 (75.567)\n",
            " * Prec@1 75.510\n",
            "current lr 9.93844e-02\n",
            "Epoch: [10][0/391]\tTime 5.348 (5.348)\tData 5.132 (5.132)\tLoss 0.5894 (0.5894)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [10][50/391]\tTime 0.062 (0.153)\tData 0.000 (0.101)\tLoss 0.7210 (0.5956)\tPrec@1 74.219 (80.040)\n",
            "Epoch: [10][100/391]\tTime 0.049 (0.101)\tData 0.001 (0.051)\tLoss 0.6367 (0.5926)\tPrec@1 78.125 (79.896)\n",
            "Epoch: [10][150/391]\tTime 0.047 (0.083)\tData 0.000 (0.034)\tLoss 0.5991 (0.6004)\tPrec@1 82.031 (79.506)\n",
            "Epoch: [10][200/391]\tTime 0.063 (0.075)\tData 0.000 (0.026)\tLoss 0.6903 (0.5983)\tPrec@1 78.906 (79.551)\n",
            "Epoch: [10][250/391]\tTime 0.047 (0.069)\tData 0.000 (0.021)\tLoss 0.5379 (0.5938)\tPrec@1 82.031 (79.619)\n",
            "Epoch: [10][300/391]\tTime 0.059 (0.066)\tData 0.000 (0.017)\tLoss 0.6193 (0.5937)\tPrec@1 78.125 (79.615)\n",
            "Epoch: [10][350/391]\tTime 0.038 (0.063)\tData 0.000 (0.015)\tLoss 0.5775 (0.5925)\tPrec@1 81.250 (79.645)\n",
            "epoch 10 training time consumed: 24.21s\n",
            "Test: [0/79]\tTime 3.736 (3.736)\tLoss 0.7374 (0.7374)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.016 (0.090)\tLoss 0.8024 (0.7541)\tPrec@1 75.000 (74.954)\n",
            " * Prec@1 74.780\n",
            "current lr 9.92555e-02\n",
            "Epoch: [11][0/391]\tTime 5.067 (5.067)\tData 4.851 (4.851)\tLoss 0.6070 (0.6070)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [11][50/391]\tTime 0.062 (0.148)\tData 0.000 (0.095)\tLoss 0.7207 (0.5785)\tPrec@1 76.562 (80.270)\n",
            "Epoch: [11][100/391]\tTime 0.054 (0.099)\tData 0.000 (0.048)\tLoss 0.5340 (0.5758)\tPrec@1 80.469 (80.167)\n",
            "Epoch: [11][150/391]\tTime 0.042 (0.082)\tData 0.000 (0.032)\tLoss 0.5639 (0.5711)\tPrec@1 82.031 (80.298)\n",
            "Epoch: [11][200/391]\tTime 0.048 (0.073)\tData 0.000 (0.024)\tLoss 0.6767 (0.5714)\tPrec@1 74.219 (80.329)\n",
            "Epoch: [11][250/391]\tTime 0.048 (0.068)\tData 0.000 (0.020)\tLoss 0.4643 (0.5696)\tPrec@1 84.375 (80.466)\n",
            "Epoch: [11][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.016)\tLoss 0.5927 (0.5723)\tPrec@1 77.344 (80.399)\n",
            "Epoch: [11][350/391]\tTime 0.047 (0.062)\tData 0.000 (0.014)\tLoss 0.5000 (0.5700)\tPrec@1 85.938 (80.438)\n",
            "epoch 11 training time consumed: 24.02s\n",
            "Test: [0/79]\tTime 3.720 (3.720)\tLoss 0.7703 (0.7703)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.016 (0.090)\tLoss 0.6302 (0.7064)\tPrec@1 77.344 (76.884)\n",
            " * Prec@1 76.600\n",
            "current lr 9.91144e-02\n",
            "Epoch: [12][0/391]\tTime 5.036 (5.036)\tData 4.835 (4.835)\tLoss 0.4269 (0.4269)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [12][50/391]\tTime 0.047 (0.146)\tData 0.000 (0.095)\tLoss 0.7602 (0.5532)\tPrec@1 75.000 (81.434)\n",
            "Epoch: [12][100/391]\tTime 0.047 (0.097)\tData 0.000 (0.048)\tLoss 0.4044 (0.5487)\tPrec@1 85.938 (81.412)\n",
            "Epoch: [12][150/391]\tTime 0.053 (0.081)\tData 0.000 (0.032)\tLoss 0.5314 (0.5452)\tPrec@1 80.469 (81.441)\n",
            "Epoch: [12][200/391]\tTime 0.056 (0.073)\tData 0.000 (0.024)\tLoss 0.5856 (0.5488)\tPrec@1 78.125 (81.176)\n",
            "Epoch: [12][250/391]\tTime 0.052 (0.068)\tData 0.000 (0.020)\tLoss 0.5143 (0.5533)\tPrec@1 79.688 (80.995)\n",
            "Epoch: [12][300/391]\tTime 0.041 (0.064)\tData 0.000 (0.016)\tLoss 0.5219 (0.5503)\tPrec@1 81.250 (81.151)\n",
            "Epoch: [12][350/391]\tTime 0.048 (0.062)\tData 0.000 (0.014)\tLoss 0.6682 (0.5508)\tPrec@1 76.562 (81.088)\n",
            "epoch 12 training time consumed: 23.83s\n",
            "Test: [0/79]\tTime 3.733 (3.733)\tLoss 0.6153 (0.6153)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.018 (0.091)\tLoss 0.7147 (0.6884)\tPrec@1 76.562 (77.068)\n",
            " * Prec@1 76.890\n",
            "current lr 9.89611e-02\n",
            "Epoch: [13][0/391]\tTime 5.091 (5.091)\tData 4.858 (4.858)\tLoss 0.6121 (0.6121)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [13][50/391]\tTime 0.047 (0.148)\tData 0.000 (0.095)\tLoss 0.5607 (0.5174)\tPrec@1 78.906 (81.495)\n",
            "Epoch: [13][100/391]\tTime 0.062 (0.099)\tData 0.000 (0.048)\tLoss 0.5373 (0.5282)\tPrec@1 79.688 (81.730)\n",
            "Epoch: [13][150/391]\tTime 0.051 (0.082)\tData 0.000 (0.033)\tLoss 0.6631 (0.5324)\tPrec@1 75.781 (81.514)\n",
            "Epoch: [13][200/391]\tTime 0.049 (0.074)\tData 0.000 (0.024)\tLoss 0.4128 (0.5288)\tPrec@1 84.375 (81.685)\n",
            "Epoch: [13][250/391]\tTime 0.050 (0.069)\tData 0.000 (0.020)\tLoss 0.6397 (0.5321)\tPrec@1 78.125 (81.552)\n",
            "Epoch: [13][300/391]\tTime 0.050 (0.066)\tData 0.000 (0.016)\tLoss 0.6602 (0.5357)\tPrec@1 78.906 (81.437)\n",
            "Epoch: [13][350/391]\tTime 0.050 (0.063)\tData 0.000 (0.014)\tLoss 0.5605 (0.5355)\tPrec@1 81.250 (81.473)\n",
            "epoch 13 training time consumed: 24.43s\n",
            "Test: [0/79]\tTime 3.882 (3.882)\tLoss 0.5748 (0.5748)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.015 (0.094)\tLoss 0.6788 (0.7050)\tPrec@1 77.344 (75.980)\n",
            " * Prec@1 75.800\n",
            "current lr 9.87958e-02\n",
            "Epoch: [14][0/391]\tTime 5.222 (5.222)\tData 5.023 (5.023)\tLoss 0.6041 (0.6041)\tPrec@1 79.688 (79.688)\n",
            "Epoch: [14][50/391]\tTime 0.049 (0.151)\tData 0.000 (0.099)\tLoss 0.4666 (0.4963)\tPrec@1 82.031 (82.782)\n",
            "Epoch: [14][100/391]\tTime 0.050 (0.101)\tData 0.000 (0.050)\tLoss 0.7845 (0.5091)\tPrec@1 71.094 (82.248)\n",
            "Epoch: [14][150/391]\tTime 0.050 (0.084)\tData 0.000 (0.033)\tLoss 0.4433 (0.5156)\tPrec@1 85.156 (82.150)\n",
            "Epoch: [14][200/391]\tTime 0.049 (0.076)\tData 0.000 (0.025)\tLoss 0.5642 (0.5206)\tPrec@1 78.125 (82.023)\n",
            "Epoch: [14][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 0.5381 (0.5161)\tPrec@1 81.250 (82.262)\n",
            "Epoch: [14][300/391]\tTime 0.050 (0.067)\tData 0.001 (0.017)\tLoss 0.5231 (0.5200)\tPrec@1 84.375 (82.062)\n",
            "Epoch: [14][350/391]\tTime 0.050 (0.064)\tData 0.000 (0.014)\tLoss 0.6097 (0.5239)\tPrec@1 76.562 (81.913)\n",
            "epoch 14 training time consumed: 24.77s\n",
            "Test: [0/79]\tTime 3.840 (3.840)\tLoss 0.8562 (0.8562)\tPrec@1 69.531 (69.531)\n",
            "Test: [50/79]\tTime 0.014 (0.092)\tLoss 0.8600 (0.9599)\tPrec@1 75.781 (70.328)\n",
            " * Prec@1 70.420\n",
            "current lr 9.86185e-02\n",
            "Epoch: [15][0/391]\tTime 5.159 (5.159)\tData 4.925 (4.925)\tLoss 0.4803 (0.4803)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [15][50/391]\tTime 0.050 (0.150)\tData 0.000 (0.097)\tLoss 0.3776 (0.5261)\tPrec@1 86.719 (81.740)\n",
            "Epoch: [15][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.3959 (0.5116)\tPrec@1 84.375 (82.426)\n",
            "Epoch: [15][150/391]\tTime 0.048 (0.083)\tData 0.000 (0.033)\tLoss 0.5065 (0.5196)\tPrec@1 87.500 (82.026)\n",
            "Epoch: [15][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.6136 (0.5227)\tPrec@1 77.344 (81.946)\n",
            "Epoch: [15][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.5874 (0.5250)\tPrec@1 79.688 (81.873)\n",
            "Epoch: [15][300/391]\tTime 0.049 (0.066)\tData 0.001 (0.017)\tLoss 0.3612 (0.5210)\tPrec@1 86.719 (82.010)\n",
            "Epoch: [15][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.4104 (0.5185)\tPrec@1 86.719 (82.147)\n",
            "epoch 15 training time consumed: 24.55s\n",
            "Test: [0/79]\tTime 3.824 (3.824)\tLoss 0.5255 (0.5255)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.014 (0.092)\tLoss 0.6455 (0.5980)\tPrec@1 82.031 (80.208)\n",
            " * Prec@1 79.950\n",
            "current lr 9.84292e-02\n",
            "Epoch: [16][0/391]\tTime 5.166 (5.166)\tData 4.929 (4.929)\tLoss 0.3253 (0.3253)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [16][50/391]\tTime 0.050 (0.150)\tData 0.000 (0.097)\tLoss 0.4645 (0.4907)\tPrec@1 81.250 (83.395)\n",
            "Epoch: [16][100/391]\tTime 0.048 (0.100)\tData 0.000 (0.049)\tLoss 0.4679 (0.4989)\tPrec@1 87.500 (83.083)\n",
            "Epoch: [16][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.033)\tLoss 0.3550 (0.5025)\tPrec@1 85.938 (82.823)\n",
            "Epoch: [16][200/391]\tTime 0.050 (0.075)\tData 0.000 (0.025)\tLoss 0.4789 (0.5052)\tPrec@1 82.812 (82.680)\n",
            "Epoch: [16][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.5379 (0.5050)\tPrec@1 80.469 (82.722)\n",
            "Epoch: [16][300/391]\tTime 0.050 (0.067)\tData 0.000 (0.017)\tLoss 0.5834 (0.5063)\tPrec@1 81.250 (82.683)\n",
            "Epoch: [16][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.6443 (0.5078)\tPrec@1 77.344 (82.659)\n",
            "epoch 16 training time consumed: 24.65s\n",
            "Test: [0/79]\tTime 3.859 (3.859)\tLoss 0.4428 (0.4428)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.014 (0.093)\tLoss 0.7783 (0.6025)\tPrec@1 76.562 (80.331)\n",
            " * Prec@1 80.410\n",
            "current lr 9.82279e-02\n",
            "Epoch: [17][0/391]\tTime 5.183 (5.183)\tData 4.947 (4.947)\tLoss 0.3839 (0.3839)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [17][50/391]\tTime 0.050 (0.150)\tData 0.000 (0.097)\tLoss 0.4944 (0.5012)\tPrec@1 81.250 (83.150)\n",
            "Epoch: [17][100/391]\tTime 0.049 (0.100)\tData 0.001 (0.049)\tLoss 0.5440 (0.4917)\tPrec@1 82.812 (83.362)\n",
            "Epoch: [17][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.033)\tLoss 0.4364 (0.4922)\tPrec@1 83.594 (83.361)\n",
            "Epoch: [17][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.5222 (0.4977)\tPrec@1 82.812 (83.050)\n",
            "Epoch: [17][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.6257 (0.4996)\tPrec@1 81.250 (82.968)\n",
            "Epoch: [17][300/391]\tTime 0.047 (0.066)\tData 0.000 (0.017)\tLoss 0.5256 (0.5042)\tPrec@1 80.469 (82.815)\n",
            "Epoch: [17][350/391]\tTime 0.050 (0.064)\tData 0.001 (0.014)\tLoss 0.5879 (0.5027)\tPrec@1 79.688 (82.906)\n",
            "epoch 17 training time consumed: 24.61s\n",
            "Test: [0/79]\tTime 3.808 (3.808)\tLoss 0.6100 (0.6100)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.015 (0.091)\tLoss 0.7411 (0.6730)\tPrec@1 77.344 (78.370)\n",
            " * Prec@1 78.640\n",
            "current lr 9.80147e-02\n",
            "Epoch: [18][0/391]\tTime 5.174 (5.174)\tData 4.939 (4.939)\tLoss 0.3904 (0.3904)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [18][50/391]\tTime 0.050 (0.150)\tData 0.000 (0.097)\tLoss 0.5075 (0.4735)\tPrec@1 84.375 (84.038)\n",
            "Epoch: [18][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.4306 (0.4691)\tPrec@1 88.281 (84.112)\n",
            "Epoch: [18][150/391]\tTime 0.049 (0.083)\tData 0.000 (0.033)\tLoss 0.5196 (0.4857)\tPrec@1 82.031 (83.635)\n",
            "Epoch: [18][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.4507 (0.4835)\tPrec@1 83.594 (83.609)\n",
            "Epoch: [18][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.5035 (0.4829)\tPrec@1 82.812 (83.588)\n",
            "Epoch: [18][300/391]\tTime 0.049 (0.066)\tData 0.000 (0.017)\tLoss 0.4286 (0.4855)\tPrec@1 85.938 (83.503)\n",
            "Epoch: [18][350/391]\tTime 0.050 (0.064)\tData 0.000 (0.014)\tLoss 0.7149 (0.4830)\tPrec@1 76.562 (83.525)\n",
            "epoch 18 training time consumed: 24.60s\n",
            "Test: [0/79]\tTime 3.799 (3.799)\tLoss 0.6520 (0.6520)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 0.9149 (0.7470)\tPrec@1 70.312 (75.797)\n",
            " * Prec@1 75.990\n",
            "current lr 9.77897e-02\n",
            "Epoch: [19][0/391]\tTime 5.159 (5.159)\tData 4.923 (4.923)\tLoss 0.4942 (0.4942)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [19][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.097)\tLoss 0.6709 (0.4871)\tPrec@1 76.562 (83.655)\n",
            "Epoch: [19][100/391]\tTime 0.048 (0.100)\tData 0.000 (0.049)\tLoss 0.4289 (0.4834)\tPrec@1 86.719 (83.617)\n",
            "Epoch: [19][150/391]\tTime 0.049 (0.083)\tData 0.000 (0.033)\tLoss 0.6085 (0.4811)\tPrec@1 80.469 (83.604)\n",
            "Epoch: [19][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.3582 (0.4782)\tPrec@1 89.062 (83.722)\n",
            "Epoch: [19][250/391]\tTime 0.050 (0.070)\tData 0.000 (0.020)\tLoss 0.5578 (0.4805)\tPrec@1 79.688 (83.650)\n",
            "Epoch: [19][300/391]\tTime 0.049 (0.066)\tData 0.000 (0.017)\tLoss 0.5101 (0.4813)\tPrec@1 82.031 (83.581)\n",
            "Epoch: [19][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.4730 (0.4830)\tPrec@1 81.250 (83.480)\n",
            "epoch 19 training time consumed: 24.58s\n",
            "Test: [0/79]\tTime 3.795 (3.795)\tLoss 0.5690 (0.5690)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.015 (0.091)\tLoss 0.7837 (0.7118)\tPrec@1 77.344 (76.440)\n",
            " * Prec@1 76.810\n",
            "current lr 9.75528e-02\n",
            "Epoch: [20][0/391]\tTime 5.160 (5.160)\tData 4.892 (4.892)\tLoss 0.5254 (0.5254)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [20][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.096)\tLoss 0.5057 (0.4649)\tPrec@1 83.594 (84.237)\n",
            "Epoch: [20][100/391]\tTime 0.048 (0.100)\tData 0.000 (0.049)\tLoss 0.4223 (0.4700)\tPrec@1 88.281 (83.934)\n",
            "Epoch: [20][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.033)\tLoss 0.3391 (0.4711)\tPrec@1 86.719 (83.733)\n",
            "Epoch: [20][200/391]\tTime 0.050 (0.075)\tData 0.000 (0.025)\tLoss 0.4853 (0.4764)\tPrec@1 85.938 (83.578)\n",
            "Epoch: [20][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.3505 (0.4749)\tPrec@1 87.500 (83.656)\n",
            "Epoch: [20][300/391]\tTime 0.048 (0.066)\tData 0.000 (0.016)\tLoss 0.4512 (0.4758)\tPrec@1 82.031 (83.640)\n",
            "Epoch: [20][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.6008 (0.4788)\tPrec@1 77.344 (83.554)\n",
            "epoch 20 training time consumed: 24.59s\n",
            "Test: [0/79]\tTime 3.811 (3.811)\tLoss 0.5401 (0.5401)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 1.0332 (0.7786)\tPrec@1 67.188 (76.241)\n",
            " * Prec@1 76.520\n",
            "current lr 9.73043e-02\n",
            "Epoch: [21][0/391]\tTime 5.160 (5.160)\tData 4.914 (4.914)\tLoss 0.3938 (0.3938)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [21][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.096)\tLoss 0.4084 (0.4444)\tPrec@1 87.500 (84.773)\n",
            "Epoch: [21][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.4173 (0.4482)\tPrec@1 86.719 (84.421)\n",
            "Epoch: [21][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.033)\tLoss 0.3825 (0.4618)\tPrec@1 87.500 (84.054)\n",
            "Epoch: [21][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.3362 (0.4641)\tPrec@1 90.625 (83.959)\n",
            "Epoch: [21][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.5277 (0.4700)\tPrec@1 81.250 (83.774)\n",
            "Epoch: [21][300/391]\tTime 0.050 (0.066)\tData 0.000 (0.016)\tLoss 0.5882 (0.4742)\tPrec@1 78.906 (83.565)\n",
            "Epoch: [21][350/391]\tTime 0.050 (0.064)\tData 0.001 (0.014)\tLoss 0.4155 (0.4734)\tPrec@1 84.375 (83.554)\n",
            "epoch 21 training time consumed: 24.58s\n",
            "Test: [0/79]\tTime 3.790 (3.790)\tLoss 0.4462 (0.4462)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.013 (0.091)\tLoss 0.5392 (0.5443)\tPrec@1 82.812 (81.985)\n",
            " * Prec@1 82.170\n",
            "current lr 9.70440e-02\n",
            "Epoch: [22][0/391]\tTime 5.164 (5.164)\tData 4.909 (4.909)\tLoss 0.4946 (0.4946)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [22][50/391]\tTime 0.049 (0.150)\tData 0.001 (0.096)\tLoss 0.4702 (0.4451)\tPrec@1 83.594 (84.681)\n",
            "Epoch: [22][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.5126 (0.4636)\tPrec@1 80.469 (84.135)\n",
            "Epoch: [22][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.033)\tLoss 0.4231 (0.4592)\tPrec@1 85.156 (84.437)\n",
            "Epoch: [22][200/391]\tTime 0.049 (0.075)\tData 0.001 (0.025)\tLoss 0.4138 (0.4652)\tPrec@1 85.156 (84.223)\n",
            "Epoch: [22][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.4558 (0.4644)\tPrec@1 84.375 (84.216)\n",
            "Epoch: [22][300/391]\tTime 0.049 (0.066)\tData 0.000 (0.016)\tLoss 0.4723 (0.4642)\tPrec@1 82.031 (84.134)\n",
            "Epoch: [22][350/391]\tTime 0.050 (0.064)\tData 0.000 (0.014)\tLoss 0.4542 (0.4633)\tPrec@1 81.250 (84.132)\n",
            "epoch 22 training time consumed: 24.60s\n",
            "Test: [0/79]\tTime 3.796 (3.796)\tLoss 0.5293 (0.5293)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.014 (0.091)\tLoss 0.6087 (0.6733)\tPrec@1 81.250 (78.125)\n",
            " * Prec@1 78.160\n",
            "current lr 9.67722e-02\n",
            "Epoch: [23][0/391]\tTime 5.161 (5.161)\tData 4.910 (4.910)\tLoss 0.4793 (0.4793)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [23][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.096)\tLoss 0.4594 (0.4600)\tPrec@1 87.500 (84.069)\n",
            "Epoch: [23][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.4320 (0.4585)\tPrec@1 83.594 (84.081)\n",
            "Epoch: [23][150/391]\tTime 0.051 (0.083)\tData 0.001 (0.033)\tLoss 0.4629 (0.4558)\tPrec@1 82.812 (84.173)\n",
            "Epoch: [23][200/391]\tTime 0.050 (0.075)\tData 0.000 (0.025)\tLoss 0.3834 (0.4574)\tPrec@1 86.719 (84.181)\n",
            "Epoch: [23][250/391]\tTime 0.051 (0.070)\tData 0.001 (0.020)\tLoss 0.4272 (0.4610)\tPrec@1 84.375 (84.160)\n",
            "Epoch: [23][300/391]\tTime 0.049 (0.067)\tData 0.001 (0.016)\tLoss 0.3721 (0.4581)\tPrec@1 87.500 (84.170)\n",
            "Epoch: [23][350/391]\tTime 0.054 (0.064)\tData 0.000 (0.014)\tLoss 0.5269 (0.4581)\tPrec@1 82.812 (84.195)\n",
            "epoch 23 training time consumed: 24.71s\n",
            "Test: [0/79]\tTime 3.856 (3.856)\tLoss 0.5620 (0.5620)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.015 (0.093)\tLoss 0.7445 (0.6073)\tPrec@1 76.562 (79.412)\n",
            " * Prec@1 79.680\n",
            "current lr 9.64888e-02\n",
            "Epoch: [24][0/391]\tTime 5.168 (5.168)\tData 4.974 (4.974)\tLoss 0.4845 (0.4845)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [24][50/391]\tTime 0.049 (0.150)\tData 0.000 (0.098)\tLoss 0.3583 (0.4686)\tPrec@1 88.281 (84.145)\n",
            "Epoch: [24][100/391]\tTime 0.049 (0.100)\tData 0.000 (0.049)\tLoss 0.4291 (0.4587)\tPrec@1 83.594 (84.182)\n",
            "Epoch: [24][150/391]\tTime 0.051 (0.084)\tData 0.001 (0.033)\tLoss 0.5222 (0.4538)\tPrec@1 80.469 (84.277)\n",
            "Epoch: [24][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.4978 (0.4521)\tPrec@1 82.031 (84.223)\n",
            "Epoch: [24][250/391]\tTime 0.047 (0.070)\tData 0.000 (0.020)\tLoss 0.5466 (0.4533)\tPrec@1 79.688 (84.241)\n",
            "Epoch: [24][300/391]\tTime 0.051 (0.066)\tData 0.000 (0.017)\tLoss 0.3561 (0.4568)\tPrec@1 85.938 (84.115)\n",
            "Epoch: [24][350/391]\tTime 0.047 (0.064)\tData 0.000 (0.014)\tLoss 0.4467 (0.4585)\tPrec@1 85.156 (84.159)\n",
            "epoch 24 training time consumed: 24.49s\n",
            "Test: [0/79]\tTime 3.838 (3.838)\tLoss 0.8530 (0.8530)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.014 (0.093)\tLoss 0.7393 (0.7826)\tPrec@1 78.125 (75.214)\n",
            " * Prec@1 75.210\n",
            "current lr 9.61940e-02\n",
            "Epoch: [25][0/391]\tTime 5.150 (5.150)\tData 4.884 (4.884)\tLoss 0.4835 (0.4835)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [25][50/391]\tTime 0.048 (0.148)\tData 0.001 (0.096)\tLoss 0.4446 (0.4746)\tPrec@1 83.594 (83.609)\n",
            "Epoch: [25][100/391]\tTime 0.047 (0.099)\tData 0.000 (0.048)\tLoss 0.5621 (0.4598)\tPrec@1 83.594 (84.104)\n",
            "Epoch: [25][150/391]\tTime 0.050 (0.083)\tData 0.000 (0.032)\tLoss 0.4407 (0.4534)\tPrec@1 84.375 (84.282)\n",
            "Epoch: [25][200/391]\tTime 0.050 (0.074)\tData 0.000 (0.024)\tLoss 0.5402 (0.4570)\tPrec@1 82.812 (84.208)\n",
            "Epoch: [25][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.5703 (0.4598)\tPrec@1 78.125 (84.098)\n",
            "Epoch: [25][300/391]\tTime 0.050 (0.066)\tData 0.000 (0.016)\tLoss 0.3507 (0.4608)\tPrec@1 89.062 (84.006)\n",
            "Epoch: [25][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.3864 (0.4595)\tPrec@1 86.719 (84.088)\n",
            "epoch 25 training time consumed: 24.55s\n",
            "Test: [0/79]\tTime 3.857 (3.857)\tLoss 0.7534 (0.7534)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.014 (0.092)\tLoss 0.7198 (0.8215)\tPrec@1 80.469 (73.958)\n",
            " * Prec@1 74.600\n",
            "current lr 9.58877e-02\n",
            "Epoch: [26][0/391]\tTime 5.218 (5.218)\tData 4.986 (4.986)\tLoss 0.4507 (0.4507)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [26][50/391]\tTime 0.048 (0.152)\tData 0.000 (0.098)\tLoss 0.4247 (0.4338)\tPrec@1 83.594 (85.218)\n",
            "Epoch: [26][100/391]\tTime 0.049 (0.101)\tData 0.001 (0.050)\tLoss 0.4368 (0.4465)\tPrec@1 81.250 (84.731)\n",
            "Epoch: [26][150/391]\tTime 0.050 (0.084)\tData 0.000 (0.033)\tLoss 0.4170 (0.4468)\tPrec@1 82.031 (84.603)\n",
            "Epoch: [26][200/391]\tTime 0.051 (0.076)\tData 0.001 (0.025)\tLoss 0.4231 (0.4402)\tPrec@1 86.719 (84.896)\n",
            "Epoch: [26][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.4353 (0.4379)\tPrec@1 84.375 (84.957)\n",
            "Epoch: [26][300/391]\tTime 0.050 (0.067)\tData 0.000 (0.017)\tLoss 0.4201 (0.4389)\tPrec@1 83.594 (84.904)\n",
            "Epoch: [26][350/391]\tTime 0.050 (0.065)\tData 0.001 (0.014)\tLoss 0.5886 (0.4413)\tPrec@1 79.688 (84.818)\n",
            "epoch 26 training time consumed: 24.84s\n",
            "Test: [0/79]\tTime 3.868 (3.868)\tLoss 0.7818 (0.7818)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.014 (0.093)\tLoss 0.6416 (0.8297)\tPrec@1 78.125 (73.529)\n",
            " * Prec@1 73.510\n",
            "current lr 9.55702e-02\n",
            "Epoch: [27][0/391]\tTime 5.191 (5.191)\tData 4.994 (4.994)\tLoss 0.4554 (0.4554)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [27][50/391]\tTime 0.051 (0.151)\tData 0.002 (0.098)\tLoss 0.4464 (0.4280)\tPrec@1 84.375 (85.386)\n",
            "Epoch: [27][100/391]\tTime 0.055 (0.100)\tData 0.000 (0.050)\tLoss 0.4542 (0.4350)\tPrec@1 86.719 (85.156)\n",
            "Epoch: [27][150/391]\tTime 0.047 (0.083)\tData 0.000 (0.033)\tLoss 0.3345 (0.4429)\tPrec@1 89.844 (84.923)\n",
            "Epoch: [27][200/391]\tTime 0.047 (0.074)\tData 0.000 (0.025)\tLoss 0.3856 (0.4437)\tPrec@1 88.281 (84.923)\n",
            "Epoch: [27][250/391]\tTime 0.050 (0.069)\tData 0.000 (0.020)\tLoss 0.4779 (0.4461)\tPrec@1 84.375 (84.957)\n",
            "Epoch: [27][300/391]\tTime 0.049 (0.066)\tData 0.000 (0.017)\tLoss 0.6050 (0.4484)\tPrec@1 82.031 (84.780)\n",
            "Epoch: [27][350/391]\tTime 0.051 (0.064)\tData 0.001 (0.014)\tLoss 0.4846 (0.4480)\tPrec@1 83.594 (84.787)\n",
            "epoch 27 training time consumed: 24.55s\n",
            "Test: [0/79]\tTime 3.934 (3.934)\tLoss 0.5631 (0.5631)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.015 (0.093)\tLoss 0.8591 (0.6708)\tPrec@1 75.781 (77.911)\n",
            " * Prec@1 77.800\n",
            "current lr 9.52414e-02\n",
            "Epoch: [28][0/391]\tTime 5.220 (5.220)\tData 4.985 (4.985)\tLoss 0.4164 (0.4164)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [28][50/391]\tTime 0.050 (0.153)\tData 0.001 (0.098)\tLoss 0.3723 (0.4027)\tPrec@1 85.156 (86.229)\n",
            "Epoch: [28][100/391]\tTime 0.050 (0.102)\tData 0.001 (0.050)\tLoss 0.5315 (0.4227)\tPrec@1 80.469 (85.574)\n",
            "Epoch: [28][150/391]\tTime 0.050 (0.084)\tData 0.000 (0.033)\tLoss 0.4179 (0.4219)\tPrec@1 88.281 (85.606)\n",
            "Epoch: [28][200/391]\tTime 0.050 (0.076)\tData 0.000 (0.025)\tLoss 0.3951 (0.4311)\tPrec@1 84.375 (85.273)\n",
            "Epoch: [28][250/391]\tTime 0.050 (0.071)\tData 0.000 (0.020)\tLoss 0.4734 (0.4368)\tPrec@1 83.594 (85.141)\n",
            "Epoch: [28][300/391]\tTime 0.049 (0.067)\tData 0.000 (0.017)\tLoss 0.3722 (0.4385)\tPrec@1 87.500 (85.034)\n",
            "Epoch: [28][350/391]\tTime 0.049 (0.065)\tData 0.000 (0.014)\tLoss 0.4411 (0.4454)\tPrec@1 89.062 (84.718)\n",
            "epoch 28 training time consumed: 24.81s\n",
            "Test: [0/79]\tTime 3.875 (3.875)\tLoss 0.5148 (0.5148)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.014 (0.094)\tLoss 0.6531 (0.6178)\tPrec@1 78.125 (79.519)\n",
            " * Prec@1 79.420\n",
            "current lr 9.49014e-02\n",
            "Epoch: [29][0/391]\tTime 5.197 (5.197)\tData 4.988 (4.988)\tLoss 0.4108 (0.4108)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [29][50/391]\tTime 0.049 (0.151)\tData 0.000 (0.098)\tLoss 0.3286 (0.4201)\tPrec@1 87.500 (85.386)\n",
            "Epoch: [29][100/391]\tTime 0.049 (0.101)\tData 0.000 (0.050)\tLoss 0.4751 (0.4230)\tPrec@1 85.156 (85.512)\n",
            "Epoch: [29][150/391]\tTime 0.049 (0.084)\tData 0.000 (0.033)\tLoss 0.4255 (0.4321)\tPrec@1 85.938 (85.244)\n",
            "Epoch: [29][200/391]\tTime 0.049 (0.075)\tData 0.000 (0.025)\tLoss 0.4361 (0.4293)\tPrec@1 82.812 (85.285)\n",
            "Epoch: [29][250/391]\tTime 0.049 (0.070)\tData 0.000 (0.020)\tLoss 0.4257 (0.4354)\tPrec@1 82.031 (85.019)\n",
            "Epoch: [29][300/391]\tTime 0.049 (0.067)\tData 0.000 (0.017)\tLoss 0.4809 (0.4374)\tPrec@1 86.719 (84.990)\n",
            "Epoch: [29][350/391]\tTime 0.049 (0.064)\tData 0.000 (0.014)\tLoss 0.3677 (0.4363)\tPrec@1 87.500 (85.045)\n",
            "epoch 29 training time consumed: 24.70s\n",
            "Test: [0/79]\tTime 3.877 (3.877)\tLoss 0.6254 (0.6254)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.014 (0.093)\tLoss 0.7053 (0.6882)\tPrec@1 78.906 (78.217)\n",
            " * Prec@1 78.030\n",
            "current lr 9.45503e-02\n",
            "Epoch: [30][0/391]\tTime 5.170 (5.170)\tData 4.967 (4.967)\tLoss 0.4621 (0.4621)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [30][50/391]\tTime 0.047 (0.150)\tData 0.000 (0.098)\tLoss 0.4912 (0.4251)\tPrec@1 82.812 (85.524)\n",
            "Epoch: [30][100/391]\tTime 0.049 (0.099)\tData 0.000 (0.050)\tLoss 0.3144 (0.4137)\tPrec@1 90.625 (85.821)\n",
            "Epoch: [30][150/391]\tTime 0.045 (0.082)\tData 0.000 (0.033)\tLoss 0.3259 (0.4182)\tPrec@1 89.062 (85.684)\n",
            "Epoch: [30][200/391]\tTime 0.052 (0.074)\tData 0.000 (0.025)\tLoss 0.4208 (0.4255)\tPrec@1 84.375 (85.428)\n",
            "Epoch: [30][250/391]\tTime 0.054 (0.069)\tData 0.000 (0.020)\tLoss 0.3488 (0.4245)\tPrec@1 85.156 (85.455)\n",
            "Epoch: [30][300/391]\tTime 0.050 (0.065)\tData 0.000 (0.017)\tLoss 0.4834 (0.4235)\tPrec@1 82.812 (85.452)\n",
            "Epoch: [30][350/391]\tTime 0.050 (0.063)\tData 0.000 (0.014)\tLoss 0.4101 (0.4252)\tPrec@1 87.500 (85.394)\n",
            "epoch 30 training time consumed: 24.28s\n",
            "Test: [0/79]\tTime 3.753 (3.753)\tLoss 0.7001 (0.7001)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.016 (0.089)\tLoss 0.8551 (0.7800)\tPrec@1 75.781 (75.919)\n",
            " * Prec@1 75.350\n",
            "current lr 9.41883e-02\n",
            "Epoch: [31][0/391]\tTime 5.034 (5.034)\tData 4.818 (4.818)\tLoss 0.4636 (0.4636)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [31][50/391]\tTime 0.033 (0.147)\tData 0.000 (0.095)\tLoss 0.4300 (0.4093)\tPrec@1 85.156 (85.723)\n",
            "Epoch: [31][100/391]\tTime 0.047 (0.098)\tData 0.000 (0.048)\tLoss 0.4197 (0.4172)\tPrec@1 80.469 (85.373)\n",
            "Epoch: [31][150/391]\tTime 0.058 (0.082)\tData 0.000 (0.032)\tLoss 0.3791 (0.4218)\tPrec@1 85.938 (85.280)\n",
            "Epoch: [31][200/391]\tTime 0.054 (0.073)\tData 0.000 (0.024)\tLoss 0.3717 (0.4191)\tPrec@1 82.812 (85.409)\n",
            "Epoch: [31][250/391]\tTime 0.053 (0.068)\tData 0.006 (0.020)\tLoss 0.5398 (0.4230)\tPrec@1 80.469 (85.253)\n",
            "Epoch: [31][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.016)\tLoss 0.4509 (0.4264)\tPrec@1 84.375 (85.187)\n",
            "Epoch: [31][350/391]\tTime 0.047 (0.062)\tData 0.000 (0.014)\tLoss 0.3783 (0.4263)\tPrec@1 85.938 (85.214)\n",
            "epoch 31 training time consumed: 23.98s\n",
            "Test: [0/79]\tTime 3.730 (3.730)\tLoss 0.4192 (0.4192)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.016 (0.090)\tLoss 0.5915 (0.5464)\tPrec@1 82.812 (81.526)\n",
            " * Prec@1 81.590\n",
            "current lr 9.38153e-02\n",
            "Epoch: [32][0/391]\tTime 5.034 (5.034)\tData 4.781 (4.781)\tLoss 0.5427 (0.5427)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [32][50/391]\tTime 0.053 (0.146)\tData 0.000 (0.094)\tLoss 0.4191 (0.4418)\tPrec@1 88.281 (84.406)\n",
            "Epoch: [32][100/391]\tTime 0.034 (0.097)\tData 0.000 (0.048)\tLoss 0.4465 (0.4215)\tPrec@1 82.031 (85.195)\n",
            "Epoch: [32][150/391]\tTime 0.047 (0.081)\tData 0.000 (0.032)\tLoss 0.3244 (0.4242)\tPrec@1 88.281 (85.265)\n",
            "Epoch: [32][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.3835 (0.4149)\tPrec@1 89.062 (85.611)\n",
            "Epoch: [32][250/391]\tTime 0.054 (0.068)\tData 0.000 (0.019)\tLoss 0.3651 (0.4220)\tPrec@1 89.062 (85.443)\n",
            "Epoch: [32][300/391]\tTime 0.049 (0.065)\tData 0.000 (0.016)\tLoss 0.2950 (0.4209)\tPrec@1 90.625 (85.512)\n",
            "Epoch: [32][350/391]\tTime 0.051 (0.062)\tData 0.000 (0.014)\tLoss 0.4381 (0.4237)\tPrec@1 85.938 (85.437)\n",
            "epoch 32 training time consumed: 24.02s\n",
            "Test: [0/79]\tTime 3.787 (3.787)\tLoss 0.5204 (0.5204)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.025 (0.091)\tLoss 0.5556 (0.5515)\tPrec@1 82.812 (81.495)\n",
            " * Prec@1 81.320\n",
            "current lr 9.34316e-02\n",
            "Epoch: [33][0/391]\tTime 5.183 (5.183)\tData 4.913 (4.913)\tLoss 0.4551 (0.4551)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [33][50/391]\tTime 0.054 (0.149)\tData 0.000 (0.097)\tLoss 0.3581 (0.3982)\tPrec@1 89.844 (85.692)\n",
            "Epoch: [33][100/391]\tTime 0.047 (0.099)\tData 0.000 (0.049)\tLoss 0.4945 (0.4116)\tPrec@1 84.375 (85.520)\n",
            "Epoch: [33][150/391]\tTime 0.054 (0.082)\tData 0.000 (0.033)\tLoss 0.3767 (0.4056)\tPrec@1 87.500 (85.813)\n",
            "Epoch: [33][200/391]\tTime 0.047 (0.074)\tData 0.000 (0.025)\tLoss 0.5164 (0.4174)\tPrec@1 82.031 (85.463)\n",
            "Epoch: [33][250/391]\tTime 0.047 (0.068)\tData 0.000 (0.020)\tLoss 0.6211 (0.4280)\tPrec@1 72.656 (85.060)\n",
            "Epoch: [33][300/391]\tTime 0.038 (0.065)\tData 0.000 (0.016)\tLoss 0.3927 (0.4271)\tPrec@1 86.719 (85.174)\n",
            "Epoch: [33][350/391]\tTime 0.047 (0.063)\tData 0.000 (0.014)\tLoss 0.4894 (0.4279)\tPrec@1 83.594 (85.265)\n",
            "epoch 33 training time consumed: 24.16s\n",
            "Test: [0/79]\tTime 3.829 (3.829)\tLoss 0.5784 (0.5784)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.010 (0.092)\tLoss 0.6763 (0.6071)\tPrec@1 74.219 (80.040)\n",
            " * Prec@1 80.010\n",
            "current lr 9.30371e-02\n",
            "Epoch: [34][0/391]\tTime 5.082 (5.082)\tData 4.833 (4.833)\tLoss 0.4284 (0.4284)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [34][50/391]\tTime 0.036 (0.147)\tData 0.000 (0.095)\tLoss 0.3711 (0.3947)\tPrec@1 85.938 (86.366)\n",
            "Epoch: [34][100/391]\tTime 0.047 (0.098)\tData 0.000 (0.048)\tLoss 0.4486 (0.4081)\tPrec@1 84.375 (85.922)\n",
            "Epoch: [34][150/391]\tTime 0.038 (0.081)\tData 0.000 (0.032)\tLoss 0.3642 (0.4105)\tPrec@1 86.719 (85.917)\n",
            "Epoch: [34][200/391]\tTime 0.048 (0.073)\tData 0.000 (0.024)\tLoss 0.4092 (0.4126)\tPrec@1 83.594 (85.844)\n",
            "Epoch: [34][250/391]\tTime 0.060 (0.068)\tData 0.000 (0.019)\tLoss 0.4189 (0.4156)\tPrec@1 85.938 (85.717)\n",
            "Epoch: [34][300/391]\tTime 0.038 (0.065)\tData 0.000 (0.016)\tLoss 0.5727 (0.4203)\tPrec@1 81.250 (85.504)\n",
            "Epoch: [34][350/391]\tTime 0.047 (0.062)\tData 0.000 (0.014)\tLoss 0.5382 (0.4223)\tPrec@1 82.812 (85.437)\n",
            "epoch 34 training time consumed: 23.95s\n",
            "Test: [0/79]\tTime 3.738 (3.738)\tLoss 0.5461 (0.5461)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.015 (0.091)\tLoss 0.6341 (0.6683)\tPrec@1 78.906 (78.600)\n",
            " * Prec@1 78.570\n",
            "current lr 9.26320e-02\n",
            "Epoch: [35][0/391]\tTime 5.037 (5.037)\tData 4.834 (4.834)\tLoss 0.3970 (0.3970)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [35][50/391]\tTime 0.047 (0.146)\tData 0.000 (0.095)\tLoss 0.4537 (0.3997)\tPrec@1 82.031 (86.244)\n",
            "Epoch: [35][100/391]\tTime 0.050 (0.098)\tData 0.000 (0.048)\tLoss 0.4585 (0.4151)\tPrec@1 84.375 (85.504)\n",
            "Epoch: [35][150/391]\tTime 0.049 (0.082)\tData 0.000 (0.032)\tLoss 0.3979 (0.4107)\tPrec@1 85.156 (85.906)\n",
            "Epoch: [35][200/391]\tTime 0.051 (0.073)\tData 0.000 (0.024)\tLoss 0.4359 (0.4131)\tPrec@1 86.719 (85.821)\n",
            "Epoch: [35][250/391]\tTime 0.048 (0.068)\tData 0.001 (0.019)\tLoss 0.3427 (0.4182)\tPrec@1 85.938 (85.589)\n",
            "Epoch: [35][300/391]\tTime 0.049 (0.065)\tData 0.000 (0.016)\tLoss 0.4853 (0.4189)\tPrec@1 85.938 (85.639)\n",
            "Epoch: [35][350/391]\tTime 0.058 (0.063)\tData 0.000 (0.014)\tLoss 0.4107 (0.4190)\tPrec@1 85.156 (85.575)\n",
            "epoch 35 training time consumed: 24.12s\n",
            "Test: [0/79]\tTime 3.736 (3.736)\tLoss 0.4742 (0.4742)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.014 (0.090)\tLoss 0.5753 (0.5941)\tPrec@1 83.594 (80.591)\n",
            " * Prec@1 80.720\n",
            "current lr 9.22164e-02\n",
            "Epoch: [36][0/391]\tTime 5.062 (5.062)\tData 4.842 (4.842)\tLoss 0.4574 (0.4574)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [36][50/391]\tTime 0.053 (0.148)\tData 0.000 (0.095)\tLoss 0.3688 (0.4073)\tPrec@1 85.938 (85.493)\n",
            "Epoch: [36][100/391]\tTime 0.033 (0.098)\tData 0.000 (0.048)\tLoss 0.5114 (0.4123)\tPrec@1 82.031 (85.427)\n",
            "Epoch: [36][150/391]\tTime 0.057 (0.082)\tData 0.000 (0.032)\tLoss 0.4543 (0.4129)\tPrec@1 85.938 (85.467)\n",
            "Epoch: [36][200/391]\tTime 0.048 (0.073)\tData 0.000 (0.024)\tLoss 0.3861 (0.4047)\tPrec@1 87.500 (85.755)\n",
            "Epoch: [36][250/391]\tTime 0.038 (0.068)\tData 0.000 (0.019)\tLoss 0.3302 (0.4108)\tPrec@1 90.625 (85.766)\n",
            "Epoch: [36][300/391]\tTime 0.035 (0.065)\tData 0.000 (0.016)\tLoss 0.3522 (0.4164)\tPrec@1 90.625 (85.621)\n",
            "Epoch: [36][350/391]\tTime 0.041 (0.062)\tData 0.000 (0.014)\tLoss 0.5618 (0.4186)\tPrec@1 83.594 (85.539)\n",
            "epoch 36 training time consumed: 23.99s\n",
            "Test: [0/79]\tTime 3.754 (3.754)\tLoss 0.8232 (0.8232)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.015 (0.091)\tLoss 0.8730 (0.7864)\tPrec@1 75.781 (75.199)\n",
            " * Prec@1 75.700\n",
            "current lr 9.17904e-02\n",
            "Epoch: [37][0/391]\tTime 5.083 (5.083)\tData 4.864 (4.864)\tLoss 0.5483 (0.5483)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [37][50/391]\tTime 0.048 (0.148)\tData 0.000 (0.096)\tLoss 0.3269 (0.3870)\tPrec@1 86.719 (86.903)\n",
            "Epoch: [37][100/391]\tTime 0.048 (0.099)\tData 0.001 (0.049)\tLoss 0.4071 (0.3850)\tPrec@1 85.156 (86.843)\n",
            "Epoch: [37][150/391]\tTime 0.048 (0.082)\tData 0.001 (0.033)\tLoss 0.4104 (0.3904)\tPrec@1 83.594 (86.626)\n",
            "Epoch: [37][200/391]\tTime 0.034 (0.073)\tData 0.000 (0.024)\tLoss 0.4571 (0.3971)\tPrec@1 86.719 (86.451)\n",
            "Epoch: [37][250/391]\tTime 0.048 (0.068)\tData 0.000 (0.020)\tLoss 0.4526 (0.4025)\tPrec@1 85.156 (86.211)\n",
            "Epoch: [37][300/391]\tTime 0.042 (0.065)\tData 0.001 (0.016)\tLoss 0.3963 (0.4036)\tPrec@1 85.938 (86.161)\n",
            "Epoch: [37][350/391]\tTime 0.047 (0.063)\tData 0.000 (0.014)\tLoss 0.4138 (0.4082)\tPrec@1 88.281 (86.064)\n",
            "epoch 37 training time consumed: 24.02s\n",
            "Test: [0/79]\tTime 3.734 (3.734)\tLoss 0.6691 (0.6691)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.016 (0.091)\tLoss 0.7410 (0.5926)\tPrec@1 73.438 (80.116)\n",
            " * Prec@1 80.060\n",
            "current lr 9.13540e-02\n",
            "Epoch: [38][0/391]\tTime 5.070 (5.070)\tData 4.852 (4.852)\tLoss 0.3696 (0.3696)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [38][50/391]\tTime 0.047 (0.148)\tData 0.000 (0.096)\tLoss 0.4195 (0.3836)\tPrec@1 89.062 (86.826)\n",
            "Epoch: [38][100/391]\tTime 0.052 (0.099)\tData 0.000 (0.048)\tLoss 0.6470 (0.3934)\tPrec@1 78.906 (86.208)\n",
            "Epoch: [38][150/391]\tTime 0.047 (0.082)\tData 0.000 (0.032)\tLoss 0.3171 (0.3939)\tPrec@1 91.406 (86.372)\n",
            "Epoch: [38][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.3524 (0.3947)\tPrec@1 87.500 (86.381)\n",
            "Epoch: [38][250/391]\tTime 0.044 (0.068)\tData 0.000 (0.020)\tLoss 0.3289 (0.3995)\tPrec@1 89.062 (86.255)\n",
            "Epoch: [38][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.016)\tLoss 0.4192 (0.4060)\tPrec@1 88.281 (86.044)\n",
            "Epoch: [38][350/391]\tTime 0.038 (0.062)\tData 0.000 (0.014)\tLoss 0.4353 (0.4063)\tPrec@1 82.812 (86.067)\n",
            "epoch 38 training time consumed: 24.00s\n",
            "Test: [0/79]\tTime 3.748 (3.748)\tLoss 0.5249 (0.5249)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.005 (0.091)\tLoss 0.4978 (0.5742)\tPrec@1 82.812 (80.928)\n",
            " * Prec@1 81.090\n",
            "current lr 9.09075e-02\n",
            "Epoch: [39][0/391]\tTime 5.033 (5.033)\tData 4.833 (4.833)\tLoss 0.2992 (0.2992)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [39][50/391]\tTime 0.050 (0.146)\tData 0.001 (0.095)\tLoss 0.3605 (0.4044)\tPrec@1 88.281 (85.800)\n",
            "Epoch: [39][100/391]\tTime 0.050 (0.098)\tData 0.000 (0.048)\tLoss 0.3801 (0.4081)\tPrec@1 84.375 (85.876)\n",
            "Epoch: [39][150/391]\tTime 0.048 (0.082)\tData 0.000 (0.032)\tLoss 0.5112 (0.4011)\tPrec@1 80.469 (86.243)\n",
            "Epoch: [39][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.3828 (0.3988)\tPrec@1 86.719 (86.326)\n",
            "Epoch: [39][250/391]\tTime 0.053 (0.068)\tData 0.000 (0.019)\tLoss 0.3094 (0.4016)\tPrec@1 92.969 (86.283)\n",
            "Epoch: [39][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.016)\tLoss 0.4088 (0.4052)\tPrec@1 83.594 (86.145)\n",
            "Epoch: [39][350/391]\tTime 0.053 (0.062)\tData 0.000 (0.014)\tLoss 0.3616 (0.4064)\tPrec@1 86.719 (86.084)\n",
            "epoch 39 training time consumed: 24.01s\n",
            "Test: [0/79]\tTime 3.751 (3.751)\tLoss 0.5399 (0.5399)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.013 (0.091)\tLoss 0.6024 (0.5746)\tPrec@1 82.031 (81.602)\n",
            " * Prec@1 81.170\n",
            "current lr 9.04508e-02\n",
            "Epoch: [40][0/391]\tTime 5.110 (5.110)\tData 4.878 (4.878)\tLoss 0.4055 (0.4055)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [40][50/391]\tTime 0.049 (0.149)\tData 0.000 (0.096)\tLoss 0.3373 (0.3766)\tPrec@1 89.844 (87.439)\n",
            "Epoch: [40][100/391]\tTime 0.047 (0.099)\tData 0.000 (0.049)\tLoss 0.4334 (0.3808)\tPrec@1 84.375 (87.106)\n",
            "Epoch: [40][150/391]\tTime 0.039 (0.082)\tData 0.000 (0.033)\tLoss 0.4404 (0.3908)\tPrec@1 83.594 (86.600)\n",
            "Epoch: [40][200/391]\tTime 0.052 (0.074)\tData 0.000 (0.025)\tLoss 0.4402 (0.3964)\tPrec@1 84.375 (86.377)\n",
            "Epoch: [40][250/391]\tTime 0.052 (0.068)\tData 0.000 (0.020)\tLoss 0.4558 (0.3989)\tPrec@1 85.156 (86.283)\n",
            "Epoch: [40][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.017)\tLoss 0.4084 (0.3991)\tPrec@1 86.719 (86.259)\n",
            "Epoch: [40][350/391]\tTime 0.047 (0.063)\tData 0.000 (0.014)\tLoss 0.3886 (0.3985)\tPrec@1 86.719 (86.222)\n",
            "epoch 40 training time consumed: 24.05s\n",
            "Test: [0/79]\tTime 3.763 (3.763)\tLoss 0.5214 (0.5214)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.016 (0.090)\tLoss 0.5982 (0.6159)\tPrec@1 81.250 (79.825)\n",
            " * Prec@1 79.600\n",
            "current lr 8.99842e-02\n",
            "Epoch: [41][0/391]\tTime 5.104 (5.104)\tData 4.856 (4.856)\tLoss 0.3600 (0.3600)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [41][50/391]\tTime 0.034 (0.148)\tData 0.000 (0.095)\tLoss 0.3433 (0.3975)\tPrec@1 87.500 (86.336)\n",
            "Epoch: [41][100/391]\tTime 0.048 (0.099)\tData 0.000 (0.048)\tLoss 0.4889 (0.3978)\tPrec@1 83.594 (86.247)\n",
            "Epoch: [41][150/391]\tTime 0.053 (0.082)\tData 0.000 (0.032)\tLoss 0.4818 (0.3957)\tPrec@1 85.156 (86.419)\n",
            "Epoch: [41][200/391]\tTime 0.061 (0.073)\tData 0.000 (0.024)\tLoss 0.3052 (0.4035)\tPrec@1 92.188 (86.093)\n",
            "Epoch: [41][250/391]\tTime 0.053 (0.068)\tData 0.000 (0.019)\tLoss 0.3612 (0.4007)\tPrec@1 87.500 (86.227)\n",
            "Epoch: [41][300/391]\tTime 0.036 (0.065)\tData 0.000 (0.016)\tLoss 0.3578 (0.4012)\tPrec@1 88.281 (86.179)\n",
            "Epoch: [41][350/391]\tTime 0.057 (0.063)\tData 0.000 (0.014)\tLoss 0.4489 (0.4034)\tPrec@1 83.594 (86.116)\n",
            "epoch 41 training time consumed: 24.04s\n",
            "Test: [0/79]\tTime 3.780 (3.780)\tLoss 0.5708 (0.5708)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.000 (0.090)\tLoss 0.5852 (0.5474)\tPrec@1 78.125 (81.464)\n",
            " * Prec@1 81.270\n",
            "current lr 8.95078e-02\n",
            "Epoch: [42][0/391]\tTime 5.103 (5.103)\tData 4.872 (4.872)\tLoss 0.3972 (0.3972)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [42][50/391]\tTime 0.048 (0.148)\tData 0.000 (0.096)\tLoss 0.3181 (0.4176)\tPrec@1 88.281 (85.646)\n",
            "Epoch: [42][100/391]\tTime 0.047 (0.099)\tData 0.000 (0.048)\tLoss 0.3874 (0.4116)\tPrec@1 86.719 (85.675)\n",
            "Epoch: [42][150/391]\tTime 0.054 (0.082)\tData 0.000 (0.032)\tLoss 0.3294 (0.3986)\tPrec@1 89.844 (86.274)\n",
            "Epoch: [42][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.3437 (0.3911)\tPrec@1 91.406 (86.517)\n",
            "Epoch: [42][250/391]\tTime 0.047 (0.068)\tData 0.000 (0.020)\tLoss 0.3604 (0.3950)\tPrec@1 87.500 (86.336)\n",
            "Epoch: [42][300/391]\tTime 0.040 (0.065)\tData 0.000 (0.016)\tLoss 0.4292 (0.4008)\tPrec@1 82.031 (86.218)\n",
            "Epoch: [42][350/391]\tTime 0.048 (0.062)\tData 0.000 (0.014)\tLoss 0.3191 (0.4011)\tPrec@1 90.625 (86.225)\n",
            "epoch 42 training time consumed: 24.02s\n",
            "Test: [0/79]\tTime 3.758 (3.758)\tLoss 0.5447 (0.5447)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.005 (0.091)\tLoss 0.7113 (0.6334)\tPrec@1 76.562 (78.447)\n",
            " * Prec@1 78.030\n",
            "current lr 8.90215e-02\n",
            "Epoch: [43][0/391]\tTime 5.049 (5.049)\tData 4.834 (4.834)\tLoss 0.3278 (0.3278)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [43][50/391]\tTime 0.047 (0.148)\tData 0.000 (0.095)\tLoss 0.4094 (0.3972)\tPrec@1 87.500 (86.458)\n",
            "Epoch: [43][100/391]\tTime 0.056 (0.098)\tData 0.000 (0.048)\tLoss 0.4007 (0.3935)\tPrec@1 85.938 (86.610)\n",
            "Epoch: [43][150/391]\tTime 0.053 (0.082)\tData 0.000 (0.032)\tLoss 0.4478 (0.3896)\tPrec@1 84.375 (86.807)\n",
            "Epoch: [43][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.3676 (0.3937)\tPrec@1 88.281 (86.622)\n",
            "Epoch: [43][250/391]\tTime 0.048 (0.068)\tData 0.000 (0.019)\tLoss 0.5642 (0.4027)\tPrec@1 82.031 (86.267)\n",
            "Epoch: [43][300/391]\tTime 0.049 (0.065)\tData 0.000 (0.016)\tLoss 0.4451 (0.3987)\tPrec@1 89.062 (86.444)\n",
            "Epoch: [43][350/391]\tTime 0.053 (0.062)\tData 0.000 (0.014)\tLoss 0.3880 (0.3979)\tPrec@1 89.844 (86.427)\n",
            "epoch 43 training time consumed: 23.95s\n",
            "Test: [0/79]\tTime 3.748 (3.748)\tLoss 0.5060 (0.5060)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.011 (0.089)\tLoss 0.5992 (0.5371)\tPrec@1 85.156 (81.924)\n",
            " * Prec@1 81.960\n",
            "current lr 8.85257e-02\n",
            "Epoch: [44][0/391]\tTime 5.046 (5.046)\tData 4.829 (4.829)\tLoss 0.3560 (0.3560)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [44][50/391]\tTime 0.062 (0.148)\tData 0.000 (0.095)\tLoss 0.4010 (0.3901)\tPrec@1 85.156 (86.581)\n",
            "Epoch: [44][100/391]\tTime 0.051 (0.098)\tData 0.000 (0.048)\tLoss 0.4992 (0.3907)\tPrec@1 82.031 (86.425)\n",
            "Epoch: [44][150/391]\tTime 0.054 (0.082)\tData 0.000 (0.032)\tLoss 0.4454 (0.3851)\tPrec@1 85.938 (86.853)\n",
            "Epoch: [44][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.2868 (0.3900)\tPrec@1 90.625 (86.641)\n",
            "Epoch: [44][250/391]\tTime 0.054 (0.068)\tData 0.000 (0.019)\tLoss 0.4020 (0.3905)\tPrec@1 86.719 (86.644)\n",
            "Epoch: [44][300/391]\tTime 0.047 (0.065)\tData 0.000 (0.016)\tLoss 0.3839 (0.3918)\tPrec@1 86.719 (86.625)\n",
            "Epoch: [44][350/391]\tTime 0.054 (0.062)\tData 0.000 (0.014)\tLoss 0.2655 (0.3903)\tPrec@1 92.188 (86.712)\n",
            "epoch 44 training time consumed: 23.96s\n",
            "Test: [0/79]\tTime 3.733 (3.733)\tLoss 0.6671 (0.6671)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.016 (0.091)\tLoss 0.7445 (0.6678)\tPrec@1 72.656 (78.002)\n",
            " * Prec@1 78.250\n",
            "current lr 8.80203e-02\n",
            "Epoch: [45][0/391]\tTime 5.043 (5.043)\tData 4.826 (4.826)\tLoss 0.3858 (0.3858)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [45][50/391]\tTime 0.047 (0.146)\tData 0.000 (0.095)\tLoss 0.3763 (0.3545)\tPrec@1 88.281 (88.159)\n",
            "Epoch: [45][100/391]\tTime 0.047 (0.097)\tData 0.000 (0.048)\tLoss 0.3646 (0.3610)\tPrec@1 86.719 (87.833)\n",
            "Epoch: [45][150/391]\tTime 0.058 (0.081)\tData 0.000 (0.032)\tLoss 0.3765 (0.3776)\tPrec@1 87.500 (87.272)\n",
            "Epoch: [45][200/391]\tTime 0.047 (0.073)\tData 0.000 (0.024)\tLoss 0.4391 (0.3822)\tPrec@1 85.156 (87.084)\n",
            "Epoch: [45][250/391]\tTime 0.040 (0.068)\tData 0.000 (0.019)\tLoss 0.4967 (0.3847)\tPrec@1 83.594 (87.002)\n",
            "Epoch: [45][300/391]\tTime 0.053 (0.064)\tData 0.000 (0.016)\tLoss 0.3876 (0.3875)\tPrec@1 89.844 (86.882)\n",
            "Epoch: [45][350/391]\tTime 0.034 (0.062)\tData 0.000 (0.014)\tLoss 0.5192 (0.3886)\tPrec@1 82.031 (86.806)\n",
            "epoch 45 training time consumed: 23.88s\n",
            "Test: [0/79]\tTime 3.749 (3.749)\tLoss 0.6295 (0.6295)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.013 (0.091)\tLoss 0.6874 (0.6562)\tPrec@1 75.781 (79.458)\n",
            " * Prec@1 79.070\n",
            "current lr 8.75056e-02\n",
            "Epoch: [46][0/391]\tTime 5.064 (5.064)\tData 4.846 (4.846)\tLoss 0.3414 (0.3414)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [46][50/391]\tTime 0.051 (0.148)\tData 0.000 (0.096)\tLoss 0.4390 (0.3761)\tPrec@1 86.719 (86.887)\n",
            "Epoch: [46][100/391]\tTime 0.049 (0.099)\tData 0.001 (0.048)\tLoss 0.4976 (0.3872)\tPrec@1 85.156 (86.479)\n",
            "Epoch: [46][150/391]\tTime 0.047 (0.082)\tData 0.000 (0.032)\tLoss 0.2905 (0.3927)\tPrec@1 90.625 (86.429)\n",
            "Epoch: [46][200/391]\tTime 0.050 (0.074)\tData 0.000 (0.024)\tLoss 0.3777 (0.3940)\tPrec@1 89.062 (86.307)\n",
            "Epoch: [46][250/391]\tTime 0.050 (0.069)\tData 0.000 (0.020)\tLoss 0.2938 (0.3921)\tPrec@1 91.406 (86.442)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], epoch)\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcurrent lr \u001b[39m\u001b[39m{:.5e}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m---> 43\u001b[0m train(train_loader, model, criterion, optimizer, epoch)\n\u001b[0;32m     44\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m \u001b[39m# evaluate on validation set\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     39\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target_var)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# compute gradient and do SGD step\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     43\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    277\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[1;32m--> 279\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[0;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_prec1 = 0\n",
        "model = torch.nn.DataParallel(resnet56())\n",
        "print(\"total params : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "model.cuda()\n",
        "\n",
        "writer = SummaryWriter(config.log_dir,filename_suffix=config.model_name)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, 4),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]),\n",
        "    download=True),\n",
        "    batch_size=config.batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "    ])),\n",
        "    batch_size=128, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
        "                                momentum=config.momentum,\n",
        "                                weight_decay=config.weight_decay)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=0)\n",
        "\n",
        "for epoch in range(config.start_epoch, config.epochs):\n",
        "    # train for one epoch\n",
        "    writer.add_scalar(\"lr\", optimizer.param_groups[0]['lr'], epoch)\n",
        "    print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "    train(train_loader, model, criterion, optimizer, epoch, writer)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on validation set\n",
        "    prec1 = validate(val_loader, model, criterion, epoch, writer)\n",
        "\n",
        "    # remember best acc and save checkpoint\n",
        "    is_best = prec1 > best_prec1\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), \"best.pth\")\n",
        "        best_prec1 = prec1\n",
        "    \n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93.82\n"
          ]
        }
      ],
      "source": [
        "print(best_prec1)\n",
        "%tensorboard --logdir=''\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5d3b2ef1d9e6660f31dedd3f271467a7eb92c8f55d3b82d5289c27a73078029c"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0522ec8260b14df091e86856a3679cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9a9077c5af4478da373747a46ef0db7",
              "IPY_MODEL_08eabf0bd60b49cea0230de641aa7bd9",
              "IPY_MODEL_75ea2b9364524b9fae61c74a95b355bf"
            ],
            "layout": "IPY_MODEL_348deaf4ccf7468fb4cf9fd832cb0529"
          }
        },
        "08eabf0bd60b49cea0230de641aa7bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c6503da647432a80c00cad7eb59277",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f10e54310c494b0bb9cd7458a7d4a9e4",
            "value": 170498071
          }
        },
        "0cae142e8de843d0a3193013084f5031": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "348deaf4ccf7468fb4cf9fd832cb0529": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c6503da647432a80c00cad7eb59277": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ea2b9364524b9fae61c74a95b355bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f303d9f20e6454cabbcfb1583b3ebfc",
            "placeholder": "",
            "style": "IPY_MODEL_0cae142e8de843d0a3193013084f5031",
            "value": " 170498071/170498071 [00:05&lt;00:00, 33833098.55it/s]"
          }
        },
        "8f303d9f20e6454cabbcfb1583b3ebfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44007ca099643ba8cbf81e2a6d693d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8b1c2903124bc488a06449b4eedf39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9a9077c5af4478da373747a46ef0db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a44007ca099643ba8cbf81e2a6d693d1",
            "placeholder": "",
            "style": "IPY_MODEL_ae8b1c2903124bc488a06449b4eedf39",
            "value": "100%"
          }
        },
        "f10e54310c494b0bb9cd7458a7d4a9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
