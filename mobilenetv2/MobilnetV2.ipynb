{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ3HdxHacQcP",
        "outputId": "2ce97868-a21e-400e-dd34-fca0858ba54a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self):    \n",
        "        # input \n",
        "        self.num_classes = 10\n",
        "        # training \n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100\n",
        "        self.start_epoch = 0\n",
        "        self.momentum = 0.9\n",
        "        self.lr = 1e-1\n",
        "        self.weight_decay = 5e-4\n",
        "        self.label_smoothing = 0\n",
        "        self.model_name = 'mobilnetv2'\n",
        "\n",
        "        self.gpu = True\n",
        "        self.log_dir = '' + self.model_name\n",
        "config = Config()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IirlCH75CtYb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearBottleNeck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
        "            nn.BatchNorm2d(in_channels * t),\n",
        "            nn.ReLU6(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
        "            nn.BatchNorm2d(in_channels * t),\n",
        "            nn.ReLU6(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = self.residual(x)\n",
        "\n",
        "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
        "            residual += x\n",
        "\n",
        "        return residual\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pre = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "        # reduce downsampling for cifar10\n",
        "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
        "        self.stage2 = self._make_stage(2, 16, 24, 1, 6)\n",
        "        self.stage3 = self._make_stage(3, 24, 32, 1, 6)\n",
        "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
        "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
        "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
        "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(320, 1280, 1),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pre(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.stage5(x)\n",
        "        x = self.stage6(x)\n",
        "        x = self.stage7(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
        "\n",
        "        layers = []\n",
        "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
        "\n",
        "        while repeat - 1:\n",
        "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
        "            repeat -= 1\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def mobilenetv2():\n",
        "    return MobileNetV2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LNQSaPhf0QFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2369380\n"
          ]
        }
      ],
      "source": [
        "model = MobileNetV2()\n",
        "count = 0\n",
        "print(\"params : \",sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SH0klInQ5Gac"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    \n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "        \n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "            # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses += loss.item()\n",
        "        accs += prec1.item()\n",
        "        \n",
        "    losses /= len(train_loader)\n",
        "    accs /= len(train_loader)\n",
        "    print('[Epoch {epoch}] Average Loss : {loss:.3f}, Average Accuracy : {acc:.3f}'\n",
        "          .format(epoch = epoch , loss=losses, acc=accs))\n",
        "    \n",
        "    writer.add_scalar(\"Loss/train\", losses, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", accs, epoch)\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses += loss.item()\n",
        "            accs += prec1.item()\n",
        "            \n",
        "        losses /= len(val_loader)\n",
        "        accs /= len(val_loader)\n",
        "        print('[Validation] : Average Loss {loss:.3f}, Average Accuracy {acc:.3f}'\n",
        "            .format(loss=losses, acc=accs))\n",
        "        \n",
        "        writer.add_scalar(\"Loss/val\", losses, epoch)\n",
        "        writer.add_scalar(\"Accuracy/val\", accs, epoch)\n",
        "    return accs\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCSPUf1q0-Yu",
        "outputId": "1ef45991-31b5-452c-f214-0b688e6e54c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "cuda: True\n",
            "current lr 1.00000e-01\n",
            "Epoch: [0][0/391]\tTime 5.592 (5.592)\tData 4.931 (4.931)\tLoss 4.7458 (4.7458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][50/391]\tTime 0.374 (0.474)\tData 0.001 (0.097)\tLoss 2.7813 (3.3069)\tPrec@1 26.562 (20.113)\n",
            "Epoch: [0][100/391]\tTime 0.376 (0.425)\tData 0.001 (0.049)\tLoss 2.7416 (3.0332)\tPrec@1 32.812 (26.818)\n",
            "Epoch: [0][150/391]\tTime 0.374 (0.408)\tData 0.001 (0.033)\tLoss 2.6571 (2.9139)\tPrec@1 37.500 (30.324)\n",
            "Epoch: [0][200/391]\tTime 0.373 (0.399)\tData 0.000 (0.025)\tLoss 2.6431 (2.8374)\tPrec@1 40.625 (33.077)\n",
            "Epoch: [0][250/391]\tTime 0.375 (0.394)\tData 0.000 (0.020)\tLoss 2.6864 (2.7842)\tPrec@1 41.406 (35.256)\n",
            "Epoch: [0][300/391]\tTime 0.374 (0.391)\tData 0.001 (0.017)\tLoss 2.4161 (2.7433)\tPrec@1 53.906 (36.968)\n",
            "Epoch: [0][350/391]\tTime 0.374 (0.388)\tData 0.001 (0.014)\tLoss 2.3411 (2.7078)\tPrec@1 51.562 (38.709)\n",
            "epoch 0 training time consumed: 153.20s\n",
            "Test: [0/79]\tTime 4.128 (4.128)\tLoss 2.5149 (2.5149)\tPrec@1 47.656 (47.656)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.5601 (2.4657)\tPrec@1 50.000 (50.551)\n",
            " * Prec@1 50.030\n",
            "current lr 9.99753e-02\n",
            "Epoch: [1][0/391]\tTime 5.591 (5.591)\tData 4.955 (4.955)\tLoss 2.3157 (2.3157)\tPrec@1 60.938 (60.938)\n",
            "Epoch: [1][50/391]\tTime 0.371 (0.476)\tData 0.000 (0.097)\tLoss 2.4437 (2.4205)\tPrec@1 53.125 (52.390)\n",
            "Epoch: [1][100/391]\tTime 0.374 (0.425)\tData 0.000 (0.049)\tLoss 2.2550 (2.3922)\tPrec@1 60.156 (54.247)\n",
            "Epoch: [1][150/391]\tTime 0.370 (0.408)\tData 0.000 (0.033)\tLoss 2.3042 (2.3676)\tPrec@1 56.250 (55.314)\n",
            "Epoch: [1][200/391]\tTime 0.373 (0.399)\tData 0.000 (0.025)\tLoss 2.2541 (2.3480)\tPrec@1 64.062 (56.285)\n",
            "Epoch: [1][250/391]\tTime 0.374 (0.394)\tData 0.000 (0.020)\tLoss 2.2362 (2.3230)\tPrec@1 58.594 (57.592)\n",
            "Epoch: [1][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 2.0438 (2.3089)\tPrec@1 70.312 (58.373)\n",
            "Epoch: [1][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 2.1366 (2.2932)\tPrec@1 67.969 (59.248)\n",
            "epoch 1 training time consumed: 151.44s\n",
            "Test: [0/79]\tTime 4.133 (4.133)\tLoss 2.5762 (2.5762)\tPrec@1 53.906 (53.906)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 2.4558 (2.5637)\tPrec@1 54.688 (53.646)\n",
            " * Prec@1 53.690\n",
            "current lr 9.99013e-02\n",
            "Epoch: [2][0/391]\tTime 5.609 (5.609)\tData 4.987 (4.987)\tLoss 2.1440 (2.1440)\tPrec@1 68.750 (68.750)\n",
            "Epoch: [2][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.098)\tLoss 2.0990 (2.1704)\tPrec@1 73.438 (65.594)\n",
            "Epoch: [2][100/391]\tTime 0.374 (0.425)\tData 0.000 (0.050)\tLoss 2.0617 (2.1457)\tPrec@1 75.781 (66.994)\n",
            "Epoch: [2][150/391]\tTime 0.374 (0.408)\tData 0.000 (0.033)\tLoss 1.9777 (2.1365)\tPrec@1 77.344 (67.488)\n",
            "Epoch: [2][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 2.2063 (2.1315)\tPrec@1 63.281 (67.561)\n",
            "Epoch: [2][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 2.1446 (2.1236)\tPrec@1 66.406 (67.944)\n",
            "Epoch: [2][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 2.0910 (2.1155)\tPrec@1 68.750 (68.433)\n",
            "Epoch: [2][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 2.0504 (2.1151)\tPrec@1 71.875 (68.498)\n",
            "epoch 2 training time consumed: 151.67s\n",
            "Test: [0/79]\tTime 4.219 (4.219)\tLoss 2.1104 (2.1104)\tPrec@1 64.062 (64.062)\n",
            "Test: [50/79]\tTime 0.112 (0.193)\tLoss 2.2602 (2.1739)\tPrec@1 63.281 (66.330)\n",
            " * Prec@1 66.490\n",
            "current lr 9.97781e-02\n",
            "Epoch: [3][0/391]\tTime 5.576 (5.576)\tData 4.949 (4.949)\tLoss 2.1421 (2.1421)\tPrec@1 65.625 (65.625)\n",
            "Epoch: [3][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.097)\tLoss 1.9150 (2.0343)\tPrec@1 77.344 (72.335)\n",
            "Epoch: [3][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 2.1154 (2.0404)\tPrec@1 64.062 (72.161)\n",
            "Epoch: [3][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.9862 (2.0331)\tPrec@1 75.781 (72.630)\n",
            "Epoch: [3][200/391]\tTime 0.373 (0.401)\tData 0.000 (0.025)\tLoss 2.0467 (2.0284)\tPrec@1 72.656 (72.788)\n",
            "Epoch: [3][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 2.1038 (2.0232)\tPrec@1 71.094 (72.983)\n",
            "Epoch: [3][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.9658 (2.0215)\tPrec@1 77.344 (73.090)\n",
            "Epoch: [3][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.9784 (2.0157)\tPrec@1 72.656 (73.369)\n",
            "epoch 3 training time consumed: 151.79s\n",
            "Test: [0/79]\tTime 4.197 (4.197)\tLoss 2.1387 (2.1387)\tPrec@1 66.406 (66.406)\n",
            "Test: [50/79]\tTime 0.112 (0.193)\tLoss 2.2790 (2.1573)\tPrec@1 64.062 (68.045)\n",
            " * Prec@1 67.350\n",
            "current lr 9.96057e-02\n",
            "Epoch: [4][0/391]\tTime 5.684 (5.684)\tData 4.985 (4.985)\tLoss 2.0011 (2.0011)\tPrec@1 74.219 (74.219)\n",
            "Epoch: [4][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 2.1208 (1.9777)\tPrec@1 66.406 (75.046)\n",
            "Epoch: [4][100/391]\tTime 0.373 (0.426)\tData 0.001 (0.050)\tLoss 1.9271 (1.9704)\tPrec@1 75.781 (75.418)\n",
            "Epoch: [4][150/391]\tTime 0.374 (0.409)\tData 0.001 (0.033)\tLoss 1.9956 (1.9667)\tPrec@1 75.000 (75.559)\n",
            "Epoch: [4][200/391]\tTime 0.374 (0.400)\tData 0.001 (0.025)\tLoss 1.8736 (1.9654)\tPrec@1 83.594 (75.704)\n",
            "Epoch: [4][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 2.0069 (1.9638)\tPrec@1 76.562 (75.840)\n",
            "Epoch: [4][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.9721 (1.9609)\tPrec@1 76.562 (75.999)\n",
            "Epoch: [4][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.9753 (1.9591)\tPrec@1 73.438 (76.088)\n",
            "epoch 4 training time consumed: 151.55s\n",
            "Test: [0/79]\tTime 4.035 (4.035)\tLoss 2.0897 (2.0897)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 2.2163 (2.1254)\tPrec@1 66.406 (68.964)\n",
            " * Prec@1 68.690\n",
            "current lr 9.93844e-02\n",
            "Epoch: [5][0/391]\tTime 5.598 (5.598)\tData 4.965 (4.965)\tLoss 1.9091 (1.9091)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [5][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.098)\tLoss 1.8725 (1.9215)\tPrec@1 78.125 (77.911)\n",
            "Epoch: [5][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.050)\tLoss 1.8087 (1.9202)\tPrec@1 81.250 (78.156)\n",
            "Epoch: [5][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.9421 (1.9174)\tPrec@1 71.875 (78.109)\n",
            "Epoch: [5][200/391]\tTime 0.377 (0.400)\tData 0.001 (0.025)\tLoss 1.8962 (1.9151)\tPrec@1 78.906 (78.284)\n",
            "Epoch: [5][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8921 (1.9134)\tPrec@1 78.125 (78.374)\n",
            "Epoch: [5][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.9000 (1.9143)\tPrec@1 82.031 (78.317)\n",
            "Epoch: [5][350/391]\tTime 0.372 (0.389)\tData 0.000 (0.014)\tLoss 1.9501 (1.9166)\tPrec@1 73.438 (78.156)\n",
            "epoch 5 training time consumed: 151.55s\n",
            "Test: [0/79]\tTime 4.133 (4.133)\tLoss 1.9668 (1.9668)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.0984 (2.0737)\tPrec@1 73.438 (71.415)\n",
            " * Prec@1 71.240\n",
            "current lr 9.91144e-02\n",
            "Epoch: [6][0/391]\tTime 5.650 (5.650)\tData 5.025 (5.025)\tLoss 1.8560 (1.8560)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [6][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.099)\tLoss 1.8772 (1.8877)\tPrec@1 79.688 (79.335)\n",
            "Epoch: [6][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.050)\tLoss 1.9211 (1.8763)\tPrec@1 78.125 (79.811)\n",
            "Epoch: [6][150/391]\tTime 0.374 (0.409)\tData 0.001 (0.034)\tLoss 1.9741 (1.8829)\tPrec@1 74.219 (79.522)\n",
            "Epoch: [6][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.9406 (1.8871)\tPrec@1 75.000 (79.478)\n",
            "Epoch: [6][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.8633 (1.8862)\tPrec@1 78.906 (79.507)\n",
            "Epoch: [6][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.8920 (1.8903)\tPrec@1 75.781 (79.290)\n",
            "Epoch: [6][350/391]\tTime 0.375 (0.389)\tData 0.001 (0.015)\tLoss 1.9497 (1.8918)\tPrec@1 79.688 (79.173)\n",
            "epoch 6 training time consumed: 151.57s\n",
            "Test: [0/79]\tTime 4.114 (4.114)\tLoss 2.0806 (2.0806)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.1179 (2.0359)\tPrec@1 68.750 (73.300)\n",
            " * Prec@1 73.370\n",
            "current lr 9.87958e-02\n",
            "Epoch: [7][0/391]\tTime 5.656 (5.656)\tData 4.961 (4.961)\tLoss 1.8270 (1.8270)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [7][50/391]\tTime 0.370 (0.476)\tData 0.000 (0.098)\tLoss 1.9071 (1.8455)\tPrec@1 79.688 (81.801)\n",
            "Epoch: [7][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.050)\tLoss 1.8265 (1.8557)\tPrec@1 83.594 (81.211)\n",
            "Epoch: [7][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.8973 (1.8573)\tPrec@1 78.906 (81.053)\n",
            "Epoch: [7][200/391]\tTime 0.376 (0.400)\tData 0.001 (0.025)\tLoss 1.9048 (1.8593)\tPrec@1 78.906 (81.013)\n",
            "Epoch: [7][250/391]\tTime 0.376 (0.395)\tData 0.001 (0.020)\tLoss 1.8746 (1.8626)\tPrec@1 79.688 (80.755)\n",
            "Epoch: [7][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.8816 (1.8637)\tPrec@1 79.688 (80.663)\n",
            "Epoch: [7][350/391]\tTime 0.377 (0.389)\tData 0.002 (0.014)\tLoss 1.9348 (1.8647)\tPrec@1 76.562 (80.564)\n",
            "epoch 7 training time consumed: 151.75s\n",
            "Test: [0/79]\tTime 4.175 (4.175)\tLoss 2.0100 (2.0100)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.9988 (1.9663)\tPrec@1 78.906 (76.547)\n",
            " * Prec@1 76.360\n",
            "current lr 9.84292e-02\n",
            "Epoch: [8][0/391]\tTime 5.596 (5.596)\tData 4.993 (4.993)\tLoss 1.8363 (1.8363)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [8][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.098)\tLoss 1.7730 (1.8504)\tPrec@1 84.375 (81.036)\n",
            "Epoch: [8][100/391]\tTime 0.377 (0.426)\tData 0.000 (0.050)\tLoss 1.8297 (1.8501)\tPrec@1 79.688 (81.080)\n",
            "Epoch: [8][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 2.0140 (1.8557)\tPrec@1 71.875 (80.784)\n",
            "Epoch: [8][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.9275 (1.8595)\tPrec@1 75.781 (80.601)\n",
            "Epoch: [8][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.8016 (1.8582)\tPrec@1 82.812 (80.618)\n",
            "Epoch: [8][300/391]\tTime 0.373 (0.392)\tData 0.001 (0.017)\tLoss 1.8986 (1.8577)\tPrec@1 80.469 (80.510)\n",
            "Epoch: [8][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.8593 (1.8544)\tPrec@1 83.594 (80.722)\n",
            "epoch 8 training time consumed: 151.70s\n",
            "Test: [0/79]\tTime 4.191 (4.191)\tLoss 1.8988 (1.8988)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.9375 (1.9197)\tPrec@1 76.562 (78.508)\n",
            " * Prec@1 78.590\n",
            "current lr 9.80147e-02\n",
            "Epoch: [9][0/391]\tTime 5.548 (5.548)\tData 4.955 (4.955)\tLoss 1.9210 (1.9210)\tPrec@1 77.344 (77.344)\n",
            "Epoch: [9][50/391]\tTime 0.374 (0.475)\tData 0.000 (0.097)\tLoss 1.8132 (1.8338)\tPrec@1 84.375 (81.909)\n",
            "Epoch: [9][100/391]\tTime 0.374 (0.425)\tData 0.000 (0.049)\tLoss 1.7979 (1.8343)\tPrec@1 85.156 (81.969)\n",
            "Epoch: [9][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.9515 (1.8366)\tPrec@1 79.688 (81.819)\n",
            "Epoch: [9][200/391]\tTime 0.372 (0.399)\tData 0.000 (0.025)\tLoss 1.9579 (1.8456)\tPrec@1 75.781 (81.483)\n",
            "Epoch: [9][250/391]\tTime 0.373 (0.394)\tData 0.000 (0.020)\tLoss 1.7746 (1.8410)\tPrec@1 83.594 (81.698)\n",
            "Epoch: [9][300/391]\tTime 0.375 (0.391)\tData 0.001 (0.017)\tLoss 1.8873 (1.8416)\tPrec@1 75.781 (81.606)\n",
            "Epoch: [9][350/391]\tTime 0.375 (0.388)\tData 0.000 (0.014)\tLoss 1.8956 (1.8436)\tPrec@1 82.812 (81.473)\n",
            "epoch 9 training time consumed: 151.31s\n",
            "Test: [0/79]\tTime 4.149 (4.149)\tLoss 1.9546 (1.9546)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9493 (1.9425)\tPrec@1 78.125 (76.762)\n",
            " * Prec@1 76.870\n",
            "current lr 9.75528e-02\n",
            "Epoch: [10][0/391]\tTime 5.533 (5.533)\tData 4.988 (4.988)\tLoss 1.8426 (1.8426)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [10][50/391]\tTime 0.376 (0.476)\tData 0.000 (0.098)\tLoss 1.7976 (1.8455)\tPrec@1 83.594 (82.031)\n",
            "Epoch: [10][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.050)\tLoss 1.7857 (1.8393)\tPrec@1 83.594 (81.938)\n",
            "Epoch: [10][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.8623 (1.8400)\tPrec@1 82.812 (81.928)\n",
            "Epoch: [10][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.9575 (1.8399)\tPrec@1 74.219 (81.845)\n",
            "Epoch: [10][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8608 (1.8355)\tPrec@1 80.469 (81.975)\n",
            "Epoch: [10][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.8719 (1.8359)\tPrec@1 79.688 (81.917)\n",
            "Epoch: [10][350/391]\tTime 0.376 (0.390)\tData 0.001 (0.014)\tLoss 1.7966 (1.8380)\tPrec@1 81.250 (81.771)\n",
            "epoch 10 training time consumed: 151.75s\n",
            "Test: [0/79]\tTime 4.039 (4.039)\tLoss 1.8414 (1.8414)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.9943 (1.9311)\tPrec@1 76.562 (77.619)\n",
            " * Prec@1 78.080\n",
            "current lr 9.70440e-02\n",
            "Epoch: [11][0/391]\tTime 5.624 (5.624)\tData 4.930 (4.930)\tLoss 1.8341 (1.8341)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [11][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.097)\tLoss 1.9512 (1.8307)\tPrec@1 73.438 (81.771)\n",
            "Epoch: [11][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.8493 (1.8253)\tPrec@1 80.469 (82.217)\n",
            "Epoch: [11][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.8489 (1.8253)\tPrec@1 82.031 (82.357)\n",
            "Epoch: [11][200/391]\tTime 0.372 (0.400)\tData 0.000 (0.025)\tLoss 1.9018 (1.8266)\tPrec@1 76.562 (82.268)\n",
            "Epoch: [11][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.7770 (1.8262)\tPrec@1 82.812 (82.199)\n",
            "Epoch: [11][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.8325 (1.8272)\tPrec@1 81.250 (82.140)\n",
            "Epoch: [11][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.8096 (1.8280)\tPrec@1 82.812 (82.147)\n",
            "epoch 11 training time consumed: 151.50s\n",
            "Test: [0/79]\tTime 4.216 (4.216)\tLoss 1.9954 (1.9954)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 2.1024 (2.0143)\tPrec@1 71.875 (74.173)\n",
            " * Prec@1 73.730\n",
            "current lr 9.64888e-02\n",
            "Epoch: [12][0/391]\tTime 5.549 (5.549)\tData 4.967 (4.967)\tLoss 1.7087 (1.7087)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [12][50/391]\tTime 0.376 (0.476)\tData 0.000 (0.098)\tLoss 1.9086 (1.8218)\tPrec@1 78.125 (82.920)\n",
            "Epoch: [12][100/391]\tTime 0.376 (0.426)\tData 0.000 (0.049)\tLoss 1.7804 (1.8180)\tPrec@1 81.250 (82.859)\n",
            "Epoch: [12][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.7858 (1.8195)\tPrec@1 82.812 (82.580)\n",
            "Epoch: [12][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.9104 (1.8193)\tPrec@1 82.812 (82.669)\n",
            "Epoch: [12][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.7733 (1.8220)\tPrec@1 85.156 (82.576)\n",
            "Epoch: [12][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.8041 (1.8222)\tPrec@1 81.250 (82.574)\n",
            "Epoch: [12][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.8988 (1.8211)\tPrec@1 76.562 (82.585)\n",
            "epoch 12 training time consumed: 151.80s\n",
            "Test: [0/79]\tTime 4.246 (4.246)\tLoss 2.0872 (2.0872)\tPrec@1 67.188 (67.188)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 2.1359 (2.0627)\tPrec@1 69.531 (72.044)\n",
            " * Prec@1 72.450\n",
            "current lr 9.58877e-02\n",
            "Epoch: [13][0/391]\tTime 5.551 (5.551)\tData 4.964 (4.964)\tLoss 1.8288 (1.8288)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [13][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.098)\tLoss 1.8111 (1.8259)\tPrec@1 82.031 (82.659)\n",
            "Epoch: [13][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.7928 (1.8306)\tPrec@1 82.031 (82.170)\n",
            "Epoch: [13][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.8281 (1.8270)\tPrec@1 82.812 (82.274)\n",
            "Epoch: [13][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.8066 (1.8227)\tPrec@1 85.156 (82.447)\n",
            "Epoch: [13][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8371 (1.8220)\tPrec@1 84.375 (82.492)\n",
            "Epoch: [13][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.7832 (1.8247)\tPrec@1 83.594 (82.332)\n",
            "Epoch: [13][350/391]\tTime 0.372 (0.389)\tData 0.000 (0.014)\tLoss 1.7216 (1.8243)\tPrec@1 88.281 (82.341)\n",
            "epoch 13 training time consumed: 151.64s\n",
            "Test: [0/79]\tTime 3.991 (3.991)\tLoss 1.9730 (1.9730)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.113 (0.189)\tLoss 1.9429 (1.9751)\tPrec@1 75.000 (75.398)\n",
            " * Prec@1 75.580\n",
            "current lr 9.52414e-02\n",
            "Epoch: [14][0/391]\tTime 5.622 (5.622)\tData 4.952 (4.952)\tLoss 1.8325 (1.8325)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [14][50/391]\tTime 0.374 (0.477)\tData 0.001 (0.097)\tLoss 1.8303 (1.7939)\tPrec@1 82.812 (83.425)\n",
            "Epoch: [14][100/391]\tTime 0.373 (0.426)\tData 0.001 (0.049)\tLoss 1.9629 (1.8022)\tPrec@1 75.781 (83.099)\n",
            "Epoch: [14][150/391]\tTime 0.375 (0.408)\tData 0.000 (0.033)\tLoss 1.7926 (1.8056)\tPrec@1 86.719 (83.035)\n",
            "Epoch: [14][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.8641 (1.8109)\tPrec@1 80.469 (82.708)\n",
            "Epoch: [14][250/391]\tTime 0.373 (0.395)\tData 0.001 (0.020)\tLoss 1.7659 (1.8102)\tPrec@1 85.156 (82.741)\n",
            "Epoch: [14][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.9262 (1.8134)\tPrec@1 78.125 (82.615)\n",
            "Epoch: [14][350/391]\tTime 0.377 (0.389)\tData 0.000 (0.014)\tLoss 1.9377 (1.8152)\tPrec@1 76.562 (82.554)\n",
            "epoch 14 training time consumed: 151.57s\n",
            "Test: [0/79]\tTime 4.089 (4.089)\tLoss 2.0289 (2.0289)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.9877 (2.0383)\tPrec@1 78.906 (74.648)\n",
            " * Prec@1 74.780\n",
            "current lr 9.45503e-02\n",
            "Epoch: [15][0/391]\tTime 5.607 (5.607)\tData 4.975 (4.975)\tLoss 1.7962 (1.7962)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [15][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.7308 (1.7959)\tPrec@1 85.156 (83.517)\n",
            "Epoch: [15][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.7741 (1.8004)\tPrec@1 82.031 (83.601)\n",
            "Epoch: [15][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.7955 (1.8039)\tPrec@1 84.375 (83.288)\n",
            "Epoch: [15][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.8481 (1.8096)\tPrec@1 79.688 (83.034)\n",
            "Epoch: [15][250/391]\tTime 0.373 (0.396)\tData 0.000 (0.020)\tLoss 1.8403 (1.8120)\tPrec@1 82.812 (82.847)\n",
            "Epoch: [15][300/391]\tTime 0.372 (0.392)\tData 0.000 (0.017)\tLoss 1.7539 (1.8118)\tPrec@1 85.938 (82.815)\n",
            "Epoch: [15][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7207 (1.8086)\tPrec@1 87.500 (83.017)\n",
            "epoch 15 training time consumed: 151.65s\n",
            "Test: [0/79]\tTime 4.212 (4.212)\tLoss 2.0243 (2.0243)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 2.1522 (2.0460)\tPrec@1 67.188 (74.893)\n",
            " * Prec@1 75.200\n",
            "current lr 9.38153e-02\n",
            "Epoch: [16][0/391]\tTime 5.608 (5.608)\tData 4.909 (4.909)\tLoss 1.7148 (1.7148)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [16][50/391]\tTime 0.373 (0.476)\tData 0.000 (0.097)\tLoss 1.7421 (1.7797)\tPrec@1 87.500 (84.513)\n",
            "Epoch: [16][100/391]\tTime 0.374 (0.425)\tData 0.000 (0.049)\tLoss 1.8394 (1.7953)\tPrec@1 78.906 (83.594)\n",
            "Epoch: [16][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.7367 (1.8017)\tPrec@1 83.594 (83.232)\n",
            "Epoch: [16][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.7748 (1.8029)\tPrec@1 83.594 (83.306)\n",
            "Epoch: [16][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8513 (1.8032)\tPrec@1 77.344 (83.292)\n",
            "Epoch: [16][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.016)\tLoss 1.8776 (1.8040)\tPrec@1 82.812 (83.277)\n",
            "Epoch: [16][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.8786 (1.8050)\tPrec@1 77.344 (83.280)\n",
            "epoch 16 training time consumed: 151.56s\n",
            "Test: [0/79]\tTime 4.095 (4.095)\tLoss 2.0310 (2.0310)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.1905 (2.1012)\tPrec@1 67.188 (71.262)\n",
            " * Prec@1 71.700\n",
            "current lr 9.30371e-02\n",
            "Epoch: [17][0/391]\tTime 5.578 (5.578)\tData 4.996 (4.996)\tLoss 1.7545 (1.7545)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [17][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.098)\tLoss 1.8957 (1.8055)\tPrec@1 81.250 (83.364)\n",
            "Epoch: [17][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.050)\tLoss 1.8953 (1.7985)\tPrec@1 75.781 (83.478)\n",
            "Epoch: [17][150/391]\tTime 0.376 (0.409)\tData 0.001 (0.033)\tLoss 1.7602 (1.7940)\tPrec@1 85.156 (83.552)\n",
            "Epoch: [17][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.8303 (1.7953)\tPrec@1 82.812 (83.578)\n",
            "Epoch: [17][250/391]\tTime 0.375 (0.395)\tData 0.001 (0.020)\tLoss 1.8742 (1.7961)\tPrec@1 79.688 (83.616)\n",
            "Epoch: [17][300/391]\tTime 0.376 (0.392)\tData 0.001 (0.017)\tLoss 1.7966 (1.8009)\tPrec@1 84.375 (83.404)\n",
            "Epoch: [17][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.015)\tLoss 1.7770 (1.7996)\tPrec@1 81.250 (83.440)\n",
            "epoch 17 training time consumed: 151.80s\n",
            "Test: [0/79]\tTime 4.064 (4.064)\tLoss 1.9583 (1.9583)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.9855 (1.9637)\tPrec@1 78.125 (77.543)\n",
            " * Prec@1 77.360\n",
            "current lr 9.22164e-02\n",
            "Epoch: [18][0/391]\tTime 5.592 (5.592)\tData 4.952 (4.952)\tLoss 1.8137 (1.8137)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [18][50/391]\tTime 0.377 (0.477)\tData 0.000 (0.097)\tLoss 1.8357 (1.7742)\tPrec@1 81.250 (85.064)\n",
            "Epoch: [18][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.8173 (1.7863)\tPrec@1 84.375 (84.236)\n",
            "Epoch: [18][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.7906 (1.7915)\tPrec@1 85.156 (83.868)\n",
            "Epoch: [18][200/391]\tTime 0.373 (0.400)\tData 0.001 (0.025)\tLoss 1.7727 (1.7897)\tPrec@1 85.156 (83.885)\n",
            "Epoch: [18][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.8189 (1.7899)\tPrec@1 82.812 (83.886)\n",
            "Epoch: [18][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7715 (1.7910)\tPrec@1 85.156 (83.882)\n",
            "Epoch: [18][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7841 (1.7897)\tPrec@1 85.156 (83.919)\n",
            "epoch 18 training time consumed: 151.57s\n",
            "Test: [0/79]\tTime 4.068 (4.068)\tLoss 2.1185 (2.1185)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 2.1263 (2.0531)\tPrec@1 65.625 (72.488)\n",
            " * Prec@1 72.540\n",
            "current lr 9.13540e-02\n",
            "Epoch: [19][0/391]\tTime 5.631 (5.631)\tData 4.997 (4.997)\tLoss 1.8377 (1.8377)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [19][50/391]\tTime 0.373 (0.476)\tData 0.000 (0.098)\tLoss 1.7740 (1.7880)\tPrec@1 84.375 (84.283)\n",
            "Epoch: [19][100/391]\tTime 0.374 (0.426)\tData 0.001 (0.050)\tLoss 1.7685 (1.7911)\tPrec@1 86.719 (84.104)\n",
            "Epoch: [19][150/391]\tTime 0.375 (0.408)\tData 0.000 (0.033)\tLoss 1.8992 (1.7898)\tPrec@1 81.250 (84.002)\n",
            "Epoch: [19][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.7613 (1.7875)\tPrec@1 85.938 (84.118)\n",
            "Epoch: [19][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.7714 (1.7890)\tPrec@1 82.812 (84.033)\n",
            "Epoch: [19][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.8431 (1.7901)\tPrec@1 82.812 (84.053)\n",
            "Epoch: [19][350/391]\tTime 0.376 (0.389)\tData 0.001 (0.014)\tLoss 1.8175 (1.7907)\tPrec@1 83.594 (84.006)\n",
            "epoch 19 training time consumed: 151.66s\n",
            "Test: [0/79]\tTime 4.218 (4.218)\tLoss 1.8329 (1.8329)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.114 (0.194)\tLoss 1.8823 (1.8465)\tPrec@1 81.250 (82.154)\n",
            " * Prec@1 82.450\n",
            "current lr 9.04508e-02\n",
            "Epoch: [20][0/391]\tTime 5.601 (5.601)\tData 4.962 (4.962)\tLoss 1.7487 (1.7487)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [20][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.097)\tLoss 1.8258 (1.7565)\tPrec@1 81.250 (85.478)\n",
            "Epoch: [20][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.7371 (1.7719)\tPrec@1 85.156 (84.940)\n",
            "Epoch: [20][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.7664 (1.7772)\tPrec@1 83.594 (84.566)\n",
            "Epoch: [20][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.8362 (1.7833)\tPrec@1 82.031 (84.204)\n",
            "Epoch: [20][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.7337 (1.7847)\tPrec@1 83.594 (84.082)\n",
            "Epoch: [20][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.7958 (1.7833)\tPrec@1 85.938 (84.136)\n",
            "Epoch: [20][350/391]\tTime 0.375 (0.389)\tData 0.001 (0.014)\tLoss 1.8870 (1.7828)\tPrec@1 81.250 (84.246)\n",
            "epoch 20 training time consumed: 151.55s\n",
            "Test: [0/79]\tTime 4.093 (4.093)\tLoss 2.5102 (2.5102)\tPrec@1 63.281 (63.281)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.7453 (2.6911)\tPrec@1 51.562 (54.810)\n",
            " * Prec@1 54.670\n",
            "current lr 8.95078e-02\n",
            "Epoch: [21][0/391]\tTime 5.613 (5.613)\tData 4.967 (4.967)\tLoss 1.6896 (1.6896)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [21][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.098)\tLoss 1.7497 (1.7666)\tPrec@1 85.938 (84.896)\n",
            "Epoch: [21][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.049)\tLoss 1.8076 (1.7767)\tPrec@1 81.250 (84.421)\n",
            "Epoch: [21][150/391]\tTime 0.376 (0.408)\tData 0.000 (0.033)\tLoss 1.7927 (1.7816)\tPrec@1 87.500 (84.272)\n",
            "Epoch: [21][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.6314 (1.7785)\tPrec@1 92.188 (84.422)\n",
            "Epoch: [21][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8839 (1.7829)\tPrec@1 78.906 (84.241)\n",
            "Epoch: [21][300/391]\tTime 0.375 (0.392)\tData 0.001 (0.017)\tLoss 1.8834 (1.7873)\tPrec@1 82.812 (83.923)\n",
            "Epoch: [21][350/391]\tTime 0.375 (0.389)\tData 0.001 (0.014)\tLoss 1.7389 (1.7857)\tPrec@1 85.156 (84.063)\n",
            "epoch 21 training time consumed: 151.72s\n",
            "Test: [0/79]\tTime 4.156 (4.156)\tLoss 2.0243 (2.0243)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.8996 (1.9204)\tPrec@1 78.125 (78.462)\n",
            " * Prec@1 78.570\n",
            "current lr 8.85257e-02\n",
            "Epoch: [22][0/391]\tTime 5.604 (5.604)\tData 4.952 (4.952)\tLoss 1.7768 (1.7768)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [22][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.097)\tLoss 1.7605 (1.7845)\tPrec@1 85.938 (83.824)\n",
            "Epoch: [22][100/391]\tTime 0.377 (0.427)\tData 0.000 (0.049)\tLoss 1.8619 (1.7880)\tPrec@1 78.125 (84.120)\n",
            "Epoch: [22][150/391]\tTime 0.374 (0.410)\tData 0.000 (0.033)\tLoss 1.7798 (1.7813)\tPrec@1 81.250 (84.478)\n",
            "Epoch: [22][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.7966 (1.7818)\tPrec@1 83.594 (84.379)\n",
            "Epoch: [22][250/391]\tTime 0.373 (0.396)\tData 0.000 (0.020)\tLoss 1.7624 (1.7805)\tPrec@1 88.281 (84.503)\n",
            "Epoch: [22][300/391]\tTime 0.375 (0.392)\tData 0.001 (0.017)\tLoss 1.7275 (1.7788)\tPrec@1 85.938 (84.564)\n",
            "Epoch: [22][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.014)\tLoss 1.7900 (1.7789)\tPrec@1 86.719 (84.493)\n",
            "epoch 22 training time consumed: 151.86s\n",
            "Test: [0/79]\tTime 4.195 (4.195)\tLoss 1.7824 (1.7824)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.9963 (1.8703)\tPrec@1 75.000 (81.173)\n",
            " * Prec@1 81.280\n",
            "current lr 8.75056e-02\n",
            "Epoch: [23][0/391]\tTime 5.597 (5.597)\tData 5.005 (5.005)\tLoss 1.7975 (1.7975)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [23][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.098)\tLoss 1.7916 (1.7626)\tPrec@1 84.375 (85.233)\n",
            "Epoch: [23][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.050)\tLoss 1.7226 (1.7693)\tPrec@1 89.062 (84.901)\n",
            "Epoch: [23][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.7676 (1.7683)\tPrec@1 85.156 (84.892)\n",
            "Epoch: [23][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.7207 (1.7702)\tPrec@1 89.062 (84.752)\n",
            "Epoch: [23][250/391]\tTime 0.372 (0.395)\tData 0.000 (0.020)\tLoss 1.7541 (1.7750)\tPrec@1 89.062 (84.612)\n",
            "Epoch: [23][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.7800 (1.7751)\tPrec@1 82.031 (84.603)\n",
            "Epoch: [23][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.8811 (1.7752)\tPrec@1 76.562 (84.558)\n",
            "epoch 23 training time consumed: 151.45s\n",
            "Test: [0/79]\tTime 4.154 (4.154)\tLoss 1.9394 (1.9394)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9680 (1.9113)\tPrec@1 80.469 (79.657)\n",
            " * Prec@1 79.690\n",
            "current lr 8.64484e-02\n",
            "Epoch: [24][0/391]\tTime 5.651 (5.651)\tData 4.933 (4.933)\tLoss 1.8070 (1.8070)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [24][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.097)\tLoss 1.6833 (1.7642)\tPrec@1 92.188 (84.911)\n",
            "Epoch: [24][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.8106 (1.7542)\tPrec@1 85.156 (85.373)\n",
            "Epoch: [24][150/391]\tTime 0.376 (0.409)\tData 0.001 (0.033)\tLoss 1.8143 (1.7573)\tPrec@1 81.250 (85.141)\n",
            "Epoch: [24][200/391]\tTime 0.377 (0.401)\tData 0.002 (0.025)\tLoss 1.7748 (1.7562)\tPrec@1 85.938 (85.246)\n",
            "Epoch: [24][250/391]\tTime 0.376 (0.396)\tData 0.000 (0.020)\tLoss 1.8903 (1.7600)\tPrec@1 77.344 (85.047)\n",
            "Epoch: [24][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7160 (1.7658)\tPrec@1 85.938 (84.832)\n",
            "Epoch: [24][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.7747 (1.7700)\tPrec@1 85.938 (84.718)\n",
            "epoch 24 training time consumed: 151.79s\n",
            "Test: [0/79]\tTime 4.130 (4.130)\tLoss 2.0015 (2.0015)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9840 (1.9893)\tPrec@1 72.656 (75.613)\n",
            " * Prec@1 75.820\n",
            "current lr 8.53553e-02\n",
            "Epoch: [25][0/391]\tTime 5.586 (5.586)\tData 4.992 (4.992)\tLoss 1.7986 (1.7986)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [25][50/391]\tTime 0.377 (0.476)\tData 0.001 (0.098)\tLoss 1.7338 (1.7691)\tPrec@1 88.281 (84.881)\n",
            "Epoch: [25][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.050)\tLoss 1.7788 (1.7622)\tPrec@1 85.156 (85.102)\n",
            "Epoch: [25][150/391]\tTime 0.375 (0.408)\tData 0.000 (0.033)\tLoss 1.7535 (1.7626)\tPrec@1 86.719 (85.089)\n",
            "Epoch: [25][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.8775 (1.7689)\tPrec@1 82.031 (84.880)\n",
            "Epoch: [25][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8499 (1.7704)\tPrec@1 81.250 (84.814)\n",
            "Epoch: [25][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.6766 (1.7724)\tPrec@1 89.844 (84.780)\n",
            "Epoch: [25][350/391]\tTime 0.376 (0.389)\tData 0.000 (0.014)\tLoss 1.7377 (1.7705)\tPrec@1 85.156 (84.845)\n",
            "epoch 25 training time consumed: 151.43s\n",
            "Test: [0/79]\tTime 4.012 (4.012)\tLoss 1.9314 (1.9314)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.113 (0.189)\tLoss 2.0006 (1.9902)\tPrec@1 76.562 (76.409)\n",
            " * Prec@1 76.400\n",
            "current lr 8.42274e-02\n",
            "Epoch: [26][0/391]\tTime 5.617 (5.617)\tData 4.977 (4.977)\tLoss 1.7543 (1.7543)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [26][50/391]\tTime 0.377 (0.477)\tData 0.000 (0.098)\tLoss 1.7205 (1.7592)\tPrec@1 89.062 (85.585)\n",
            "Epoch: [26][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.050)\tLoss 1.8529 (1.7635)\tPrec@1 82.812 (85.450)\n",
            "Epoch: [26][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.7280 (1.7650)\tPrec@1 86.719 (85.053)\n",
            "Epoch: [26][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.7759 (1.7603)\tPrec@1 82.031 (85.300)\n",
            "Epoch: [26][250/391]\tTime 0.375 (0.396)\tData 0.001 (0.020)\tLoss 1.6892 (1.7609)\tPrec@1 84.375 (85.306)\n",
            "Epoch: [26][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.7916 (1.7591)\tPrec@1 84.375 (85.372)\n",
            "Epoch: [26][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.8239 (1.7608)\tPrec@1 79.688 (85.248)\n",
            "epoch 26 training time consumed: 151.82s\n",
            "Test: [0/79]\tTime 4.068 (4.068)\tLoss 1.9243 (1.9243)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.9023 (1.9572)\tPrec@1 80.469 (78.202)\n",
            " * Prec@1 78.150\n",
            "current lr 8.30656e-02\n",
            "Epoch: [27][0/391]\tTime 5.565 (5.565)\tData 4.921 (4.921)\tLoss 1.7695 (1.7695)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [27][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.097)\tLoss 1.7511 (1.7448)\tPrec@1 82.031 (85.830)\n",
            "Epoch: [27][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.7711 (1.7559)\tPrec@1 86.719 (85.520)\n",
            "Epoch: [27][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.6581 (1.7614)\tPrec@1 91.406 (85.224)\n",
            "Epoch: [27][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.7856 (1.7608)\tPrec@1 85.156 (85.183)\n",
            "Epoch: [27][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.8124 (1.7633)\tPrec@1 82.031 (85.069)\n",
            "Epoch: [27][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7149 (1.7637)\tPrec@1 90.625 (85.081)\n",
            "Epoch: [27][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.8370 (1.7639)\tPrec@1 84.375 (85.049)\n",
            "epoch 27 training time consumed: 151.85s\n",
            "Test: [0/79]\tTime 4.229 (4.229)\tLoss 1.8721 (1.8721)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 2.0160 (1.9294)\tPrec@1 74.219 (78.202)\n",
            " * Prec@1 78.550\n",
            "current lr 8.18712e-02\n",
            "Epoch: [28][0/391]\tTime 5.574 (5.574)\tData 4.959 (4.959)\tLoss 1.7114 (1.7114)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [28][50/391]\tTime 0.373 (0.475)\tData 0.000 (0.097)\tLoss 1.6473 (1.7401)\tPrec@1 90.625 (86.305)\n",
            "Epoch: [28][100/391]\tTime 0.374 (0.424)\tData 0.000 (0.049)\tLoss 1.7680 (1.7436)\tPrec@1 84.375 (86.077)\n",
            "Epoch: [28][150/391]\tTime 0.375 (0.408)\tData 0.000 (0.033)\tLoss 1.7565 (1.7393)\tPrec@1 85.156 (86.212)\n",
            "Epoch: [28][200/391]\tTime 0.375 (0.399)\tData 0.002 (0.025)\tLoss 1.6746 (1.7434)\tPrec@1 89.062 (86.077)\n",
            "Epoch: [28][250/391]\tTime 0.375 (0.394)\tData 0.000 (0.020)\tLoss 1.6771 (1.7460)\tPrec@1 91.406 (85.950)\n",
            "Epoch: [28][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.6480 (1.7479)\tPrec@1 90.625 (85.831)\n",
            "Epoch: [28][350/391]\tTime 0.374 (0.388)\tData 0.000 (0.014)\tLoss 1.7718 (1.7536)\tPrec@1 85.156 (85.584)\n",
            "epoch 28 training time consumed: 151.34s\n",
            "Test: [0/79]\tTime 4.126 (4.126)\tLoss 1.9149 (1.9149)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9529 (1.9539)\tPrec@1 80.469 (77.436)\n",
            " * Prec@1 77.660\n",
            "current lr 8.06454e-02\n",
            "Epoch: [29][0/391]\tTime 5.567 (5.567)\tData 4.953 (4.953)\tLoss 1.7597 (1.7597)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [29][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.097)\tLoss 1.6153 (1.7491)\tPrec@1 92.969 (85.493)\n",
            "Epoch: [29][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.8119 (1.7438)\tPrec@1 82.812 (86.023)\n",
            "Epoch: [29][150/391]\tTime 0.375 (0.409)\tData 0.001 (0.033)\tLoss 1.7780 (1.7504)\tPrec@1 85.938 (85.808)\n",
            "Epoch: [29][200/391]\tTime 0.377 (0.401)\tData 0.002 (0.025)\tLoss 1.7441 (1.7541)\tPrec@1 85.156 (85.564)\n",
            "Epoch: [29][250/391]\tTime 0.374 (0.395)\tData 0.001 (0.020)\tLoss 1.7274 (1.7589)\tPrec@1 85.938 (85.331)\n",
            "Epoch: [29][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.8531 (1.7596)\tPrec@1 83.594 (85.296)\n",
            "Epoch: [29][350/391]\tTime 0.375 (0.390)\tData 0.001 (0.014)\tLoss 1.7668 (1.7583)\tPrec@1 85.156 (85.334)\n",
            "epoch 29 training time consumed: 151.81s\n",
            "Test: [0/79]\tTime 4.033 (4.033)\tLoss 1.9617 (1.9617)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 1.9573 (1.9540)\tPrec@1 75.781 (77.237)\n",
            " * Prec@1 77.150\n",
            "current lr 7.93893e-02\n",
            "Epoch: [30][0/391]\tTime 5.617 (5.617)\tData 4.969 (4.969)\tLoss 1.7349 (1.7349)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [30][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.098)\tLoss 1.7865 (1.7509)\tPrec@1 82.031 (85.355)\n",
            "Epoch: [30][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.049)\tLoss 1.7606 (1.7393)\tPrec@1 85.156 (86.270)\n",
            "Epoch: [30][150/391]\tTime 0.374 (0.408)\tData 0.000 (0.033)\tLoss 1.7511 (1.7421)\tPrec@1 86.719 (86.062)\n",
            "Epoch: [30][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.8452 (1.7458)\tPrec@1 81.250 (85.918)\n",
            "Epoch: [30][250/391]\tTime 0.372 (0.395)\tData 0.000 (0.020)\tLoss 1.6849 (1.7447)\tPrec@1 89.844 (85.966)\n",
            "Epoch: [30][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.7558 (1.7451)\tPrec@1 82.812 (85.899)\n",
            "Epoch: [30][350/391]\tTime 0.375 (0.389)\tData 0.001 (0.014)\tLoss 1.7799 (1.7479)\tPrec@1 83.594 (85.768)\n",
            "epoch 30 training time consumed: 151.54s\n",
            "Test: [0/79]\tTime 4.184 (4.184)\tLoss 1.7750 (1.7750)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.7808 (1.8366)\tPrec@1 84.375 (82.123)\n",
            " * Prec@1 82.150\n",
            "current lr 7.81042e-02\n",
            "Epoch: [31][0/391]\tTime 5.544 (5.544)\tData 4.946 (4.946)\tLoss 1.7658 (1.7658)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [31][50/391]\tTime 0.374 (0.475)\tData 0.000 (0.097)\tLoss 1.8076 (1.7411)\tPrec@1 84.375 (86.428)\n",
            "Epoch: [31][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.049)\tLoss 1.7267 (1.7346)\tPrec@1 85.938 (86.587)\n",
            "Epoch: [31][150/391]\tTime 0.375 (0.408)\tData 0.001 (0.033)\tLoss 1.7211 (1.7353)\tPrec@1 86.719 (86.507)\n",
            "Epoch: [31][200/391]\tTime 0.377 (0.400)\tData 0.000 (0.025)\tLoss 1.7279 (1.7410)\tPrec@1 82.812 (86.194)\n",
            "Epoch: [31][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.7785 (1.7426)\tPrec@1 84.375 (86.127)\n",
            "Epoch: [31][300/391]\tTime 0.376 (0.392)\tData 0.001 (0.017)\tLoss 1.8244 (1.7452)\tPrec@1 82.812 (86.036)\n",
            "Epoch: [31][350/391]\tTime 0.376 (0.389)\tData 0.000 (0.014)\tLoss 1.6995 (1.7465)\tPrec@1 88.281 (85.989)\n",
            "epoch 31 training time consumed: 151.71s\n",
            "Test: [0/79]\tTime 4.096 (4.096)\tLoss 2.3054 (2.3054)\tPrec@1 61.719 (61.719)\n",
            "Test: [50/79]\tTime 0.112 (0.191)\tLoss 2.3581 (2.2653)\tPrec@1 57.031 (65.518)\n",
            " * Prec@1 65.830\n",
            "current lr 7.67913e-02\n",
            "Epoch: [32][0/391]\tTime 5.617 (5.617)\tData 5.010 (5.010)\tLoss 1.8070 (1.8070)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [32][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.099)\tLoss 1.7679 (1.7618)\tPrec@1 84.375 (85.401)\n",
            "Epoch: [32][100/391]\tTime 0.376 (0.426)\tData 0.001 (0.050)\tLoss 1.8200 (1.7473)\tPrec@1 81.250 (85.968)\n",
            "Epoch: [32][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.6961 (1.7449)\tPrec@1 87.500 (85.974)\n",
            "Epoch: [32][200/391]\tTime 0.374 (0.401)\tData 0.001 (0.025)\tLoss 1.7322 (1.7415)\tPrec@1 88.281 (86.101)\n",
            "Epoch: [32][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.6926 (1.7424)\tPrec@1 89.062 (86.043)\n",
            "Epoch: [32][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.6665 (1.7416)\tPrec@1 91.406 (86.111)\n",
            "Epoch: [32][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.7130 (1.7437)\tPrec@1 88.281 (86.018)\n",
            "epoch 32 training time consumed: 151.74s\n",
            "Test: [0/79]\tTime 4.137 (4.137)\tLoss 1.7980 (1.7980)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.7972 (1.8110)\tPrec@1 85.156 (83.778)\n",
            " * Prec@1 83.610\n",
            "current lr 7.54521e-02\n",
            "Epoch: [33][0/391]\tTime 5.613 (5.613)\tData 4.949 (4.949)\tLoss 1.7487 (1.7487)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [33][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.097)\tLoss 1.7498 (1.7366)\tPrec@1 85.938 (86.351)\n",
            "Epoch: [33][100/391]\tTime 0.374 (0.425)\tData 0.002 (0.049)\tLoss 1.7502 (1.7386)\tPrec@1 85.156 (86.247)\n",
            "Epoch: [33][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.7618 (1.7347)\tPrec@1 84.375 (86.377)\n",
            "Epoch: [33][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.8137 (1.7420)\tPrec@1 83.594 (86.085)\n",
            "Epoch: [33][250/391]\tTime 0.373 (0.394)\tData 0.000 (0.020)\tLoss 1.8658 (1.7477)\tPrec@1 77.344 (85.822)\n",
            "Epoch: [33][300/391]\tTime 0.376 (0.391)\tData 0.000 (0.017)\tLoss 1.7267 (1.7456)\tPrec@1 85.156 (85.852)\n",
            "Epoch: [33][350/391]\tTime 0.375 (0.389)\tData 0.001 (0.014)\tLoss 1.8003 (1.7465)\tPrec@1 84.375 (85.833)\n",
            "epoch 33 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.122 (4.122)\tLoss 1.8380 (1.8380)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.8759 (1.8867)\tPrec@1 83.594 (79.228)\n",
            " * Prec@1 79.640\n",
            "current lr 7.40877e-02\n",
            "Epoch: [34][0/391]\tTime 5.619 (5.619)\tData 4.985 (4.985)\tLoss 1.7572 (1.7572)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [34][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.098)\tLoss 1.7201 (1.7110)\tPrec@1 88.281 (87.653)\n",
            "Epoch: [34][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.050)\tLoss 1.7667 (1.7216)\tPrec@1 88.281 (87.454)\n",
            "Epoch: [34][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.6976 (1.7250)\tPrec@1 86.719 (87.096)\n",
            "Epoch: [34][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.6916 (1.7282)\tPrec@1 88.281 (86.870)\n",
            "Epoch: [34][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.7061 (1.7315)\tPrec@1 87.500 (86.610)\n",
            "Epoch: [34][300/391]\tTime 0.376 (0.392)\tData 0.001 (0.017)\tLoss 1.7732 (1.7310)\tPrec@1 85.938 (86.641)\n",
            "Epoch: [34][350/391]\tTime 0.373 (0.389)\tData 0.000 (0.014)\tLoss 1.7673 (1.7310)\tPrec@1 81.250 (86.625)\n",
            "epoch 34 training time consumed: 151.73s\n",
            "Test: [0/79]\tTime 4.156 (4.156)\tLoss 1.9990 (1.9990)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.9399 (1.9696)\tPrec@1 78.906 (76.869)\n",
            " * Prec@1 77.050\n",
            "current lr 7.26995e-02\n",
            "Epoch: [35][0/391]\tTime 5.627 (5.627)\tData 4.918 (4.918)\tLoss 1.6847 (1.6847)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [35][50/391]\tTime 0.374 (0.476)\tData 0.001 (0.097)\tLoss 1.6592 (1.7180)\tPrec@1 89.062 (87.117)\n",
            "Epoch: [35][100/391]\tTime 0.375 (0.425)\tData 0.001 (0.049)\tLoss 1.7619 (1.7318)\tPrec@1 86.719 (86.672)\n",
            "Epoch: [35][150/391]\tTime 0.374 (0.408)\tData 0.000 (0.033)\tLoss 1.7938 (1.7334)\tPrec@1 82.031 (86.812)\n",
            "Epoch: [35][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.7391 (1.7314)\tPrec@1 85.156 (86.793)\n",
            "Epoch: [35][250/391]\tTime 0.376 (0.395)\tData 0.000 (0.020)\tLoss 1.7157 (1.7348)\tPrec@1 86.719 (86.566)\n",
            "Epoch: [35][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.7509 (1.7348)\tPrec@1 87.500 (86.511)\n",
            "Epoch: [35][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7576 (1.7354)\tPrec@1 86.719 (86.449)\n",
            "epoch 35 training time consumed: 151.61s\n",
            "Test: [0/79]\tTime 4.065 (4.065)\tLoss 1.7913 (1.7913)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.114 (0.190)\tLoss 1.8211 (1.8417)\tPrec@1 82.812 (80.607)\n",
            " * Prec@1 80.690\n",
            "current lr 7.12890e-02\n",
            "Epoch: [36][0/391]\tTime 5.623 (5.623)\tData 4.998 (4.998)\tLoss 1.7336 (1.7336)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [36][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.099)\tLoss 1.6925 (1.7252)\tPrec@1 89.062 (86.994)\n",
            "Epoch: [36][100/391]\tTime 0.372 (0.426)\tData 0.000 (0.050)\tLoss 1.8020 (1.7224)\tPrec@1 82.812 (86.935)\n",
            "Epoch: [36][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.034)\tLoss 1.7270 (1.7258)\tPrec@1 85.938 (86.905)\n",
            "Epoch: [36][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.6730 (1.7224)\tPrec@1 85.938 (87.034)\n",
            "Epoch: [36][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.6649 (1.7242)\tPrec@1 90.625 (86.996)\n",
            "Epoch: [36][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.6833 (1.7273)\tPrec@1 90.625 (86.825)\n",
            "Epoch: [36][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.015)\tLoss 1.7684 (1.7288)\tPrec@1 88.281 (86.779)\n",
            "epoch 36 training time consumed: 151.87s\n",
            "Test: [0/79]\tTime 4.159 (4.159)\tLoss 1.8779 (1.8779)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.8675 (1.8569)\tPrec@1 77.344 (81.235)\n",
            " * Prec@1 81.110\n",
            "current lr 6.98574e-02\n",
            "Epoch: [37][0/391]\tTime 5.650 (5.650)\tData 4.967 (4.967)\tLoss 1.7390 (1.7390)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [37][50/391]\tTime 0.374 (0.478)\tData 0.000 (0.098)\tLoss 1.6842 (1.7116)\tPrec@1 89.844 (87.806)\n",
            "Epoch: [37][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.7533 (1.7140)\tPrec@1 87.500 (87.662)\n",
            "Epoch: [37][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.6724 (1.7145)\tPrec@1 86.719 (87.567)\n",
            "Epoch: [37][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.7388 (1.7146)\tPrec@1 89.062 (87.547)\n",
            "Epoch: [37][250/391]\tTime 0.376 (0.395)\tData 0.000 (0.020)\tLoss 1.6936 (1.7191)\tPrec@1 88.281 (87.310)\n",
            "Epoch: [37][300/391]\tTime 0.381 (0.392)\tData 0.006 (0.017)\tLoss 1.7122 (1.7223)\tPrec@1 89.062 (87.098)\n",
            "Epoch: [37][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7148 (1.7230)\tPrec@1 87.500 (87.053)\n",
            "epoch 37 training time consumed: 151.67s\n",
            "Test: [0/79]\tTime 4.049 (4.049)\tLoss 1.8151 (1.8151)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.8383 (1.8288)\tPrec@1 85.156 (82.736)\n",
            " * Prec@1 83.070\n",
            "current lr 6.84062e-02\n",
            "Epoch: [38][0/391]\tTime 5.594 (5.594)\tData 4.931 (4.931)\tLoss 1.6763 (1.6763)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [38][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.097)\tLoss 1.7096 (1.7157)\tPrec@1 90.625 (87.393)\n",
            "Epoch: [38][100/391]\tTime 0.372 (0.425)\tData 0.000 (0.049)\tLoss 1.7937 (1.7119)\tPrec@1 84.375 (87.593)\n",
            "Epoch: [38][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.7289 (1.7179)\tPrec@1 86.719 (87.314)\n",
            "Epoch: [38][200/391]\tTime 0.375 (0.399)\tData 0.000 (0.025)\tLoss 1.6861 (1.7186)\tPrec@1 89.844 (87.267)\n",
            "Epoch: [38][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.6656 (1.7184)\tPrec@1 89.062 (87.273)\n",
            "Epoch: [38][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.7787 (1.7216)\tPrec@1 84.375 (87.072)\n",
            "Epoch: [38][350/391]\tTime 0.377 (0.389)\tData 0.001 (0.014)\tLoss 1.7837 (1.7225)\tPrec@1 83.594 (87.026)\n",
            "epoch 38 training time consumed: 151.62s\n",
            "Test: [0/79]\tTime 4.261 (4.261)\tLoss 1.9017 (1.9017)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.112 (0.194)\tLoss 1.8633 (1.8619)\tPrec@1 82.031 (81.434)\n",
            " * Prec@1 81.450\n",
            "current lr 6.69369e-02\n",
            "Epoch: [39][0/391]\tTime 5.606 (5.606)\tData 4.990 (4.990)\tLoss 1.6476 (1.6476)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [39][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.6843 (1.7046)\tPrec@1 92.188 (87.837)\n",
            "Epoch: [39][100/391]\tTime 0.380 (0.426)\tData 0.000 (0.050)\tLoss 1.7426 (1.7103)\tPrec@1 83.594 (87.531)\n",
            "Epoch: [39][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.7463 (1.7071)\tPrec@1 82.812 (87.531)\n",
            "Epoch: [39][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.6953 (1.7118)\tPrec@1 85.938 (87.345)\n",
            "Epoch: [39][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.6868 (1.7118)\tPrec@1 86.719 (87.379)\n",
            "Epoch: [39][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7220 (1.7164)\tPrec@1 85.156 (87.131)\n",
            "Epoch: [39][350/391]\tTime 0.378 (0.389)\tData 0.000 (0.014)\tLoss 1.6966 (1.7180)\tPrec@1 85.938 (87.033)\n",
            "epoch 39 training time consumed: 151.61s\n",
            "Test: [0/79]\tTime 4.060 (4.060)\tLoss 1.7304 (1.7304)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 1.7623 (1.7592)\tPrec@1 86.719 (85.846)\n",
            " * Prec@1 85.630\n",
            "current lr 6.54508e-02\n",
            "Epoch: [40][0/391]\tTime 5.583 (5.583)\tData 4.942 (4.942)\tLoss 1.7457 (1.7457)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [40][50/391]\tTime 0.372 (0.475)\tData 0.000 (0.097)\tLoss 1.5877 (1.6985)\tPrec@1 93.750 (88.419)\n",
            "Epoch: [40][100/391]\tTime 0.374 (0.424)\tData 0.000 (0.049)\tLoss 1.7784 (1.7006)\tPrec@1 85.156 (88.537)\n",
            "Epoch: [40][150/391]\tTime 0.376 (0.408)\tData 0.000 (0.033)\tLoss 1.7795 (1.7057)\tPrec@1 84.375 (88.116)\n",
            "Epoch: [40][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.7481 (1.7092)\tPrec@1 85.938 (87.893)\n",
            "Epoch: [40][250/391]\tTime 0.376 (0.395)\tData 0.000 (0.020)\tLoss 1.7952 (1.7123)\tPrec@1 84.375 (87.631)\n",
            "Epoch: [40][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7157 (1.7110)\tPrec@1 85.156 (87.689)\n",
            "Epoch: [40][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7468 (1.7112)\tPrec@1 89.844 (87.709)\n",
            "epoch 40 training time consumed: 151.65s\n",
            "Test: [0/79]\tTime 4.164 (4.164)\tLoss 1.7451 (1.7451)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.8015 (1.7834)\tPrec@1 83.594 (84.268)\n",
            " * Prec@1 84.450\n",
            "current lr 6.39496e-02\n",
            "Epoch: [41][0/391]\tTime 5.658 (5.658)\tData 5.035 (5.035)\tLoss 1.6389 (1.6389)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [41][50/391]\tTime 0.371 (0.478)\tData 0.000 (0.099)\tLoss 1.6296 (1.7006)\tPrec@1 93.750 (88.205)\n",
            "Epoch: [41][100/391]\tTime 0.376 (0.427)\tData 0.001 (0.050)\tLoss 1.7501 (1.7016)\tPrec@1 84.375 (88.134)\n",
            "Epoch: [41][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.034)\tLoss 1.7525 (1.7053)\tPrec@1 86.719 (87.759)\n",
            "Epoch: [41][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.6359 (1.7115)\tPrec@1 90.625 (87.426)\n",
            "Epoch: [41][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.7438 (1.7111)\tPrec@1 84.375 (87.447)\n",
            "Epoch: [41][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.6749 (1.7112)\tPrec@1 88.281 (87.443)\n",
            "Epoch: [41][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.015)\tLoss 1.7174 (1.7106)\tPrec@1 87.500 (87.462)\n",
            "epoch 41 training time consumed: 151.90s\n",
            "Test: [0/79]\tTime 4.155 (4.155)\tLoss 1.9570 (1.9570)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9704 (1.9672)\tPrec@1 76.562 (76.670)\n",
            " * Prec@1 76.780\n",
            "current lr 6.24345e-02\n",
            "Epoch: [42][0/391]\tTime 5.651 (5.651)\tData 4.974 (4.974)\tLoss 1.6838 (1.6838)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [42][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.098)\tLoss 1.7057 (1.7035)\tPrec@1 87.500 (87.730)\n",
            "Epoch: [42][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.050)\tLoss 1.7195 (1.7030)\tPrec@1 86.719 (87.755)\n",
            "Epoch: [42][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.6581 (1.6988)\tPrec@1 90.625 (88.023)\n",
            "Epoch: [42][200/391]\tTime 0.374 (0.399)\tData 0.000 (0.025)\tLoss 1.7025 (1.7007)\tPrec@1 86.719 (87.850)\n",
            "Epoch: [42][250/391]\tTime 0.376 (0.394)\tData 0.001 (0.020)\tLoss 1.7252 (1.7017)\tPrec@1 87.500 (87.886)\n",
            "Epoch: [42][300/391]\tTime 0.375 (0.391)\tData 0.001 (0.017)\tLoss 1.6980 (1.7051)\tPrec@1 91.406 (87.744)\n",
            "Epoch: [42][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.6844 (1.7068)\tPrec@1 87.500 (87.698)\n",
            "epoch 42 training time consumed: 151.37s\n",
            "Test: [0/79]\tTime 4.122 (4.122)\tLoss 1.9965 (1.9965)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 2.1040 (2.0562)\tPrec@1 73.438 (73.591)\n",
            " * Prec@1 73.140\n",
            "current lr 6.09072e-02\n",
            "Epoch: [43][0/391]\tTime 5.626 (5.626)\tData 4.997 (4.997)\tLoss 1.7282 (1.7282)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [43][50/391]\tTime 0.375 (0.476)\tData 0.001 (0.098)\tLoss 1.7188 (1.6930)\tPrec@1 89.062 (88.756)\n",
            "Epoch: [43][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.050)\tLoss 1.6484 (1.6921)\tPrec@1 91.406 (88.683)\n",
            "Epoch: [43][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.7433 (1.6930)\tPrec@1 85.156 (88.535)\n",
            "Epoch: [43][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.6837 (1.6955)\tPrec@1 89.844 (88.456)\n",
            "Epoch: [43][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.8014 (1.7004)\tPrec@1 84.375 (88.116)\n",
            "Epoch: [43][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.7815 (1.6992)\tPrec@1 85.156 (88.136)\n",
            "Epoch: [43][350/391]\tTime 0.376 (0.389)\tData 0.001 (0.014)\tLoss 1.7603 (1.6990)\tPrec@1 85.938 (88.145)\n",
            "epoch 43 training time consumed: 151.74s\n",
            "Test: [0/79]\tTime 4.160 (4.160)\tLoss 1.7711 (1.7711)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.7850 (1.7964)\tPrec@1 84.375 (83.900)\n",
            " * Prec@1 83.680\n",
            "current lr 5.93691e-02\n",
            "Epoch: [44][0/391]\tTime 5.594 (5.594)\tData 4.971 (4.971)\tLoss 1.6711 (1.6711)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [44][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.098)\tLoss 1.6902 (1.7081)\tPrec@1 87.500 (87.408)\n",
            "Epoch: [44][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.050)\tLoss 1.7435 (1.7054)\tPrec@1 85.938 (87.554)\n",
            "Epoch: [44][150/391]\tTime 0.375 (0.408)\tData 0.000 (0.033)\tLoss 1.7015 (1.6981)\tPrec@1 89.062 (87.929)\n",
            "Epoch: [44][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.6578 (1.6956)\tPrec@1 89.062 (88.052)\n",
            "Epoch: [44][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.7562 (1.6939)\tPrec@1 83.594 (88.151)\n",
            "Epoch: [44][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.6935 (1.6951)\tPrec@1 91.406 (88.126)\n",
            "Epoch: [44][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.6761 (1.6945)\tPrec@1 90.625 (88.128)\n",
            "epoch 44 training time consumed: 151.61s\n",
            "Test: [0/79]\tTime 4.174 (4.174)\tLoss 1.8556 (1.8556)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.8744 (1.9087)\tPrec@1 84.375 (78.983)\n",
            " * Prec@1 78.690\n",
            "current lr 5.78217e-02\n",
            "Epoch: [45][0/391]\tTime 5.644 (5.644)\tData 4.979 (4.979)\tLoss 1.7030 (1.7030)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [45][50/391]\tTime 0.374 (0.478)\tData 0.000 (0.098)\tLoss 1.6453 (1.6699)\tPrec@1 89.062 (89.491)\n",
            "Epoch: [45][100/391]\tTime 0.376 (0.427)\tData 0.000 (0.049)\tLoss 1.6671 (1.6731)\tPrec@1 87.500 (89.264)\n",
            "Epoch: [45][150/391]\tTime 0.374 (0.410)\tData 0.000 (0.033)\tLoss 1.7681 (1.6813)\tPrec@1 84.375 (88.768)\n",
            "Epoch: [45][200/391]\tTime 0.376 (0.401)\tData 0.001 (0.025)\tLoss 1.6803 (1.6814)\tPrec@1 89.062 (88.841)\n",
            "Epoch: [45][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.6863 (1.6837)\tPrec@1 89.062 (88.745)\n",
            "Epoch: [45][300/391]\tTime 0.377 (0.392)\tData 0.000 (0.017)\tLoss 1.7401 (1.6885)\tPrec@1 88.281 (88.504)\n",
            "Epoch: [45][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.014)\tLoss 1.7257 (1.6908)\tPrec@1 85.938 (88.399)\n",
            "epoch 45 training time consumed: 152.01s\n",
            "Test: [0/79]\tTime 5.351 (5.351)\tLoss 1.7111 (1.7111)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.113 (0.216)\tLoss 1.7384 (1.7582)\tPrec@1 83.594 (85.294)\n",
            " * Prec@1 85.340\n",
            "current lr 5.62667e-02\n",
            "Epoch: [46][0/391]\tTime 5.540 (5.540)\tData 4.921 (4.921)\tLoss 1.6604 (1.6604)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [46][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.097)\tLoss 1.7006 (1.6673)\tPrec@1 89.062 (89.629)\n",
            "Epoch: [46][100/391]\tTime 0.376 (0.426)\tData 0.001 (0.049)\tLoss 1.7457 (1.6770)\tPrec@1 85.156 (89.062)\n",
            "Epoch: [46][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.6731 (1.6847)\tPrec@1 87.500 (88.716)\n",
            "Epoch: [46][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.7092 (1.6862)\tPrec@1 89.062 (88.724)\n",
            "Epoch: [46][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 1.6173 (1.6856)\tPrec@1 92.188 (88.720)\n",
            "Epoch: [46][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.6794 (1.6845)\tPrec@1 89.062 (88.772)\n",
            "Epoch: [46][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.014)\tLoss 1.6620 (1.6846)\tPrec@1 90.625 (88.771)\n",
            "epoch 46 training time consumed: 151.97s\n",
            "Test: [0/79]\tTime 4.056 (4.056)\tLoss 1.8012 (1.8012)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 1.8077 (1.8035)\tPrec@1 85.156 (83.869)\n",
            " * Prec@1 83.720\n",
            "current lr 5.47054e-02\n",
            "Epoch: [47][0/391]\tTime 5.590 (5.590)\tData 4.971 (4.971)\tLoss 1.6679 (1.6679)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [47][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.098)\tLoss 1.6347 (1.6835)\tPrec@1 90.625 (88.511)\n",
            "Epoch: [47][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.049)\tLoss 1.7037 (1.6842)\tPrec@1 90.625 (88.660)\n",
            "Epoch: [47][150/391]\tTime 0.374 (0.408)\tData 0.001 (0.033)\tLoss 1.6667 (1.6806)\tPrec@1 88.281 (88.783)\n",
            "Epoch: [47][200/391]\tTime 0.374 (0.400)\tData 0.001 (0.025)\tLoss 1.6372 (1.6808)\tPrec@1 91.406 (88.709)\n",
            "Epoch: [47][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.7136 (1.6802)\tPrec@1 86.719 (88.723)\n",
            "Epoch: [47][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.6685 (1.6814)\tPrec@1 88.281 (88.655)\n",
            "Epoch: [47][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.6737 (1.6830)\tPrec@1 89.062 (88.584)\n",
            "epoch 47 training time consumed: 151.51s\n",
            "Test: [0/79]\tTime 4.155 (4.155)\tLoss 1.7583 (1.7583)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.114 (0.192)\tLoss 1.7922 (1.7454)\tPrec@1 83.594 (86.183)\n",
            " * Prec@1 86.070\n",
            "current lr 5.31395e-02\n",
            "Epoch: [48][0/391]\tTime 5.588 (5.588)\tData 4.957 (4.957)\tLoss 1.6215 (1.6215)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [48][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.097)\tLoss 1.6593 (1.6697)\tPrec@1 92.969 (89.415)\n",
            "Epoch: [48][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.049)\tLoss 1.6566 (1.6620)\tPrec@1 90.625 (89.759)\n",
            "Epoch: [48][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.6856 (1.6673)\tPrec@1 89.844 (89.606)\n",
            "Epoch: [48][200/391]\tTime 0.373 (0.401)\tData 0.000 (0.025)\tLoss 1.7041 (1.6728)\tPrec@1 84.375 (89.272)\n",
            "Epoch: [48][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.7269 (1.6736)\tPrec@1 84.375 (89.255)\n",
            "Epoch: [48][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.7213 (1.6736)\tPrec@1 86.719 (89.268)\n",
            "Epoch: [48][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.6403 (1.6750)\tPrec@1 92.969 (89.221)\n",
            "epoch 48 training time consumed: 151.93s\n",
            "Test: [0/79]\tTime 4.113 (4.113)\tLoss 1.6883 (1.6883)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.7950 (1.7877)\tPrec@1 82.812 (84.865)\n",
            " * Prec@1 84.810\n",
            "current lr 5.15705e-02\n",
            "Epoch: [49][0/391]\tTime 5.599 (5.599)\tData 4.982 (4.982)\tLoss 1.6320 (1.6320)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [49][50/391]\tTime 0.373 (0.476)\tData 0.000 (0.098)\tLoss 1.6694 (1.6560)\tPrec@1 89.062 (89.997)\n",
            "Epoch: [49][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.6448 (1.6657)\tPrec@1 89.844 (89.403)\n",
            "Epoch: [49][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.6037 (1.6641)\tPrec@1 91.406 (89.471)\n",
            "Epoch: [49][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.7438 (1.6664)\tPrec@1 82.812 (89.416)\n",
            "Epoch: [49][250/391]\tTime 0.375 (0.395)\tData 0.001 (0.020)\tLoss 1.6445 (1.6643)\tPrec@1 91.406 (89.576)\n",
            "Epoch: [49][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.7319 (1.6686)\tPrec@1 86.719 (89.319)\n",
            "Epoch: [49][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.7289 (1.6700)\tPrec@1 88.281 (89.281)\n",
            "epoch 49 training time consumed: 151.62s\n",
            "Test: [0/79]\tTime 4.156 (4.156)\tLoss 1.9786 (1.9786)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 2.0906 (1.9270)\tPrec@1 71.875 (78.799)\n",
            " * Prec@1 79.030\n",
            "current lr 5.00000e-02\n",
            "Epoch: [50][0/391]\tTime 6.085 (6.085)\tData 5.557 (5.557)\tLoss 1.6723 (1.6723)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [50][50/391]\tTime 0.375 (0.485)\tData 0.000 (0.109)\tLoss 1.5988 (1.6511)\tPrec@1 92.188 (90.043)\n",
            "Epoch: [50][100/391]\tTime 0.375 (0.431)\tData 0.000 (0.055)\tLoss 1.6293 (1.6653)\tPrec@1 90.625 (89.527)\n",
            "Epoch: [50][150/391]\tTime 0.375 (0.412)\tData 0.001 (0.037)\tLoss 1.6954 (1.6667)\tPrec@1 87.500 (89.513)\n",
            "Epoch: [50][200/391]\tTime 0.374 (0.403)\tData 0.000 (0.028)\tLoss 1.5872 (1.6688)\tPrec@1 95.312 (89.416)\n",
            "Epoch: [50][250/391]\tTime 0.377 (0.397)\tData 0.000 (0.022)\tLoss 1.7279 (1.6695)\tPrec@1 89.844 (89.330)\n",
            "Epoch: [50][300/391]\tTime 0.375 (0.394)\tData 0.000 (0.019)\tLoss 1.7128 (1.6683)\tPrec@1 90.625 (89.410)\n",
            "Epoch: [50][350/391]\tTime 0.376 (0.391)\tData 0.001 (0.016)\tLoss 1.7151 (1.6698)\tPrec@1 89.062 (89.336)\n",
            "epoch 50 training time consumed: 152.43s\n",
            "Test: [0/79]\tTime 4.104 (4.104)\tLoss 1.7534 (1.7534)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.8517 (1.8076)\tPrec@1 83.594 (83.272)\n",
            " * Prec@1 83.370\n",
            "current lr 4.84295e-02\n",
            "Epoch: [51][0/391]\tTime 5.600 (5.600)\tData 4.937 (4.937)\tLoss 1.6041 (1.6041)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [51][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.097)\tLoss 1.6157 (1.6596)\tPrec@1 91.406 (89.798)\n",
            "Epoch: [51][100/391]\tTime 0.376 (0.426)\tData 0.000 (0.049)\tLoss 1.6114 (1.6643)\tPrec@1 96.094 (89.759)\n",
            "Epoch: [51][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.6411 (1.6582)\tPrec@1 87.500 (89.927)\n",
            "Epoch: [51][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.6019 (1.6591)\tPrec@1 91.406 (89.844)\n",
            "Epoch: [51][250/391]\tTime 0.377 (0.396)\tData 0.000 (0.020)\tLoss 1.6251 (1.6577)\tPrec@1 89.844 (89.884)\n",
            "Epoch: [51][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.7198 (1.6585)\tPrec@1 85.938 (89.826)\n",
            "Epoch: [51][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.6521 (1.6603)\tPrec@1 89.844 (89.717)\n",
            "epoch 51 training time consumed: 151.94s\n",
            "Test: [0/79]\tTime 4.122 (4.122)\tLoss 1.7846 (1.7846)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.7863 (1.8221)\tPrec@1 85.938 (83.395)\n",
            " * Prec@1 83.240\n",
            "current lr 4.68605e-02\n",
            "Epoch: [52][0/391]\tTime 5.629 (5.629)\tData 4.966 (4.966)\tLoss 1.6412 (1.6412)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [52][50/391]\tTime 0.374 (0.477)\tData 0.002 (0.098)\tLoss 1.7054 (1.6519)\tPrec@1 87.500 (90.441)\n",
            "Epoch: [52][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.6069 (1.6527)\tPrec@1 92.969 (90.145)\n",
            "Epoch: [52][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.6777 (1.6538)\tPrec@1 88.281 (90.020)\n",
            "Epoch: [52][200/391]\tTime 0.372 (0.400)\tData 0.000 (0.025)\tLoss 1.6562 (1.6577)\tPrec@1 89.062 (89.793)\n",
            "Epoch: [52][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.7219 (1.6586)\tPrec@1 87.500 (89.769)\n",
            "Epoch: [52][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.6695 (1.6584)\tPrec@1 89.062 (89.815)\n",
            "Epoch: [52][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.6296 (1.6598)\tPrec@1 89.844 (89.806)\n",
            "epoch 52 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.184 (4.184)\tLoss 1.8848 (1.8848)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.8464 (1.8621)\tPrec@1 82.031 (80.944)\n",
            " * Prec@1 81.130\n",
            "current lr 4.52946e-02\n",
            "Epoch: [53][0/391]\tTime 5.550 (5.550)\tData 4.932 (4.932)\tLoss 1.6273 (1.6273)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [53][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.097)\tLoss 1.6039 (1.6477)\tPrec@1 93.750 (90.303)\n",
            "Epoch: [53][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.5906 (1.6457)\tPrec@1 94.531 (90.370)\n",
            "Epoch: [53][150/391]\tTime 0.377 (0.409)\tData 0.000 (0.033)\tLoss 1.6160 (1.6447)\tPrec@1 92.969 (90.501)\n",
            "Epoch: [53][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.5775 (1.6493)\tPrec@1 94.531 (90.264)\n",
            "Epoch: [53][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.6326 (1.6527)\tPrec@1 89.062 (90.037)\n",
            "Epoch: [53][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.7219 (1.6541)\tPrec@1 87.500 (89.927)\n",
            "Epoch: [53][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.6343 (1.6541)\tPrec@1 89.062 (89.968)\n",
            "epoch 53 training time consumed: 151.79s\n",
            "Test: [0/79]\tTime 4.165 (4.165)\tLoss 1.7062 (1.7062)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.7592 (1.7464)\tPrec@1 87.500 (86.259)\n",
            " * Prec@1 86.470\n",
            "current lr 4.37333e-02\n",
            "Epoch: [54][0/391]\tTime 5.646 (5.646)\tData 4.969 (4.969)\tLoss 1.6114 (1.6114)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [54][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.5910 (1.6572)\tPrec@1 92.969 (89.905)\n",
            "Epoch: [54][100/391]\tTime 0.373 (0.426)\tData 0.001 (0.049)\tLoss 1.6827 (1.6528)\tPrec@1 89.062 (90.084)\n",
            "Epoch: [54][150/391]\tTime 0.375 (0.409)\tData 0.001 (0.033)\tLoss 1.6206 (1.6493)\tPrec@1 92.188 (90.371)\n",
            "Epoch: [54][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.6119 (1.6462)\tPrec@1 91.406 (90.431)\n",
            "Epoch: [54][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.6847 (1.6477)\tPrec@1 86.719 (90.336)\n",
            "Epoch: [54][300/391]\tTime 0.377 (0.391)\tData 0.000 (0.017)\tLoss 1.6950 (1.6495)\tPrec@1 86.719 (90.230)\n",
            "Epoch: [54][350/391]\tTime 0.376 (0.389)\tData 0.001 (0.014)\tLoss 1.6664 (1.6516)\tPrec@1 91.406 (90.089)\n",
            "epoch 54 training time consumed: 151.63s\n",
            "Test: [0/79]\tTime 4.097 (4.097)\tLoss 1.9067 (1.9067)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.8706 (1.8596)\tPrec@1 80.469 (81.051)\n",
            " * Prec@1 81.070\n",
            "current lr 4.21783e-02\n",
            "Epoch: [55][0/391]\tTime 5.594 (5.594)\tData 4.949 (4.949)\tLoss 1.6399 (1.6399)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [55][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.097)\tLoss 1.5534 (1.6233)\tPrec@1 95.312 (91.268)\n",
            "Epoch: [55][100/391]\tTime 0.375 (0.427)\tData 0.001 (0.049)\tLoss 1.6811 (1.6276)\tPrec@1 87.500 (91.190)\n",
            "Epoch: [55][150/391]\tTime 0.375 (0.410)\tData 0.000 (0.033)\tLoss 1.5421 (1.6275)\tPrec@1 95.312 (91.215)\n",
            "Epoch: [55][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.6202 (1.6323)\tPrec@1 94.531 (91.014)\n",
            "Epoch: [55][250/391]\tTime 0.376 (0.396)\tData 0.001 (0.020)\tLoss 1.5851 (1.6371)\tPrec@1 93.750 (90.715)\n",
            "Epoch: [55][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.6534 (1.6403)\tPrec@1 91.406 (90.609)\n",
            "Epoch: [55][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.6314 (1.6406)\tPrec@1 92.969 (90.543)\n",
            "epoch 55 training time consumed: 151.94s\n",
            "Test: [0/79]\tTime 4.117 (4.117)\tLoss 1.7929 (1.7929)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.8097 (1.8006)\tPrec@1 85.938 (83.778)\n",
            " * Prec@1 84.120\n",
            "current lr 4.06309e-02\n",
            "Epoch: [56][0/391]\tTime 5.581 (5.581)\tData 5.018 (5.018)\tLoss 1.6545 (1.6545)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [56][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.099)\tLoss 1.6574 (1.6306)\tPrec@1 87.500 (91.023)\n",
            "Epoch: [56][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.050)\tLoss 1.5814 (1.6317)\tPrec@1 93.750 (91.058)\n",
            "Epoch: [56][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.6096 (1.6361)\tPrec@1 92.188 (90.832)\n",
            "Epoch: [56][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.5927 (1.6371)\tPrec@1 92.969 (90.780)\n",
            "Epoch: [56][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.7445 (1.6393)\tPrec@1 84.375 (90.647)\n",
            "Epoch: [56][300/391]\tTime 0.372 (0.392)\tData 0.000 (0.017)\tLoss 1.5968 (1.6386)\tPrec@1 92.188 (90.690)\n",
            "Epoch: [56][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.015)\tLoss 1.6814 (1.6384)\tPrec@1 91.406 (90.652)\n",
            "epoch 56 training time consumed: 151.69s\n",
            "Test: [0/79]\tTime 4.177 (4.177)\tLoss 1.6547 (1.6547)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.7637 (1.7390)\tPrec@1 86.719 (86.183)\n",
            " * Prec@1 86.290\n",
            "current lr 3.90928e-02\n",
            "Epoch: [57][0/391]\tTime 5.560 (5.560)\tData 4.947 (4.947)\tLoss 1.6866 (1.6866)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [57][50/391]\tTime 0.375 (0.476)\tData 0.001 (0.097)\tLoss 1.6817 (1.6279)\tPrec@1 87.500 (90.794)\n",
            "Epoch: [57][100/391]\tTime 0.372 (0.425)\tData 0.000 (0.049)\tLoss 1.6580 (1.6238)\tPrec@1 90.625 (91.089)\n",
            "Epoch: [57][150/391]\tTime 0.374 (0.408)\tData 0.000 (0.033)\tLoss 1.7091 (1.6262)\tPrec@1 89.062 (91.029)\n",
            "Epoch: [57][200/391]\tTime 0.372 (0.399)\tData 0.000 (0.025)\tLoss 1.6088 (1.6290)\tPrec@1 91.406 (90.951)\n",
            "Epoch: [57][250/391]\tTime 0.375 (0.394)\tData 0.000 (0.020)\tLoss 1.5915 (1.6290)\tPrec@1 93.750 (91.051)\n",
            "Epoch: [57][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.6725 (1.6295)\tPrec@1 90.625 (91.053)\n",
            "Epoch: [57][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.6655 (1.6304)\tPrec@1 91.406 (90.988)\n",
            "epoch 57 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.113 (4.113)\tLoss 1.7423 (1.7423)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.9785 (1.8625)\tPrec@1 72.656 (82.016)\n",
            " * Prec@1 81.670\n",
            "current lr 3.75655e-02\n",
            "Epoch: [58][0/391]\tTime 5.587 (5.587)\tData 4.978 (4.978)\tLoss 1.6126 (1.6126)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [58][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.098)\tLoss 1.6531 (1.6228)\tPrec@1 89.844 (91.468)\n",
            "Epoch: [58][100/391]\tTime 0.374 (0.427)\tData 0.000 (0.050)\tLoss 1.6085 (1.6246)\tPrec@1 92.188 (91.460)\n",
            "Epoch: [58][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.5984 (1.6232)\tPrec@1 93.750 (91.515)\n",
            "Epoch: [58][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.6306 (1.6225)\tPrec@1 89.062 (91.535)\n",
            "Epoch: [58][250/391]\tTime 0.372 (0.396)\tData 0.000 (0.020)\tLoss 1.5426 (1.6223)\tPrec@1 96.094 (91.497)\n",
            "Epoch: [58][300/391]\tTime 0.373 (0.392)\tData 0.001 (0.017)\tLoss 1.6541 (1.6244)\tPrec@1 89.844 (91.367)\n",
            "Epoch: [58][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.6914 (1.6267)\tPrec@1 91.406 (91.246)\n",
            "epoch 58 training time consumed: 151.69s\n",
            "Test: [0/79]\tTime 4.195 (4.195)\tLoss 1.6946 (1.6946)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.8104 (1.7354)\tPrec@1 80.469 (86.550)\n",
            " * Prec@1 86.510\n",
            "current lr 3.60504e-02\n",
            "Epoch: [59][0/391]\tTime 5.553 (5.553)\tData 4.925 (4.925)\tLoss 1.5415 (1.5415)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [59][50/391]\tTime 0.375 (0.475)\tData 0.002 (0.097)\tLoss 1.6279 (1.6008)\tPrec@1 92.188 (92.249)\n",
            "Epoch: [59][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.049)\tLoss 1.7027 (1.6095)\tPrec@1 86.719 (92.033)\n",
            "Epoch: [59][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.6114 (1.6177)\tPrec@1 91.406 (91.737)\n",
            "Epoch: [59][200/391]\tTime 0.373 (0.399)\tData 0.000 (0.025)\tLoss 1.6540 (1.6191)\tPrec@1 86.719 (91.589)\n",
            "Epoch: [59][250/391]\tTime 0.372 (0.395)\tData 0.000 (0.020)\tLoss 1.7091 (1.6202)\tPrec@1 85.938 (91.497)\n",
            "Epoch: [59][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.6607 (1.6214)\tPrec@1 91.406 (91.489)\n",
            "Epoch: [59][350/391]\tTime 0.373 (0.389)\tData 0.000 (0.014)\tLoss 1.6636 (1.6220)\tPrec@1 87.500 (91.464)\n",
            "epoch 59 training time consumed: 151.57s\n",
            "Test: [0/79]\tTime 4.067 (4.067)\tLoss 1.8141 (1.8141)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.7333 (1.7561)\tPrec@1 89.062 (86.290)\n",
            " * Prec@1 86.120\n",
            "current lr 3.45492e-02\n",
            "Epoch: [60][0/391]\tTime 5.608 (5.608)\tData 4.992 (4.992)\tLoss 1.6573 (1.6573)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [60][50/391]\tTime 0.373 (0.477)\tData 0.001 (0.098)\tLoss 1.5555 (1.5957)\tPrec@1 95.312 (93.168)\n",
            "Epoch: [60][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.050)\tLoss 1.5943 (1.6006)\tPrec@1 92.188 (92.683)\n",
            "Epoch: [60][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.6687 (1.6037)\tPrec@1 91.406 (92.405)\n",
            "Epoch: [60][200/391]\tTime 0.374 (0.400)\tData 0.001 (0.025)\tLoss 1.6400 (1.6115)\tPrec@1 87.500 (91.966)\n",
            "Epoch: [60][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.6597 (1.6141)\tPrec@1 89.062 (91.808)\n",
            "Epoch: [60][300/391]\tTime 0.372 (0.392)\tData 0.000 (0.017)\tLoss 1.5510 (1.6143)\tPrec@1 94.531 (91.770)\n",
            "Epoch: [60][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.5871 (1.6167)\tPrec@1 92.969 (91.611)\n",
            "epoch 60 training time consumed: 151.57s\n",
            "Test: [0/79]\tTime 4.077 (4.077)\tLoss 1.9095 (1.9095)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.9323 (1.8851)\tPrec@1 78.125 (80.944)\n",
            " * Prec@1 81.340\n",
            "current lr 3.30631e-02\n",
            "Epoch: [61][0/391]\tTime 5.620 (5.620)\tData 4.952 (4.952)\tLoss 1.6883 (1.6883)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [61][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.097)\tLoss 1.6431 (1.6058)\tPrec@1 90.625 (92.111)\n",
            "Epoch: [61][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.5619 (1.6022)\tPrec@1 92.969 (92.427)\n",
            "Epoch: [61][150/391]\tTime 0.375 (0.409)\tData 0.001 (0.033)\tLoss 1.5749 (1.6014)\tPrec@1 93.750 (92.446)\n",
            "Epoch: [61][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.5741 (1.6044)\tPrec@1 95.312 (92.254)\n",
            "Epoch: [61][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.5974 (1.6074)\tPrec@1 92.969 (92.088)\n",
            "Epoch: [61][300/391]\tTime 0.373 (0.391)\tData 0.000 (0.017)\tLoss 1.6161 (1.6097)\tPrec@1 90.625 (91.985)\n",
            "Epoch: [61][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.5531 (1.6099)\tPrec@1 96.875 (92.012)\n",
            "epoch 61 training time consumed: 151.51s\n",
            "Test: [0/79]\tTime 4.202 (4.202)\tLoss 1.6730 (1.6730)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.6696 (1.6985)\tPrec@1 87.500 (87.868)\n",
            " * Prec@1 88.010\n",
            "current lr 3.15938e-02\n",
            "Epoch: [62][0/391]\tTime 5.561 (5.561)\tData 4.908 (4.908)\tLoss 1.5536 (1.5536)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [62][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.097)\tLoss 1.5549 (1.5849)\tPrec@1 95.312 (93.214)\n",
            "Epoch: [62][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.049)\tLoss 1.6022 (1.5853)\tPrec@1 89.844 (93.007)\n",
            "Epoch: [62][150/391]\tTime 0.376 (0.408)\tData 0.000 (0.033)\tLoss 1.5732 (1.5907)\tPrec@1 93.750 (92.829)\n",
            "Epoch: [62][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.6937 (1.5988)\tPrec@1 86.719 (92.522)\n",
            "Epoch: [62][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.6176 (1.6017)\tPrec@1 91.406 (92.331)\n",
            "Epoch: [62][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.016)\tLoss 1.6138 (1.6034)\tPrec@1 92.188 (92.273)\n",
            "Epoch: [62][350/391]\tTime 0.376 (0.389)\tData 0.001 (0.014)\tLoss 1.5530 (1.6040)\tPrec@1 93.750 (92.281)\n",
            "epoch 62 training time consumed: 151.70s\n",
            "Test: [0/79]\tTime 4.119 (4.119)\tLoss 1.7326 (1.7326)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.6655 (1.6991)\tPrec@1 90.625 (88.680)\n",
            " * Prec@1 88.570\n",
            "current lr 3.01426e-02\n",
            "Epoch: [63][0/391]\tTime 5.586 (5.586)\tData 4.974 (4.974)\tLoss 1.5703 (1.5703)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [63][50/391]\tTime 0.375 (0.477)\tData 0.001 (0.098)\tLoss 1.6640 (1.5833)\tPrec@1 87.500 (93.045)\n",
            "Epoch: [63][100/391]\tTime 0.375 (0.427)\tData 0.000 (0.049)\tLoss 1.6668 (1.5856)\tPrec@1 90.625 (93.162)\n",
            "Epoch: [63][150/391]\tTime 0.372 (0.409)\tData 0.000 (0.033)\tLoss 1.6575 (1.5897)\tPrec@1 89.062 (92.917)\n",
            "Epoch: [63][200/391]\tTime 0.375 (0.400)\tData 0.001 (0.025)\tLoss 1.5642 (1.5922)\tPrec@1 93.750 (92.802)\n",
            "Epoch: [63][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.6248 (1.5951)\tPrec@1 91.406 (92.664)\n",
            "Epoch: [63][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.6453 (1.5979)\tPrec@1 90.625 (92.569)\n",
            "Epoch: [63][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.5517 (1.5985)\tPrec@1 92.969 (92.521)\n",
            "epoch 63 training time consumed: 151.59s\n",
            "Test: [0/79]\tTime 4.110 (4.110)\tLoss 1.6199 (1.6199)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.7292 (1.7193)\tPrec@1 87.500 (87.286)\n",
            " * Prec@1 87.550\n",
            "current lr 2.87110e-02\n",
            "Epoch: [64][0/391]\tTime 5.556 (5.556)\tData 4.932 (4.932)\tLoss 1.5229 (1.5229)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [64][50/391]\tTime 0.373 (0.475)\tData 0.001 (0.097)\tLoss 1.5815 (1.5878)\tPrec@1 93.750 (92.846)\n",
            "Epoch: [64][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.049)\tLoss 1.5925 (1.5829)\tPrec@1 93.750 (93.178)\n",
            "Epoch: [64][150/391]\tTime 0.376 (0.408)\tData 0.000 (0.033)\tLoss 1.6268 (1.5838)\tPrec@1 90.625 (93.222)\n",
            "Epoch: [64][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.5844 (1.5846)\tPrec@1 91.406 (93.179)\n",
            "Epoch: [64][250/391]\tTime 0.375 (0.395)\tData 0.001 (0.020)\tLoss 1.5539 (1.5857)\tPrec@1 96.094 (93.171)\n",
            "Epoch: [64][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.5799 (1.5871)\tPrec@1 91.406 (93.127)\n",
            "Epoch: [64][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.6562 (1.5892)\tPrec@1 88.281 (93.024)\n",
            "epoch 64 training time consumed: 151.74s\n",
            "Test: [0/79]\tTime 4.081 (4.081)\tLoss 1.6876 (1.6876)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.112 (0.191)\tLoss 1.6434 (1.6889)\tPrec@1 91.406 (88.419)\n",
            " * Prec@1 88.560\n",
            "current lr 2.73005e-02\n",
            "Epoch: [65][0/391]\tTime 5.615 (5.615)\tData 4.997 (4.997)\tLoss 1.5236 (1.5236)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [65][50/391]\tTime 0.373 (0.478)\tData 0.000 (0.098)\tLoss 1.5562 (1.5702)\tPrec@1 95.312 (93.949)\n",
            "Epoch: [65][100/391]\tTime 0.376 (0.427)\tData 0.000 (0.050)\tLoss 1.6451 (1.5748)\tPrec@1 88.281 (93.533)\n",
            "Epoch: [65][150/391]\tTime 0.379 (0.410)\tData 0.003 (0.033)\tLoss 1.5411 (1.5769)\tPrec@1 96.094 (93.377)\n",
            "Epoch: [65][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.6187 (1.5785)\tPrec@1 91.406 (93.326)\n",
            "Epoch: [65][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 1.6039 (1.5835)\tPrec@1 94.531 (93.084)\n",
            "Epoch: [65][300/391]\tTime 0.374 (0.392)\tData 0.001 (0.017)\tLoss 1.5547 (1.5846)\tPrec@1 94.531 (92.974)\n",
            "Epoch: [65][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.014)\tLoss 1.5683 (1.5841)\tPrec@1 92.188 (92.969)\n",
            "epoch 65 training time consumed: 151.91s\n",
            "Test: [0/79]\tTime 4.134 (4.134)\tLoss 1.6465 (1.6465)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.6362 (1.6900)\tPrec@1 91.406 (88.817)\n",
            " * Prec@1 88.920\n",
            "current lr 2.59123e-02\n",
            "Epoch: [66][0/391]\tTime 5.596 (5.596)\tData 4.940 (4.940)\tLoss 1.5647 (1.5647)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [66][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.097)\tLoss 1.5502 (1.5753)\tPrec@1 93.750 (93.719)\n",
            "Epoch: [66][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.5541 (1.5768)\tPrec@1 94.531 (93.487)\n",
            "Epoch: [66][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.5281 (1.5711)\tPrec@1 94.531 (93.843)\n",
            "Epoch: [66][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.6006 (1.5732)\tPrec@1 92.188 (93.703)\n",
            "Epoch: [66][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.5267 (1.5764)\tPrec@1 97.656 (93.566)\n",
            "Epoch: [66][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.7026 (1.5786)\tPrec@1 87.500 (93.420)\n",
            "Epoch: [66][350/391]\tTime 0.374 (0.389)\tData 0.001 (0.014)\tLoss 1.5685 (1.5801)\tPrec@1 92.969 (93.347)\n",
            "epoch 66 training time consumed: 151.47s\n",
            "Test: [0/79]\tTime 4.238 (4.238)\tLoss 1.6292 (1.6292)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.114 (0.194)\tLoss 1.6691 (1.6834)\tPrec@1 88.281 (88.787)\n",
            " * Prec@1 89.000\n",
            "current lr 2.45479e-02\n",
            "Epoch: [67][0/391]\tTime 5.731 (5.731)\tData 5.044 (5.044)\tLoss 1.5779 (1.5779)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [67][50/391]\tTime 0.374 (0.479)\tData 0.001 (0.099)\tLoss 1.6100 (1.5599)\tPrec@1 91.406 (94.210)\n",
            "Epoch: [67][100/391]\tTime 0.373 (0.427)\tData 0.000 (0.050)\tLoss 1.6269 (1.5617)\tPrec@1 92.969 (94.268)\n",
            "Epoch: [67][150/391]\tTime 0.374 (0.410)\tData 0.000 (0.034)\tLoss 1.6456 (1.5657)\tPrec@1 89.844 (94.014)\n",
            "Epoch: [67][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.6377 (1.5666)\tPrec@1 89.062 (94.014)\n",
            "Epoch: [67][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 1.5489 (1.5681)\tPrec@1 94.531 (93.937)\n",
            "Epoch: [67][300/391]\tTime 0.374 (0.393)\tData 0.000 (0.017)\tLoss 1.5925 (1.5693)\tPrec@1 92.969 (93.838)\n",
            "Epoch: [67][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.015)\tLoss 1.6101 (1.5711)\tPrec@1 92.969 (93.723)\n",
            "epoch 67 training time consumed: 151.94s\n",
            "Test: [0/79]\tTime 4.108 (4.108)\tLoss 1.6964 (1.6964)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.6855 (1.6917)\tPrec@1 86.719 (88.634)\n",
            " * Prec@1 88.440\n",
            "current lr 2.32087e-02\n",
            "Epoch: [68][0/391]\tTime 5.541 (5.541)\tData 4.956 (4.956)\tLoss 1.5988 (1.5988)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [68][50/391]\tTime 0.372 (0.475)\tData 0.000 (0.097)\tLoss 1.5675 (1.5645)\tPrec@1 92.969 (94.056)\n",
            "Epoch: [68][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.049)\tLoss 1.5357 (1.5596)\tPrec@1 95.312 (94.315)\n",
            "Epoch: [68][150/391]\tTime 0.373 (0.408)\tData 0.000 (0.033)\tLoss 1.5235 (1.5561)\tPrec@1 96.094 (94.469)\n",
            "Epoch: [68][200/391]\tTime 0.374 (0.400)\tData 0.001 (0.025)\tLoss 1.4981 (1.5564)\tPrec@1 96.094 (94.380)\n",
            "Epoch: [68][250/391]\tTime 0.374 (0.394)\tData 0.000 (0.020)\tLoss 1.6180 (1.5588)\tPrec@1 92.188 (94.257)\n",
            "Epoch: [68][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.5058 (1.5603)\tPrec@1 96.875 (94.194)\n",
            "Epoch: [68][350/391]\tTime 0.376 (0.389)\tData 0.000 (0.014)\tLoss 1.5756 (1.5606)\tPrec@1 92.969 (94.193)\n",
            "epoch 68 training time consumed: 151.39s\n",
            "Test: [0/79]\tTime 4.044 (4.044)\tLoss 1.6076 (1.6076)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.6200 (1.6898)\tPrec@1 91.406 (89.170)\n",
            " * Prec@1 88.780\n",
            "current lr 2.18958e-02\n",
            "Epoch: [69][0/391]\tTime 5.570 (5.570)\tData 4.957 (4.957)\tLoss 1.5481 (1.5481)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [69][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.097)\tLoss 1.5503 (1.5532)\tPrec@1 93.750 (94.317)\n",
            "Epoch: [69][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.049)\tLoss 1.5299 (1.5498)\tPrec@1 95.312 (94.531)\n",
            "Epoch: [69][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.5245 (1.5527)\tPrec@1 95.312 (94.361)\n",
            "Epoch: [69][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.5439 (1.5544)\tPrec@1 96.875 (94.310)\n",
            "Epoch: [69][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.5404 (1.5552)\tPrec@1 94.531 (94.304)\n",
            "Epoch: [69][300/391]\tTime 0.373 (0.392)\tData 0.000 (0.017)\tLoss 1.5525 (1.5563)\tPrec@1 93.750 (94.251)\n",
            "Epoch: [69][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.5124 (1.5571)\tPrec@1 96.094 (94.264)\n",
            "epoch 69 training time consumed: 151.89s\n",
            "Test: [0/79]\tTime 4.187 (4.187)\tLoss 1.6595 (1.6595)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.5925 (1.6644)\tPrec@1 91.406 (89.828)\n",
            " * Prec@1 89.690\n",
            "current lr 2.06107e-02\n",
            "Epoch: [70][0/391]\tTime 5.553 (5.553)\tData 4.928 (4.928)\tLoss 1.6114 (1.6114)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [70][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.097)\tLoss 1.5103 (1.5421)\tPrec@1 98.438 (95.129)\n",
            "Epoch: [70][100/391]\tTime 0.376 (0.426)\tData 0.001 (0.049)\tLoss 1.5372 (1.5397)\tPrec@1 93.750 (95.227)\n",
            "Epoch: [70][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.5515 (1.5427)\tPrec@1 93.750 (95.059)\n",
            "Epoch: [70][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.5555 (1.5423)\tPrec@1 92.969 (95.157)\n",
            "Epoch: [70][250/391]\tTime 0.375 (0.396)\tData 0.001 (0.020)\tLoss 1.5346 (1.5420)\tPrec@1 94.531 (95.132)\n",
            "Epoch: [70][300/391]\tTime 0.377 (0.392)\tData 0.003 (0.017)\tLoss 1.5730 (1.5437)\tPrec@1 92.969 (95.011)\n",
            "Epoch: [70][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.5800 (1.5452)\tPrec@1 93.750 (94.903)\n",
            "epoch 70 training time consumed: 151.84s\n",
            "Test: [0/79]\tTime 4.225 (4.225)\tLoss 1.6005 (1.6005)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 1.6195 (1.6619)\tPrec@1 91.406 (90.089)\n",
            " * Prec@1 89.760\n",
            "current lr 1.93546e-02\n",
            "Epoch: [71][0/391]\tTime 5.552 (5.552)\tData 4.955 (4.955)\tLoss 1.5868 (1.5868)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [71][50/391]\tTime 0.372 (0.475)\tData 0.000 (0.097)\tLoss 1.5354 (1.5287)\tPrec@1 94.531 (95.512)\n",
            "Epoch: [71][100/391]\tTime 0.372 (0.425)\tData 0.000 (0.049)\tLoss 1.5353 (1.5275)\tPrec@1 94.531 (95.668)\n",
            "Epoch: [71][150/391]\tTime 0.373 (0.408)\tData 0.001 (0.033)\tLoss 1.5380 (1.5302)\tPrec@1 96.094 (95.582)\n",
            "Epoch: [71][200/391]\tTime 0.359 (0.399)\tData 0.000 (0.025)\tLoss 1.4938 (1.5312)\tPrec@1 98.438 (95.553)\n",
            "Epoch: [71][250/391]\tTime 0.374 (0.394)\tData 0.000 (0.020)\tLoss 1.6158 (1.5336)\tPrec@1 92.188 (95.431)\n",
            "Epoch: [71][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.5902 (1.5361)\tPrec@1 94.531 (95.302)\n",
            "Epoch: [71][350/391]\tTime 0.375 (0.388)\tData 0.000 (0.014)\tLoss 1.5506 (1.5382)\tPrec@1 94.531 (95.199)\n",
            "epoch 71 training time consumed: 151.33s\n",
            "Test: [0/79]\tTime 4.133 (4.133)\tLoss 1.6254 (1.6254)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.6332 (1.6426)\tPrec@1 90.625 (90.717)\n",
            " * Prec@1 90.570\n",
            "current lr 1.81288e-02\n",
            "Epoch: [72][0/391]\tTime 5.542 (5.542)\tData 4.951 (4.951)\tLoss 1.5313 (1.5313)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [72][50/391]\tTime 0.375 (0.476)\tData 0.001 (0.097)\tLoss 1.4748 (1.5333)\tPrec@1 97.656 (95.573)\n",
            "Epoch: [72][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.049)\tLoss 1.5327 (1.5314)\tPrec@1 95.312 (95.537)\n",
            "Epoch: [72][150/391]\tTime 0.373 (0.409)\tData 0.001 (0.033)\tLoss 1.5094 (1.5286)\tPrec@1 96.094 (95.680)\n",
            "Epoch: [72][200/391]\tTime 0.375 (0.401)\tData 0.001 (0.025)\tLoss 1.5206 (1.5296)\tPrec@1 95.312 (95.674)\n",
            "Epoch: [72][250/391]\tTime 0.374 (0.395)\tData 0.000 (0.020)\tLoss 1.5225 (1.5304)\tPrec@1 96.875 (95.639)\n",
            "Epoch: [72][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.5058 (1.5308)\tPrec@1 96.094 (95.624)\n",
            "Epoch: [72][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.5437 (1.5325)\tPrec@1 96.875 (95.513)\n",
            "epoch 72 training time consumed: 151.76s\n",
            "Test: [0/79]\tTime 4.042 (4.042)\tLoss 1.5771 (1.5771)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.6297 (1.6357)\tPrec@1 91.406 (91.039)\n",
            " * Prec@1 90.960\n",
            "current lr 1.69344e-02\n",
            "Epoch: [73][0/391]\tTime 5.624 (5.624)\tData 4.950 (4.950)\tLoss 1.5391 (1.5391)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [73][50/391]\tTime 0.375 (0.477)\tData 0.000 (0.097)\tLoss 1.5272 (1.5236)\tPrec@1 96.094 (95.879)\n",
            "Epoch: [73][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.5260 (1.5235)\tPrec@1 95.312 (95.746)\n",
            "Epoch: [73][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.5498 (1.5247)\tPrec@1 91.406 (95.721)\n",
            "Epoch: [73][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.5594 (1.5243)\tPrec@1 93.750 (95.728)\n",
            "Epoch: [73][250/391]\tTime 0.371 (0.395)\tData 0.000 (0.020)\tLoss 1.4951 (1.5250)\tPrec@1 97.656 (95.705)\n",
            "Epoch: [73][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.4974 (1.5254)\tPrec@1 96.875 (95.704)\n",
            "Epoch: [73][350/391]\tTime 0.373 (0.389)\tData 0.000 (0.014)\tLoss 1.5087 (1.5264)\tPrec@1 96.875 (95.695)\n",
            "epoch 73 training time consumed: 151.52s\n",
            "Test: [0/79]\tTime 4.193 (4.193)\tLoss 1.5625 (1.5625)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.6047 (1.6260)\tPrec@1 91.406 (91.651)\n",
            " * Prec@1 91.220\n",
            "current lr 1.57726e-02\n",
            "Epoch: [74][0/391]\tTime 5.546 (5.546)\tData 4.953 (4.953)\tLoss 1.4873 (1.4873)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [74][50/391]\tTime 0.374 (0.476)\tData 0.001 (0.097)\tLoss 1.5321 (1.5101)\tPrec@1 94.531 (96.477)\n",
            "Epoch: [74][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.5069 (1.5092)\tPrec@1 98.438 (96.581)\n",
            "Epoch: [74][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.5006 (1.5117)\tPrec@1 95.312 (96.425)\n",
            "Epoch: [74][200/391]\tTime 0.377 (0.401)\tData 0.000 (0.025)\tLoss 1.5496 (1.5141)\tPrec@1 96.094 (96.292)\n",
            "Epoch: [74][250/391]\tTime 0.375 (0.396)\tData 0.001 (0.020)\tLoss 1.5319 (1.5148)\tPrec@1 95.312 (96.246)\n",
            "Epoch: [74][300/391]\tTime 0.375 (0.392)\tData 0.001 (0.017)\tLoss 1.5312 (1.5168)\tPrec@1 95.312 (96.172)\n",
            "Epoch: [74][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.5439 (1.5172)\tPrec@1 96.875 (96.174)\n",
            "epoch 74 training time consumed: 151.89s\n",
            "Test: [0/79]\tTime 4.154 (4.154)\tLoss 1.6109 (1.6109)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.6291 (1.6236)\tPrec@1 89.844 (91.636)\n",
            " * Prec@1 91.760\n",
            "current lr 1.46447e-02\n",
            "Epoch: [75][0/391]\tTime 5.556 (5.556)\tData 4.932 (4.932)\tLoss 1.4960 (1.4960)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [75][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.097)\tLoss 1.5406 (1.5052)\tPrec@1 95.312 (96.814)\n",
            "Epoch: [75][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.049)\tLoss 1.4967 (1.5047)\tPrec@1 96.875 (96.774)\n",
            "Epoch: [75][150/391]\tTime 0.374 (0.409)\tData 0.001 (0.033)\tLoss 1.5160 (1.5057)\tPrec@1 96.094 (96.772)\n",
            "Epoch: [75][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.5019 (1.5065)\tPrec@1 97.656 (96.712)\n",
            "Epoch: [75][250/391]\tTime 0.375 (0.396)\tData 0.001 (0.020)\tLoss 1.5126 (1.5061)\tPrec@1 96.875 (96.688)\n",
            "Epoch: [75][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4975 (1.5079)\tPrec@1 98.438 (96.592)\n",
            "Epoch: [75][350/391]\tTime 0.372 (0.390)\tData 0.000 (0.014)\tLoss 1.4751 (1.5092)\tPrec@1 97.656 (96.521)\n",
            "epoch 75 training time consumed: 151.79s\n",
            "Test: [0/79]\tTime 4.036 (4.036)\tLoss 1.5871 (1.5871)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.5687 (1.5974)\tPrec@1 93.750 (92.509)\n",
            " * Prec@1 92.550\n",
            "current lr 1.35516e-02\n",
            "Epoch: [76][0/391]\tTime 5.607 (5.607)\tData 4.961 (4.961)\tLoss 1.4770 (1.4770)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [76][50/391]\tTime 0.373 (0.476)\tData 0.000 (0.097)\tLoss 1.4706 (1.4949)\tPrec@1 98.438 (97.151)\n",
            "Epoch: [76][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.4569 (1.4934)\tPrec@1 99.219 (97.239)\n",
            "Epoch: [76][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.4744 (1.4931)\tPrec@1 98.438 (97.211)\n",
            "Epoch: [76][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.4784 (1.4945)\tPrec@1 98.438 (97.143)\n",
            "Epoch: [76][250/391]\tTime 0.375 (0.395)\tData 0.001 (0.020)\tLoss 1.5282 (1.4960)\tPrec@1 92.188 (97.034)\n",
            "Epoch: [76][300/391]\tTime 0.375 (0.391)\tData 0.000 (0.017)\tLoss 1.5048 (1.4973)\tPrec@1 97.656 (96.976)\n",
            "Epoch: [76][350/391]\tTime 0.376 (0.389)\tData 0.000 (0.014)\tLoss 1.5278 (1.4995)\tPrec@1 96.094 (96.864)\n",
            "epoch 76 training time consumed: 151.59s\n",
            "Test: [0/79]\tTime 4.102 (4.102)\tLoss 1.5685 (1.5685)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.6001 (1.6161)\tPrec@1 92.188 (92.065)\n",
            " * Prec@1 92.360\n",
            "current lr 1.24944e-02\n",
            "Epoch: [77][0/391]\tTime 5.572 (5.572)\tData 4.946 (4.946)\tLoss 1.5165 (1.5165)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [77][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.097)\tLoss 1.5255 (1.4922)\tPrec@1 95.312 (97.396)\n",
            "Epoch: [77][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.4791 (1.4881)\tPrec@1 97.656 (97.556)\n",
            "Epoch: [77][150/391]\tTime 0.369 (0.409)\tData 0.000 (0.033)\tLoss 1.4703 (1.4899)\tPrec@1 98.438 (97.506)\n",
            "Epoch: [77][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.4734 (1.4923)\tPrec@1 98.438 (97.361)\n",
            "Epoch: [77][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4802 (1.4920)\tPrec@1 97.656 (97.361)\n",
            "Epoch: [77][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.4550 (1.4920)\tPrec@1 100.000 (97.389)\n",
            "Epoch: [77][350/391]\tTime 0.377 (0.390)\tData 0.001 (0.014)\tLoss 1.5425 (1.4944)\tPrec@1 96.094 (97.256)\n",
            "epoch 77 training time consumed: 151.79s\n",
            "Test: [0/79]\tTime 4.042 (4.042)\tLoss 1.5832 (1.5832)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 1.5632 (1.6231)\tPrec@1 94.531 (91.636)\n",
            " * Prec@1 91.790\n",
            "current lr 1.14743e-02\n",
            "Epoch: [78][0/391]\tTime 5.623 (5.623)\tData 4.998 (4.998)\tLoss 1.4730 (1.4730)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [78][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.5012 (1.4849)\tPrec@1 96.875 (97.580)\n",
            "Epoch: [78][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.050)\tLoss 1.5574 (1.4833)\tPrec@1 93.750 (97.664)\n",
            "Epoch: [78][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.4654 (1.4835)\tPrec@1 98.438 (97.672)\n",
            "Epoch: [78][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.4754 (1.4837)\tPrec@1 97.656 (97.699)\n",
            "Epoch: [78][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.4460 (1.4838)\tPrec@1 100.000 (97.687)\n",
            "Epoch: [78][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4936 (1.4857)\tPrec@1 96.875 (97.589)\n",
            "Epoch: [78][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.5367 (1.4864)\tPrec@1 92.188 (97.556)\n",
            "epoch 78 training time consumed: 151.66s\n",
            "Test: [0/79]\tTime 4.219 (4.219)\tLoss 1.6075 (1.6075)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 1.6125 (1.6249)\tPrec@1 92.188 (91.590)\n",
            " * Prec@1 91.790\n",
            "current lr 1.04922e-02\n",
            "Epoch: [79][0/391]\tTime 5.589 (5.589)\tData 4.947 (4.947)\tLoss 1.4641 (1.4641)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [79][50/391]\tTime 0.376 (0.477)\tData 0.001 (0.097)\tLoss 1.4813 (1.4741)\tPrec@1 99.219 (98.009)\n",
            "Epoch: [79][100/391]\tTime 0.375 (0.427)\tData 0.000 (0.049)\tLoss 1.4800 (1.4766)\tPrec@1 97.656 (97.811)\n",
            "Epoch: [79][150/391]\tTime 0.376 (0.410)\tData 0.000 (0.033)\tLoss 1.4977 (1.4748)\tPrec@1 96.094 (98.008)\n",
            "Epoch: [79][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.5103 (1.4751)\tPrec@1 95.312 (97.991)\n",
            "Epoch: [79][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4565 (1.4756)\tPrec@1 99.219 (97.921)\n",
            "Epoch: [79][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4641 (1.4765)\tPrec@1 97.656 (97.918)\n",
            "Epoch: [79][350/391]\tTime 0.375 (0.390)\tData 0.001 (0.014)\tLoss 1.4818 (1.4768)\tPrec@1 96.875 (97.903)\n",
            "epoch 79 training time consumed: 151.94s\n",
            "Test: [0/79]\tTime 4.120 (4.120)\tLoss 1.5262 (1.5262)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.112 (0.192)\tLoss 1.5549 (1.5894)\tPrec@1 94.531 (93.122)\n",
            " * Prec@1 93.030\n",
            "current lr 9.54915e-03\n",
            "Epoch: [80][0/391]\tTime 5.569 (5.569)\tData 5.025 (5.025)\tLoss 1.4657 (1.4657)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [80][50/391]\tTime 0.374 (0.478)\tData 0.000 (0.099)\tLoss 1.4380 (1.4652)\tPrec@1 100.000 (98.438)\n",
            "Epoch: [80][100/391]\tTime 0.375 (0.427)\tData 0.000 (0.050)\tLoss 1.4565 (1.4642)\tPrec@1 99.219 (98.561)\n",
            "Epoch: [80][150/391]\tTime 0.375 (0.410)\tData 0.001 (0.034)\tLoss 1.4849 (1.4660)\tPrec@1 98.438 (98.453)\n",
            "Epoch: [80][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.4523 (1.4662)\tPrec@1 99.219 (98.426)\n",
            "Epoch: [80][250/391]\tTime 0.373 (0.396)\tData 0.001 (0.020)\tLoss 1.4475 (1.4672)\tPrec@1 99.219 (98.391)\n",
            "Epoch: [80][300/391]\tTime 0.374 (0.392)\tData 0.000 (0.017)\tLoss 1.4741 (1.4682)\tPrec@1 98.438 (98.352)\n",
            "Epoch: [80][350/391]\tTime 0.373 (0.390)\tData 0.001 (0.015)\tLoss 1.4841 (1.4687)\tPrec@1 96.875 (98.324)\n",
            "epoch 80 training time consumed: 151.71s\n",
            "Test: [0/79]\tTime 4.149 (4.149)\tLoss 1.5804 (1.5804)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.5950 (1.6215)\tPrec@1 92.969 (92.203)\n",
            " * Prec@1 91.880\n",
            "current lr 8.64597e-03\n",
            "Epoch: [81][0/391]\tTime 5.565 (5.565)\tData 4.926 (4.926)\tLoss 1.4726 (1.4726)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [81][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.097)\tLoss 1.5006 (1.4670)\tPrec@1 96.094 (98.422)\n",
            "Epoch: [81][100/391]\tTime 0.375 (0.425)\tData 0.001 (0.049)\tLoss 1.4542 (1.4656)\tPrec@1 98.438 (98.492)\n",
            "Epoch: [81][150/391]\tTime 0.377 (0.408)\tData 0.001 (0.033)\tLoss 1.4963 (1.4661)\tPrec@1 96.875 (98.463)\n",
            "Epoch: [81][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.4690 (1.4655)\tPrec@1 99.219 (98.504)\n",
            "Epoch: [81][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4591 (1.4651)\tPrec@1 99.219 (98.518)\n",
            "Epoch: [81][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.4701 (1.4655)\tPrec@1 97.656 (98.453)\n",
            "Epoch: [81][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.4491 (1.4655)\tPrec@1 100.000 (98.462)\n",
            "epoch 81 training time consumed: 151.65s\n",
            "Test: [0/79]\tTime 4.152 (4.152)\tLoss 1.5584 (1.5584)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.114 (0.192)\tLoss 1.5359 (1.5840)\tPrec@1 96.875 (93.413)\n",
            " * Prec@1 93.220\n",
            "current lr 7.78360e-03\n",
            "Epoch: [82][0/391]\tTime 5.626 (5.626)\tData 4.980 (4.980)\tLoss 1.4409 (1.4409)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [82][50/391]\tTime 0.375 (0.478)\tData 0.000 (0.098)\tLoss 1.4455 (1.4586)\tPrec@1 100.000 (98.775)\n",
            "Epoch: [82][100/391]\tTime 0.376 (0.427)\tData 0.001 (0.050)\tLoss 1.4581 (1.4569)\tPrec@1 99.219 (98.817)\n",
            "Epoch: [82][150/391]\tTime 0.375 (0.410)\tData 0.000 (0.033)\tLoss 1.4520 (1.4574)\tPrec@1 99.219 (98.841)\n",
            "Epoch: [82][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.4632 (1.4570)\tPrec@1 98.438 (98.818)\n",
            "Epoch: [82][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4429 (1.4566)\tPrec@1 100.000 (98.845)\n",
            "Epoch: [82][300/391]\tTime 0.373 (0.392)\tData 0.000 (0.017)\tLoss 1.4754 (1.4571)\tPrec@1 97.656 (98.848)\n",
            "Epoch: [82][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.4687 (1.4577)\tPrec@1 99.219 (98.823)\n",
            "epoch 82 training time consumed: 151.71s\n",
            "Test: [0/79]\tTime 4.180 (4.180)\tLoss 1.5529 (1.5529)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.5072 (1.5815)\tPrec@1 97.656 (93.811)\n",
            " * Prec@1 93.870\n",
            "current lr 6.96290e-03\n",
            "Epoch: [83][0/391]\tTime 5.603 (5.603)\tData 4.955 (4.955)\tLoss 1.4575 (1.4575)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [83][50/391]\tTime 0.375 (0.476)\tData 0.001 (0.097)\tLoss 1.4450 (1.4475)\tPrec@1 99.219 (99.188)\n",
            "Epoch: [83][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.049)\tLoss 1.4426 (1.4487)\tPrec@1 100.000 (99.165)\n",
            "Epoch: [83][150/391]\tTime 0.377 (0.408)\tData 0.000 (0.033)\tLoss 1.4509 (1.4490)\tPrec@1 99.219 (99.141)\n",
            "Epoch: [83][200/391]\tTime 0.376 (0.400)\tData 0.001 (0.025)\tLoss 1.4570 (1.4495)\tPrec@1 97.656 (99.125)\n",
            "Epoch: [83][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4419 (1.4491)\tPrec@1 100.000 (99.160)\n",
            "Epoch: [83][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4360 (1.4496)\tPrec@1 100.000 (99.138)\n",
            "Epoch: [83][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.4367 (1.4494)\tPrec@1 99.219 (99.156)\n",
            "epoch 83 training time consumed: 151.71s\n",
            "Test: [0/79]\tTime 4.190 (4.190)\tLoss 1.5270 (1.5270)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.114 (0.193)\tLoss 1.5181 (1.5779)\tPrec@1 96.094 (93.658)\n",
            " * Prec@1 93.760\n",
            "current lr 6.18467e-03\n",
            "Epoch: [84][0/391]\tTime 5.536 (5.536)\tData 4.959 (4.959)\tLoss 1.4302 (1.4302)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [84][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.097)\tLoss 1.4337 (1.4463)\tPrec@1 100.000 (99.249)\n",
            "Epoch: [84][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.049)\tLoss 1.4383 (1.4450)\tPrec@1 100.000 (99.343)\n",
            "Epoch: [84][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.4422 (1.4443)\tPrec@1 99.219 (99.333)\n",
            "Epoch: [84][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.4418 (1.4452)\tPrec@1 98.438 (99.242)\n",
            "Epoch: [84][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 1.4418 (1.4451)\tPrec@1 100.000 (99.253)\n",
            "Epoch: [84][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4304 (1.4450)\tPrec@1 100.000 (99.240)\n",
            "Epoch: [84][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.4571 (1.4458)\tPrec@1 99.219 (99.201)\n",
            "epoch 84 training time consumed: 151.86s\n",
            "Test: [0/79]\tTime 4.186 (4.186)\tLoss 1.5028 (1.5028)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.113 (0.193)\tLoss 1.5012 (1.5742)\tPrec@1 96.094 (93.873)\n",
            " * Prec@1 93.860\n",
            "current lr 5.44967e-03\n",
            "Epoch: [85][0/391]\tTime 5.544 (5.544)\tData 4.959 (4.959)\tLoss 1.4370 (1.4370)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [85][50/391]\tTime 0.375 (0.477)\tData 0.001 (0.098)\tLoss 1.4325 (1.4379)\tPrec@1 100.000 (99.617)\n",
            "Epoch: [85][100/391]\tTime 0.375 (0.426)\tData 0.001 (0.049)\tLoss 1.4270 (1.4385)\tPrec@1 100.000 (99.544)\n",
            "Epoch: [85][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.4287 (1.4399)\tPrec@1 100.000 (99.431)\n",
            "Epoch: [85][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.4711 (1.4397)\tPrec@1 98.438 (99.468)\n",
            "Epoch: [85][250/391]\tTime 0.372 (0.395)\tData 0.000 (0.020)\tLoss 1.4443 (1.4399)\tPrec@1 99.219 (99.471)\n",
            "Epoch: [85][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4726 (1.4400)\tPrec@1 96.875 (99.481)\n",
            "Epoch: [85][350/391]\tTime 0.372 (0.389)\tData 0.001 (0.014)\tLoss 1.4294 (1.4404)\tPrec@1 100.000 (99.475)\n",
            "epoch 85 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.096 (4.096)\tLoss 1.5406 (1.5406)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.112 (0.191)\tLoss 1.5189 (1.5676)\tPrec@1 94.531 (94.148)\n",
            " * Prec@1 94.130\n",
            "current lr 4.75865e-03\n",
            "Epoch: [86][0/391]\tTime 5.599 (5.599)\tData 5.050 (5.050)\tLoss 1.4338 (1.4338)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [86][50/391]\tTime 0.375 (0.476)\tData 0.000 (0.099)\tLoss 1.4392 (1.4336)\tPrec@1 99.219 (99.755)\n",
            "Epoch: [86][100/391]\tTime 0.373 (0.425)\tData 0.000 (0.050)\tLoss 1.4314 (1.4337)\tPrec@1 99.219 (99.729)\n",
            "Epoch: [86][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.034)\tLoss 1.4345 (1.4336)\tPrec@1 100.000 (99.690)\n",
            "Epoch: [86][200/391]\tTime 0.374 (0.400)\tData 0.000 (0.025)\tLoss 1.4322 (1.4339)\tPrec@1 100.000 (99.685)\n",
            "Epoch: [86][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4356 (1.4340)\tPrec@1 100.000 (99.670)\n",
            "Epoch: [86][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4284 (1.4344)\tPrec@1 100.000 (99.660)\n",
            "Epoch: [86][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.015)\tLoss 1.4346 (1.4343)\tPrec@1 100.000 (99.666)\n",
            "epoch 86 training time consumed: 151.78s\n",
            "Test: [0/79]\tTime 4.106 (4.106)\tLoss 1.5346 (1.5346)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.5129 (1.5731)\tPrec@1 95.312 (93.827)\n",
            " * Prec@1 93.790\n",
            "current lr 4.11227e-03\n",
            "Epoch: [87][0/391]\tTime 5.542 (5.542)\tData 5.000 (5.000)\tLoss 1.4554 (1.4554)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [87][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.098)\tLoss 1.4266 (1.4343)\tPrec@1 100.000 (99.632)\n",
            "Epoch: [87][100/391]\tTime 0.375 (0.426)\tData 0.000 (0.050)\tLoss 1.4275 (1.4335)\tPrec@1 100.000 (99.675)\n",
            "Epoch: [87][150/391]\tTime 0.375 (0.409)\tData 0.000 (0.033)\tLoss 1.4349 (1.4333)\tPrec@1 99.219 (99.659)\n",
            "Epoch: [87][200/391]\tTime 0.371 (0.400)\tData 0.000 (0.025)\tLoss 1.4381 (1.4334)\tPrec@1 99.219 (99.662)\n",
            "Epoch: [87][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4229 (1.4329)\tPrec@1 100.000 (99.689)\n",
            "Epoch: [87][300/391]\tTime 0.376 (0.391)\tData 0.000 (0.017)\tLoss 1.4298 (1.4328)\tPrec@1 100.000 (99.691)\n",
            "Epoch: [87][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.4334 (1.4326)\tPrec@1 100.000 (99.706)\n",
            "epoch 87 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.077 (4.077)\tLoss 1.5278 (1.5278)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.112 (0.191)\tLoss 1.5027 (1.5602)\tPrec@1 97.656 (94.271)\n",
            " * Prec@1 94.210\n",
            "current lr 3.51118e-03\n",
            "Epoch: [88][0/391]\tTime 5.602 (5.602)\tData 4.929 (4.929)\tLoss 1.4360 (1.4360)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [88][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.097)\tLoss 1.4251 (1.4283)\tPrec@1 100.000 (99.893)\n",
            "Epoch: [88][100/391]\tTime 0.375 (0.425)\tData 0.000 (0.049)\tLoss 1.4286 (1.4280)\tPrec@1 100.000 (99.892)\n",
            "Epoch: [88][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.4247 (1.4283)\tPrec@1 100.000 (99.855)\n",
            "Epoch: [88][200/391]\tTime 0.375 (0.400)\tData 0.000 (0.025)\tLoss 1.4219 (1.4284)\tPrec@1 100.000 (99.813)\n",
            "Epoch: [88][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4245 (1.4285)\tPrec@1 100.000 (99.816)\n",
            "Epoch: [88][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4254 (1.4289)\tPrec@1 100.000 (99.782)\n",
            "Epoch: [88][350/391]\tTime 0.373 (0.390)\tData 0.001 (0.014)\tLoss 1.4261 (1.4290)\tPrec@1 100.000 (99.786)\n",
            "epoch 88 training time consumed: 151.84s\n",
            "Test: [0/79]\tTime 4.219 (4.219)\tLoss 1.5287 (1.5287)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 1.4972 (1.5633)\tPrec@1 94.531 (94.317)\n",
            " * Prec@1 94.310\n",
            "current lr 2.95596e-03\n",
            "Epoch: [89][0/391]\tTime 5.604 (5.604)\tData 5.018 (5.018)\tLoss 1.4213 (1.4213)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [89][50/391]\tTime 0.377 (0.477)\tData 0.000 (0.099)\tLoss 1.4275 (1.4268)\tPrec@1 100.000 (99.801)\n",
            "Epoch: [89][100/391]\tTime 0.374 (0.427)\tData 0.000 (0.050)\tLoss 1.4351 (1.4263)\tPrec@1 99.219 (99.861)\n",
            "Epoch: [89][150/391]\tTime 0.377 (0.410)\tData 0.000 (0.033)\tLoss 1.4179 (1.4271)\tPrec@1 100.000 (99.819)\n",
            "Epoch: [89][200/391]\tTime 0.374 (0.401)\tData 0.000 (0.025)\tLoss 1.4461 (1.4272)\tPrec@1 100.000 (99.825)\n",
            "Epoch: [89][250/391]\tTime 0.371 (0.396)\tData 0.000 (0.020)\tLoss 1.4277 (1.4274)\tPrec@1 100.000 (99.816)\n",
            "Epoch: [89][300/391]\tTime 0.374 (0.392)\tData 0.001 (0.017)\tLoss 1.4341 (1.4275)\tPrec@1 99.219 (99.805)\n",
            "Epoch: [89][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.015)\tLoss 1.4252 (1.4275)\tPrec@1 100.000 (99.804)\n",
            "epoch 89 training time consumed: 151.92s\n",
            "Test: [0/79]\tTime 4.144 (4.144)\tLoss 1.5327 (1.5327)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.114 (0.192)\tLoss 1.4875 (1.5608)\tPrec@1 96.875 (94.271)\n",
            " * Prec@1 94.280\n",
            "current lr 2.44717e-03\n",
            "Epoch: [90][0/391]\tTime 5.580 (5.580)\tData 4.948 (4.948)\tLoss 1.4223 (1.4223)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [90][50/391]\tTime 0.373 (0.477)\tData 0.000 (0.097)\tLoss 1.4206 (1.4264)\tPrec@1 100.000 (99.862)\n",
            "Epoch: [90][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.4267 (1.4261)\tPrec@1 100.000 (99.861)\n",
            "Epoch: [90][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.4428 (1.4259)\tPrec@1 99.219 (99.865)\n",
            "Epoch: [90][200/391]\tTime 0.377 (0.400)\tData 0.002 (0.025)\tLoss 1.4257 (1.4256)\tPrec@1 100.000 (99.868)\n",
            "Epoch: [90][250/391]\tTime 0.370 (0.395)\tData 0.000 (0.020)\tLoss 1.4273 (1.4257)\tPrec@1 99.219 (99.851)\n",
            "Epoch: [90][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.4221 (1.4258)\tPrec@1 100.000 (99.852)\n",
            "Epoch: [90][350/391]\tTime 0.374 (0.389)\tData 0.000 (0.014)\tLoss 1.4211 (1.4259)\tPrec@1 100.000 (99.846)\n",
            "epoch 90 training time consumed: 151.48s\n",
            "Test: [0/79]\tTime 4.131 (4.131)\tLoss 1.5179 (1.5179)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.4916 (1.5558)\tPrec@1 96.094 (94.715)\n",
            " * Prec@1 94.780\n",
            "current lr 1.98532e-03\n",
            "Epoch: [91][0/391]\tTime 5.615 (5.615)\tData 4.997 (4.997)\tLoss 1.4232 (1.4232)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [91][50/391]\tTime 0.374 (0.477)\tData 0.001 (0.098)\tLoss 1.4243 (1.4251)\tPrec@1 100.000 (99.893)\n",
            "Epoch: [91][100/391]\tTime 0.376 (0.426)\tData 0.000 (0.050)\tLoss 1.4196 (1.4246)\tPrec@1 100.000 (99.892)\n",
            "Epoch: [91][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.4267 (1.4245)\tPrec@1 99.219 (99.907)\n",
            "Epoch: [91][200/391]\tTime 0.375 (0.401)\tData 0.000 (0.025)\tLoss 1.4274 (1.4249)\tPrec@1 100.000 (99.891)\n",
            "Epoch: [91][250/391]\tTime 0.374 (0.396)\tData 0.000 (0.020)\tLoss 1.4194 (1.4248)\tPrec@1 100.000 (99.885)\n",
            "Epoch: [91][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4234 (1.4248)\tPrec@1 100.000 (99.886)\n",
            "Epoch: [91][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.4211 (1.4247)\tPrec@1 100.000 (99.878)\n",
            "epoch 91 training time consumed: 151.86s\n",
            "Test: [0/79]\tTime 4.011 (4.011)\tLoss 1.5099 (1.5099)\tPrec@1 97.656 (97.656)\n",
            "Test: [50/79]\tTime 0.113 (0.190)\tLoss 1.4878 (1.5584)\tPrec@1 96.875 (94.577)\n",
            " * Prec@1 94.650\n",
            "current lr 1.57084e-03\n",
            "Epoch: [92][0/391]\tTime 5.588 (5.588)\tData 4.956 (4.956)\tLoss 1.4194 (1.4194)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [92][50/391]\tTime 0.373 (0.476)\tData 0.001 (0.097)\tLoss 1.4178 (1.4218)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [92][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.049)\tLoss 1.4177 (1.4226)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [92][150/391]\tTime 0.373 (0.409)\tData 0.000 (0.033)\tLoss 1.4270 (1.4229)\tPrec@1 100.000 (99.907)\n",
            "Epoch: [92][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.4260 (1.4228)\tPrec@1 100.000 (99.914)\n",
            "Epoch: [92][250/391]\tTime 0.372 (0.395)\tData 0.000 (0.020)\tLoss 1.4255 (1.4231)\tPrec@1 100.000 (99.910)\n",
            "Epoch: [92][300/391]\tTime 0.375 (0.391)\tData 0.001 (0.017)\tLoss 1.4252 (1.4230)\tPrec@1 100.000 (99.914)\n",
            "Epoch: [92][350/391]\tTime 0.373 (0.389)\tData 0.000 (0.014)\tLoss 1.4290 (1.4231)\tPrec@1 100.000 (99.900)\n",
            "epoch 92 training time consumed: 151.48s\n",
            "Test: [0/79]\tTime 4.134 (4.134)\tLoss 1.5117 (1.5117)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.4898 (1.5555)\tPrec@1 96.094 (94.485)\n",
            " * Prec@1 94.600\n",
            "current lr 1.20416e-03\n",
            "Epoch: [93][0/391]\tTime 5.633 (5.633)\tData 4.957 (4.957)\tLoss 1.4227 (1.4227)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [93][50/391]\tTime 0.377 (0.478)\tData 0.000 (0.098)\tLoss 1.4239 (1.4225)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [93][100/391]\tTime 0.374 (0.427)\tData 0.000 (0.049)\tLoss 1.4217 (1.4223)\tPrec@1 100.000 (99.977)\n",
            "Epoch: [93][150/391]\tTime 0.375 (0.410)\tData 0.000 (0.033)\tLoss 1.4185 (1.4222)\tPrec@1 100.000 (99.948)\n",
            "Epoch: [93][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.4218 (1.4223)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [93][250/391]\tTime 0.374 (0.396)\tData 0.001 (0.020)\tLoss 1.4186 (1.4222)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [93][300/391]\tTime 0.375 (0.393)\tData 0.000 (0.017)\tLoss 1.4235 (1.4223)\tPrec@1 100.000 (99.933)\n",
            "Epoch: [93][350/391]\tTime 0.374 (0.390)\tData 0.000 (0.014)\tLoss 1.4221 (1.4222)\tPrec@1 100.000 (99.931)\n",
            "epoch 93 training time consumed: 151.97s\n",
            "Test: [0/79]\tTime 4.098 (4.098)\tLoss 1.5194 (1.5194)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.113 (0.191)\tLoss 1.4857 (1.5537)\tPrec@1 96.875 (94.577)\n",
            " * Prec@1 94.690\n",
            "current lr 8.85637e-04\n",
            "Epoch: [94][0/391]\tTime 5.593 (5.593)\tData 4.929 (4.929)\tLoss 1.4211 (1.4211)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [94][50/391]\tTime 0.376 (0.477)\tData 0.000 (0.097)\tLoss 1.4316 (1.4230)\tPrec@1 100.000 (99.877)\n",
            "Epoch: [94][100/391]\tTime 0.375 (0.427)\tData 0.000 (0.049)\tLoss 1.4210 (1.4230)\tPrec@1 100.000 (99.892)\n",
            "Epoch: [94][150/391]\tTime 0.376 (0.410)\tData 0.000 (0.033)\tLoss 1.4257 (1.4228)\tPrec@1 99.219 (99.886)\n",
            "Epoch: [94][200/391]\tTime 0.373 (0.401)\tData 0.000 (0.025)\tLoss 1.4166 (1.4222)\tPrec@1 100.000 (99.891)\n",
            "Epoch: [94][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4292 (1.4224)\tPrec@1 100.000 (99.885)\n",
            "Epoch: [94][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4243 (1.4223)\tPrec@1 100.000 (99.896)\n",
            "Epoch: [94][350/391]\tTime 0.376 (0.390)\tData 0.000 (0.014)\tLoss 1.4202 (1.4223)\tPrec@1 100.000 (99.898)\n",
            "epoch 94 training time consumed: 151.89s\n",
            "Test: [0/79]\tTime 4.154 (4.154)\tLoss 1.5079 (1.5079)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.113 (0.192)\tLoss 1.4871 (1.5534)\tPrec@1 97.656 (94.776)\n",
            " * Prec@1 94.780\n",
            "current lr 6.15583e-04\n",
            "Epoch: [95][0/391]\tTime 5.621 (5.621)\tData 4.979 (4.979)\tLoss 1.4166 (1.4166)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [95][50/391]\tTime 0.376 (0.476)\tData 0.000 (0.098)\tLoss 1.4272 (1.4212)\tPrec@1 100.000 (99.939)\n",
            "Epoch: [95][100/391]\tTime 0.373 (0.426)\tData 0.000 (0.050)\tLoss 1.4225 (1.4210)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [95][150/391]\tTime 0.375 (0.408)\tData 0.001 (0.033)\tLoss 1.4222 (1.4216)\tPrec@1 100.000 (99.943)\n",
            "Epoch: [95][200/391]\tTime 0.373 (0.400)\tData 0.000 (0.025)\tLoss 1.4169 (1.4215)\tPrec@1 100.000 (99.934)\n",
            "Epoch: [95][250/391]\tTime 0.375 (0.395)\tData 0.000 (0.020)\tLoss 1.4187 (1.4218)\tPrec@1 100.000 (99.932)\n",
            "Epoch: [95][300/391]\tTime 0.374 (0.391)\tData 0.000 (0.017)\tLoss 1.4314 (1.4218)\tPrec@1 99.219 (99.925)\n",
            "Epoch: [95][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.4184 (1.4217)\tPrec@1 100.000 (99.920)\n",
            "epoch 95 training time consumed: 151.45s\n",
            "Test: [0/79]\tTime 4.264 (4.264)\tLoss 1.5143 (1.5143)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.113 (0.194)\tLoss 1.4891 (1.5541)\tPrec@1 96.875 (94.746)\n",
            " * Prec@1 94.720\n",
            "current lr 3.94265e-04\n",
            "Epoch: [96][0/391]\tTime 5.542 (5.542)\tData 4.960 (4.960)\tLoss 1.4215 (1.4215)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [96][50/391]\tTime 0.374 (0.476)\tData 0.000 (0.097)\tLoss 1.4166 (1.4203)\tPrec@1 100.000 (99.939)\n",
            "Epoch: [96][100/391]\tTime 0.376 (0.426)\tData 0.000 (0.049)\tLoss 1.4167 (1.4211)\tPrec@1 100.000 (99.899)\n",
            "Epoch: [96][150/391]\tTime 0.374 (0.409)\tData 0.000 (0.033)\tLoss 1.4210 (1.4213)\tPrec@1 100.000 (99.912)\n",
            "Epoch: [96][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.4251 (1.4212)\tPrec@1 100.000 (99.922)\n",
            "Epoch: [96][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4330 (1.4210)\tPrec@1 100.000 (99.925)\n",
            "Epoch: [96][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4220 (1.4210)\tPrec@1 100.000 (99.935)\n",
            "Epoch: [96][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.4253 (1.4210)\tPrec@1 99.219 (99.933)\n",
            "epoch 96 training time consumed: 151.85s\n",
            "Test: [0/79]\tTime 4.194 (4.194)\tLoss 1.5191 (1.5191)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.112 (0.193)\tLoss 1.4879 (1.5542)\tPrec@1 97.656 (94.746)\n",
            " * Prec@1 94.820\n",
            "current lr 2.21902e-04\n",
            "Epoch: [97][0/391]\tTime 5.563 (5.563)\tData 4.924 (4.924)\tLoss 1.4192 (1.4192)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [97][50/391]\tTime 0.372 (0.476)\tData 0.000 (0.097)\tLoss 1.4170 (1.4206)\tPrec@1 100.000 (99.969)\n",
            "Epoch: [97][100/391]\tTime 0.377 (0.425)\tData 0.002 (0.049)\tLoss 1.4278 (1.4203)\tPrec@1 100.000 (99.977)\n",
            "Epoch: [97][150/391]\tTime 0.372 (0.408)\tData 0.000 (0.033)\tLoss 1.4239 (1.4205)\tPrec@1 100.000 (99.969)\n",
            "Epoch: [97][200/391]\tTime 0.374 (0.400)\tData 0.001 (0.025)\tLoss 1.4210 (1.4205)\tPrec@1 100.000 (99.957)\n",
            "Epoch: [97][250/391]\tTime 0.373 (0.395)\tData 0.000 (0.020)\tLoss 1.4321 (1.4205)\tPrec@1 99.219 (99.956)\n",
            "Epoch: [97][300/391]\tTime 0.372 (0.391)\tData 0.000 (0.017)\tLoss 1.4175 (1.4205)\tPrec@1 100.000 (99.958)\n",
            "Epoch: [97][350/391]\tTime 0.375 (0.389)\tData 0.000 (0.014)\tLoss 1.4177 (1.4204)\tPrec@1 100.000 (99.960)\n",
            "epoch 97 training time consumed: 151.53s\n",
            "Test: [0/79]\tTime 4.120 (4.120)\tLoss 1.5099 (1.5099)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.114 (0.192)\tLoss 1.4867 (1.5557)\tPrec@1 96.875 (94.792)\n",
            " * Prec@1 94.680\n",
            "current lr 9.86636e-05\n",
            "Epoch: [98][0/391]\tTime 5.577 (5.577)\tData 4.970 (4.970)\tLoss 1.4160 (1.4160)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [98][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.4200 (1.4206)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [98][100/391]\tTime 0.374 (0.427)\tData 0.000 (0.049)\tLoss 1.4188 (1.4207)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [98][150/391]\tTime 0.376 (0.409)\tData 0.000 (0.033)\tLoss 1.4200 (1.4206)\tPrec@1 100.000 (99.933)\n",
            "Epoch: [98][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.4168 (1.4208)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [98][250/391]\tTime 0.375 (0.396)\tData 0.000 (0.020)\tLoss 1.4181 (1.4209)\tPrec@1 100.000 (99.919)\n",
            "Epoch: [98][300/391]\tTime 0.376 (0.392)\tData 0.000 (0.017)\tLoss 1.4171 (1.4209)\tPrec@1 100.000 (99.914)\n",
            "Epoch: [98][350/391]\tTime 0.375 (0.390)\tData 0.000 (0.014)\tLoss 1.4207 (1.4209)\tPrec@1 100.000 (99.920)\n",
            "epoch 98 training time consumed: 151.92s\n",
            "Test: [0/79]\tTime 4.119 (4.119)\tLoss 1.5103 (1.5103)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.114 (0.192)\tLoss 1.4915 (1.5566)\tPrec@1 97.656 (94.669)\n",
            " * Prec@1 94.680\n",
            "current lr 2.46720e-05\n",
            "Epoch: [99][0/391]\tTime 5.590 (5.590)\tData 4.961 (4.961)\tLoss 1.4273 (1.4273)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [99][50/391]\tTime 0.374 (0.477)\tData 0.000 (0.098)\tLoss 1.4162 (1.4200)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [99][100/391]\tTime 0.374 (0.426)\tData 0.000 (0.049)\tLoss 1.4210 (1.4202)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [99][150/391]\tTime 0.377 (0.409)\tData 0.000 (0.033)\tLoss 1.4204 (1.4202)\tPrec@1 100.000 (99.959)\n",
            "Epoch: [99][200/391]\tTime 0.376 (0.401)\tData 0.000 (0.025)\tLoss 1.4224 (1.4201)\tPrec@1 100.000 (99.953)\n",
            "Epoch: [99][250/391]\tTime 0.376 (0.396)\tData 0.000 (0.020)\tLoss 1.4173 (1.4202)\tPrec@1 100.000 (99.953)\n",
            "Epoch: [99][300/391]\tTime 0.375 (0.392)\tData 0.000 (0.017)\tLoss 1.4218 (1.4203)\tPrec@1 100.000 (99.951)\n",
            "Epoch: [99][350/391]\tTime 0.373 (0.390)\tData 0.000 (0.014)\tLoss 1.4284 (1.4205)\tPrec@1 100.000 (99.944)\n",
            "epoch 99 training time consumed: 151.82s\n",
            "Test: [0/79]\tTime 4.037 (4.037)\tLoss 1.5112 (1.5112)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.112 (0.190)\tLoss 1.4874 (1.5539)\tPrec@1 97.656 (94.776)\n",
            " * Prec@1 94.740\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random \n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "config = Config()\n",
        "writer = SummaryWriter(config.log_dir, filename_suffix=config.model_name)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, 4),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]),\n",
        "    download=True),\n",
        "    batch_size=config.batch_size, shuffle=True,\n",
        "    num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        ])),\n",
        "        batch_size=config.batch_size, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "best_prec1 = 0\n",
        "model = torch.nn.DataParallel(model)\n",
        "model.cuda()\n",
        "\n",
        "print(\"cuda:\", torch.cuda.is_available())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing).cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
        "                                  momentum=config.momentum,\n",
        "                                  weight_decay=config.weight_decay)\n",
        "    \n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
        "\n",
        "\n",
        "for epoch in range(config.start_epoch, config.epochs):\n",
        "    # train for one epoch\n",
        "    print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "    train(train_loader, model, criterion, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on validation set\n",
        "    prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "    # remember best prec@1 and save checkpoint\n",
        "    is_best = prec1 > best_prec1\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), \"best.pth\")\n",
        "        best_prec1 = prec1\n",
        "        \n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94.82\n"
          ]
        }
      ],
      "source": [
        "print(best_prec1)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.10 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "5d3b2ef1d9e6660f31dedd3f271467a7eb92c8f55d3b82d5289c27a73078029c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
