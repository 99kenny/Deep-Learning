{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WN4L9TQP4dRK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self):    \n",
        "        # input \n",
        "        self.num_classes = 10\n",
        "        # training \n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100\n",
        "        self.start_epoch = 0\n",
        "        self.momentum = 0.9\n",
        "        self.lr = 1e-1\n",
        "        self.weight_decay = 5e-4\n",
        "        self.label_smoothing = 0\n",
        "        self.model_name = 'vgg_nomaxpool'\n",
        "\n",
        "        self.gpu = True\n",
        "        self.log_dir = '' + self.model_name\n",
        "config = Config()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ikkG8-7g4dRN"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VggNet(nn.Module):\n",
        "\n",
        "    def __init__(self, features):\n",
        "        # features : [64, 64, M, ...]\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512*4*4, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, config.num_classes), # num class = 10 (CIFAR10)\n",
        "        )\n",
        "\n",
        "        self.layers = [] # construct layers using 'features' paremeter\n",
        "        in_channels = 3\n",
        "        for i in features:\n",
        "            if i == 'M': #MaxPooling layer\n",
        "                self.layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else: #Convolution + Batchnorm + Relu\n",
        "                conv2d = nn.Conv2d(in_channels, i, kernel_size=3, padding=1)\n",
        "                self.layers += [conv2d, nn.BatchNorm2d(i), nn.ReLU(inplace=True)]\n",
        "                in_channels = i\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1) #flattens the tensor with batch size\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        # compute loss\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()    # set gradients to zero\n",
        "        loss.backward()          # compute gradients\n",
        "        optimizer.step()         # step with learning rate\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc = accuracy(output.data, target)[0]\n",
        "        losses += loss.item()\n",
        "        accs += acc\n",
        "    accs /= len(train_loader)\n",
        "    losses /= len(train_loader)\n",
        "    print('[Epoch {epoch}] Average Loss : {loss:.3f}, Average Accuracy : {acc:.3f}'\n",
        "          .format(epoch = epoch , loss=losses, acc=accs))\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", losses, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", accs, epoch)\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # disable tracking gradient to reduce memory use and increase computation speed\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses += loss.item()\n",
        "            accs += prec1.item()\n",
        "\n",
        "        losses /= len(val_loader)\n",
        "        accs /= len(val_loader)\n",
        "        print('[Validation] : Average Loss {loss:.3f}, Average Accuracy {acc:.3f}'\n",
        "            .format(loss=losses, acc=accs))\n",
        "\n",
        "        writer.add_scalar(\"Loss/val\", losses, epoch)\n",
        "        writer.add_scalar(\"Accuracy/val\", accs, epoch)\n",
        "\n",
        "    return accs\n",
        "# top k accuacry\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpTrXFHp4dRQ",
        "outputId": "3135b612-a49b-44dd-b107-1949c24b3ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24497994\n"
          ]
        }
      ],
      "source": [
        "features = [64, 64, 128, 128, 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "model = VggNet(features)\n",
        "count = 0\n",
        "# total params\n",
        "print(\"total params :\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "6a9320e65b7a4c649a47061f03413b5c",
            "c08f673139c84571abb66ad4b6922449",
            "c6790688d326402ba083d5c22ed8360a",
            "e1126fe3e80745f88d650d7e5d47e26a",
            "2b20b61f9e214f8b8951c16a5c7e1693",
            "977834f92b1642549d77a2148ceba40e",
            "8d0c08e4da9d425683bea661d71dc1c7",
            "2337266e20af4655a6cb20e52efb4b07",
            "ae60e27f93a64ed2892d2f064508f96e",
            "49338d75a7094f61880c852c0a484a25",
            "5fc7f3a8141140918fd8cb6999ecc95a"
          ]
        },
        "id": "InZqbajr4dRT",
        "outputId": "6f2a9a96-2cf6-4373-ca25-27e8350f2148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, 4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    download=True),\n",
        "    batch_size=config.batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225]),\n",
        "        ])),\n",
        "        batch_size=config.batch_size, shuffle=False,\n",
        "        num_workers=2, pin_memory=True)\n",
        "\n",
        "writer = SummaryWriter(config.log_dir, filename_suffix=config.model_name)\n",
        "\n",
        "input_tensor = torch.Tensor(1, 3, 32, 32)\n",
        "if config.gpu:\n",
        "    input_tensor = input_tensor.cuda()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL32DGh64dRT",
        "outputId": "a1f2b89f-10c0-41d6-e4c6-5b9c57782ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda: True\n",
            "current lr 1.00000e-01\n",
            "Epoch: [0][0/391]\tTime 10.284 (10.284)\tLoss 2.3382 (2.3382)\tPrec@1 12.500 (12.500)\n",
            "Epoch: [0][50/391]\tTime 0.250 (0.456)\tLoss 2.3464 (2.3703)\tPrec@1 12.500 (10.187)\n",
            "Epoch: [0][100/391]\tTime 0.263 (0.359)\tLoss 2.2943 (2.3412)\tPrec@1 14.062 (10.682)\n",
            "Epoch: [0][150/391]\tTime 0.262 (0.326)\tLoss 2.2826 (2.3273)\tPrec@1 15.625 (10.865)\n",
            "Epoch: [0][200/391]\tTime 0.260 (0.310)\tLoss 2.2997 (2.3193)\tPrec@1 10.938 (10.899)\n",
            "Epoch: [0][250/391]\tTime 0.259 (0.300)\tLoss 2.2907 (2.3145)\tPrec@1 9.375 (10.751)\n",
            "Epoch: [0][300/391]\tTime 0.260 (0.293)\tLoss 2.2729 (2.3092)\tPrec@1 10.156 (11.054)\n",
            "Epoch: [0][350/391]\tTime 0.260 (0.289)\tLoss 2.1243 (2.2925)\tPrec@1 11.719 (11.699)\n",
            "epoch 0 training time consumed: 114.66s\n",
            "Test: [0/79]\tTime 4.049 (4.049)\tLoss 2.1663 (2.1663)\tPrec@1 15.625 (15.625)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 2.1181 (2.1963)\tPrec@1 19.531 (13.925)\n",
            " * Prec@1 13.930\n",
            "current lr 9.99753e-02\n",
            "Epoch: [1][0/391]\tTime 5.387 (5.387)\tLoss 2.1449 (2.1449)\tPrec@1 16.406 (16.406)\n",
            "Epoch: [1][50/391]\tTime 0.261 (0.361)\tLoss 2.0404 (2.0945)\tPrec@1 15.625 (16.529)\n",
            "Epoch: [1][100/391]\tTime 0.260 (0.311)\tLoss 2.0580 (2.0822)\tPrec@1 13.281 (16.244)\n",
            "Epoch: [1][150/391]\tTime 0.261 (0.295)\tLoss 1.9943 (2.0735)\tPrec@1 16.406 (16.955)\n",
            "Epoch: [1][200/391]\tTime 0.261 (0.286)\tLoss 2.0578 (2.0588)\tPrec@1 17.188 (17.351)\n",
            "Epoch: [1][250/391]\tTime 0.261 (0.281)\tLoss 1.9844 (2.0488)\tPrec@1 19.531 (17.611)\n",
            "Epoch: [1][300/391]\tTime 0.262 (0.278)\tLoss 1.8759 (2.0362)\tPrec@1 23.438 (17.919)\n",
            "Epoch: [1][350/391]\tTime 0.263 (0.276)\tLoss 2.0388 (2.0265)\tPrec@1 20.312 (18.245)\n",
            "epoch 1 training time consumed: 107.35s\n",
            "Test: [0/79]\tTime 3.997 (3.997)\tLoss 1.9176 (1.9176)\tPrec@1 20.312 (20.312)\n",
            "Test: [50/79]\tTime 0.085 (0.163)\tLoss 1.9366 (1.9208)\tPrec@1 20.312 (20.879)\n",
            " * Prec@1 20.240\n",
            "current lr 9.99013e-02\n",
            "Epoch: [2][0/391]\tTime 5.400 (5.400)\tLoss 1.8911 (1.8911)\tPrec@1 21.094 (21.094)\n",
            "Epoch: [2][50/391]\tTime 0.262 (0.361)\tLoss 1.9931 (1.9543)\tPrec@1 19.531 (19.792)\n",
            "Epoch: [2][100/391]\tTime 0.262 (0.312)\tLoss 2.0358 (1.9523)\tPrec@1 14.062 (20.088)\n",
            "Epoch: [2][150/391]\tTime 0.263 (0.295)\tLoss 1.9077 (1.9465)\tPrec@1 30.469 (19.883)\n",
            "Epoch: [2][200/391]\tTime 0.262 (0.287)\tLoss 1.9434 (1.9465)\tPrec@1 14.844 (19.803)\n",
            "Epoch: [2][250/391]\tTime 0.263 (0.282)\tLoss 1.8396 (1.9388)\tPrec@1 25.781 (20.054)\n",
            "Epoch: [2][300/391]\tTime 0.262 (0.279)\tLoss 1.8753 (1.9348)\tPrec@1 25.000 (20.224)\n",
            "Epoch: [2][350/391]\tTime 0.263 (0.277)\tLoss 1.9032 (1.9353)\tPrec@1 21.094 (20.215)\n",
            "epoch 2 training time consumed: 107.67s\n",
            "Test: [0/79]\tTime 4.092 (4.092)\tLoss 1.9649 (1.9649)\tPrec@1 21.094 (21.094)\n",
            "Test: [50/79]\tTime 0.086 (0.165)\tLoss 1.8304 (1.9134)\tPrec@1 27.344 (24.096)\n",
            " * Prec@1 24.270\n",
            "current lr 9.97781e-02\n",
            "Epoch: [3][0/391]\tTime 5.486 (5.486)\tLoss 1.8173 (1.8173)\tPrec@1 29.688 (29.688)\n",
            "Epoch: [3][50/391]\tTime 0.262 (0.365)\tLoss 1.8324 (1.9170)\tPrec@1 22.656 (22.963)\n",
            "Epoch: [3][100/391]\tTime 0.263 (0.314)\tLoss 1.8919 (1.9096)\tPrec@1 16.406 (23.082)\n",
            "Epoch: [3][150/391]\tTime 0.263 (0.297)\tLoss 1.8639 (1.8950)\tPrec@1 26.562 (23.401)\n",
            "Epoch: [3][200/391]\tTime 0.263 (0.289)\tLoss 1.9296 (1.8841)\tPrec@1 28.906 (23.916)\n",
            "Epoch: [3][250/391]\tTime 0.262 (0.283)\tLoss 1.8975 (1.8815)\tPrec@1 18.750 (24.072)\n",
            "Epoch: [3][300/391]\tTime 0.262 (0.280)\tLoss 1.7716 (1.8767)\tPrec@1 26.562 (24.377)\n",
            "Epoch: [3][350/391]\tTime 0.263 (0.278)\tLoss 1.9268 (1.8726)\tPrec@1 27.344 (24.666)\n",
            "epoch 3 training time consumed: 108.02s\n",
            "Test: [0/79]\tTime 4.014 (4.014)\tLoss 1.8162 (1.8162)\tPrec@1 25.000 (25.000)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 1.8340 (1.7783)\tPrec@1 30.469 (29.626)\n",
            " * Prec@1 29.470\n",
            "current lr 9.96057e-02\n",
            "Epoch: [4][0/391]\tTime 5.399 (5.399)\tLoss 1.7398 (1.7398)\tPrec@1 25.000 (25.000)\n",
            "Epoch: [4][50/391]\tTime 0.263 (0.362)\tLoss 1.9629 (1.8395)\tPrec@1 18.750 (25.965)\n",
            "Epoch: [4][100/391]\tTime 0.262 (0.313)\tLoss 1.8484 (1.8324)\tPrec@1 19.531 (26.191)\n",
            "Epoch: [4][150/391]\tTime 0.263 (0.296)\tLoss 1.7967 (1.8203)\tPrec@1 32.031 (27.276)\n",
            "Epoch: [4][200/391]\tTime 0.263 (0.288)\tLoss 1.7917 (1.8148)\tPrec@1 25.000 (27.678)\n",
            "Epoch: [4][250/391]\tTime 0.263 (0.283)\tLoss 1.6764 (1.8061)\tPrec@1 32.031 (28.181)\n",
            "Epoch: [4][300/391]\tTime 0.262 (0.280)\tLoss 1.7289 (1.7961)\tPrec@1 23.438 (28.940)\n",
            "Epoch: [4][350/391]\tTime 0.263 (0.277)\tLoss 1.7166 (1.7827)\tPrec@1 32.031 (29.558)\n",
            "epoch 4 training time consumed: 107.90s\n",
            "Test: [0/79]\tTime 4.029 (4.029)\tLoss 1.6053 (1.6053)\tPrec@1 39.844 (39.844)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 1.6179 (1.6273)\tPrec@1 38.281 (36.229)\n",
            " * Prec@1 36.080\n",
            "current lr 9.93844e-02\n",
            "Epoch: [5][0/391]\tTime 5.563 (5.563)\tLoss 1.7107 (1.7107)\tPrec@1 28.906 (28.906)\n",
            "Epoch: [5][50/391]\tTime 0.260 (0.364)\tLoss 1.5671 (1.6736)\tPrec@1 39.062 (35.585)\n",
            "Epoch: [5][100/391]\tTime 0.262 (0.313)\tLoss 1.5980 (1.6571)\tPrec@1 40.625 (35.976)\n",
            "Epoch: [5][150/391]\tTime 0.262 (0.296)\tLoss 1.4977 (1.6384)\tPrec@1 36.719 (36.589)\n",
            "Epoch: [5][200/391]\tTime 0.261 (0.288)\tLoss 1.4950 (1.6316)\tPrec@1 45.312 (37.205)\n",
            "Epoch: [5][250/391]\tTime 0.263 (0.282)\tLoss 1.5130 (1.6163)\tPrec@1 43.750 (37.802)\n",
            "Epoch: [5][300/391]\tTime 0.262 (0.279)\tLoss 1.4279 (1.6019)\tPrec@1 42.188 (38.484)\n",
            "Epoch: [5][350/391]\tTime 0.263 (0.277)\tLoss 1.4928 (1.5852)\tPrec@1 43.750 (39.307)\n",
            "epoch 5 training time consumed: 107.68s\n",
            "Test: [0/79]\tTime 4.041 (4.041)\tLoss 1.6128 (1.6128)\tPrec@1 44.531 (44.531)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 1.6553 (1.7846)\tPrec@1 39.844 (38.404)\n",
            " * Prec@1 38.370\n",
            "current lr 9.91144e-02\n",
            "Epoch: [6][0/391]\tTime 5.327 (5.327)\tLoss 1.4657 (1.4657)\tPrec@1 44.531 (44.531)\n",
            "Epoch: [6][50/391]\tTime 0.262 (0.360)\tLoss 1.4083 (1.4305)\tPrec@1 48.438 (47.518)\n",
            "Epoch: [6][100/391]\tTime 0.262 (0.312)\tLoss 1.4910 (1.4199)\tPrec@1 43.750 (47.486)\n",
            "Epoch: [6][150/391]\tTime 0.261 (0.296)\tLoss 1.4203 (1.4119)\tPrec@1 46.094 (48.008)\n",
            "Epoch: [6][200/391]\tTime 0.262 (0.288)\tLoss 1.3674 (1.4027)\tPrec@1 53.125 (48.725)\n",
            "Epoch: [6][250/391]\tTime 0.262 (0.282)\tLoss 1.3405 (1.3850)\tPrec@1 53.906 (49.427)\n",
            "Epoch: [6][300/391]\tTime 0.262 (0.279)\tLoss 1.3869 (1.3798)\tPrec@1 54.688 (49.766)\n",
            "Epoch: [6][350/391]\tTime 0.263 (0.277)\tLoss 1.3441 (1.3674)\tPrec@1 51.562 (50.407)\n",
            "epoch 6 training time consumed: 107.75s\n",
            "Test: [0/79]\tTime 3.958 (3.958)\tLoss 1.3350 (1.3350)\tPrec@1 52.344 (52.344)\n",
            "Test: [50/79]\tTime 0.086 (0.163)\tLoss 1.5718 (1.4702)\tPrec@1 48.438 (47.258)\n",
            " * Prec@1 47.420\n",
            "current lr 9.87958e-02\n",
            "Epoch: [7][0/391]\tTime 5.314 (5.314)\tLoss 1.2461 (1.2461)\tPrec@1 57.812 (57.812)\n",
            "Epoch: [7][50/391]\tTime 0.261 (0.360)\tLoss 1.2580 (1.2469)\tPrec@1 55.469 (55.545)\n",
            "Epoch: [7][100/391]\tTime 0.261 (0.311)\tLoss 1.2570 (1.2333)\tPrec@1 57.031 (56.884)\n",
            "Epoch: [7][150/391]\tTime 0.262 (0.295)\tLoss 1.3662 (1.2154)\tPrec@1 54.688 (57.378)\n",
            "Epoch: [7][200/391]\tTime 0.262 (0.287)\tLoss 1.2622 (1.2085)\tPrec@1 54.688 (57.579)\n",
            "Epoch: [7][250/391]\tTime 0.261 (0.282)\tLoss 0.9802 (1.2099)\tPrec@1 68.750 (57.750)\n",
            "Epoch: [7][300/391]\tTime 0.261 (0.278)\tLoss 1.2272 (1.2013)\tPrec@1 58.594 (58.077)\n",
            "Epoch: [7][350/391]\tTime 0.263 (0.276)\tLoss 1.2545 (1.1933)\tPrec@1 60.156 (58.405)\n",
            "epoch 7 training time consumed: 107.41s\n",
            "Test: [0/79]\tTime 4.060 (4.060)\tLoss 1.2758 (1.2758)\tPrec@1 58.594 (58.594)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 1.1512 (1.3244)\tPrec@1 64.844 (55.423)\n",
            " * Prec@1 55.630\n",
            "current lr 9.84292e-02\n",
            "Epoch: [8][0/391]\tTime 5.374 (5.374)\tLoss 1.1026 (1.1026)\tPrec@1 60.938 (60.938)\n",
            "Epoch: [8][50/391]\tTime 0.262 (0.361)\tLoss 0.9760 (1.0819)\tPrec@1 69.531 (62.898)\n",
            "Epoch: [8][100/391]\tTime 0.262 (0.312)\tLoss 1.0878 (1.0789)\tPrec@1 64.062 (62.771)\n",
            "Epoch: [8][150/391]\tTime 0.263 (0.295)\tLoss 1.0882 (1.0771)\tPrec@1 60.156 (63.157)\n",
            "Epoch: [8][200/391]\tTime 0.264 (0.287)\tLoss 1.2211 (1.0757)\tPrec@1 58.594 (63.242)\n",
            "Epoch: [8][250/391]\tTime 0.262 (0.282)\tLoss 1.0048 (1.0669)\tPrec@1 70.312 (63.630)\n",
            "Epoch: [8][300/391]\tTime 0.262 (0.279)\tLoss 1.0383 (1.0640)\tPrec@1 66.406 (63.787)\n",
            "Epoch: [8][350/391]\tTime 0.262 (0.277)\tLoss 1.0640 (1.0568)\tPrec@1 62.500 (64.100)\n",
            "epoch 8 training time consumed: 107.71s\n",
            "Test: [0/79]\tTime 4.068 (4.068)\tLoss 1.1865 (1.1865)\tPrec@1 60.156 (60.156)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.3384 (1.2260)\tPrec@1 53.906 (57.001)\n",
            " * Prec@1 57.470\n",
            "current lr 9.80147e-02\n",
            "Epoch: [9][0/391]\tTime 5.305 (5.305)\tLoss 1.2709 (1.2709)\tPrec@1 57.812 (57.812)\n",
            "Epoch: [9][50/391]\tTime 0.261 (0.361)\tLoss 1.0344 (1.0110)\tPrec@1 67.188 (65.794)\n",
            "Epoch: [9][100/391]\tTime 0.264 (0.312)\tLoss 0.9207 (0.9915)\tPrec@1 70.312 (67.002)\n",
            "Epoch: [9][150/391]\tTime 0.262 (0.296)\tLoss 1.1930 (0.9846)\tPrec@1 57.812 (67.115)\n",
            "Epoch: [9][200/391]\tTime 0.262 (0.287)\tLoss 0.9954 (0.9880)\tPrec@1 66.406 (66.950)\n",
            "Epoch: [9][250/391]\tTime 0.262 (0.282)\tLoss 0.8399 (0.9826)\tPrec@1 67.969 (67.184)\n",
            "Epoch: [9][300/391]\tTime 0.263 (0.279)\tLoss 1.0749 (0.9766)\tPrec@1 62.500 (67.411)\n",
            "Epoch: [9][350/391]\tTime 0.263 (0.277)\tLoss 0.8811 (0.9738)\tPrec@1 71.094 (67.639)\n",
            "epoch 9 training time consumed: 107.78s\n",
            "Test: [0/79]\tTime 4.119 (4.119)\tLoss 1.0373 (1.0373)\tPrec@1 64.844 (64.844)\n",
            "Test: [50/79]\tTime 0.086 (0.166)\tLoss 1.2444 (1.0816)\tPrec@1 57.812 (63.205)\n",
            " * Prec@1 63.240\n",
            "current lr 9.75528e-02\n",
            "Epoch: [10][0/391]\tTime 5.306 (5.306)\tLoss 0.8314 (0.8314)\tPrec@1 71.094 (71.094)\n",
            "Epoch: [10][50/391]\tTime 0.262 (0.361)\tLoss 0.9927 (0.9398)\tPrec@1 71.094 (69.164)\n",
            "Epoch: [10][100/391]\tTime 0.262 (0.312)\tLoss 0.7933 (0.9246)\tPrec@1 73.438 (69.748)\n",
            "Epoch: [10][150/391]\tTime 0.262 (0.296)\tLoss 0.7234 (0.9157)\tPrec@1 75.000 (69.909)\n",
            "Epoch: [10][200/391]\tTime 0.263 (0.288)\tLoss 1.0203 (0.9191)\tPrec@1 64.062 (69.974)\n",
            "Epoch: [10][250/391]\tTime 0.263 (0.283)\tLoss 0.7408 (0.9072)\tPrec@1 75.000 (70.375)\n",
            "Epoch: [10][300/391]\tTime 0.263 (0.280)\tLoss 0.9386 (0.9005)\tPrec@1 67.969 (70.629)\n",
            "Epoch: [10][350/391]\tTime 0.264 (0.277)\tLoss 0.7905 (0.8965)\tPrec@1 73.438 (70.804)\n",
            "epoch 10 training time consumed: 107.92s\n",
            "Test: [0/79]\tTime 3.973 (3.973)\tLoss 0.7914 (0.7914)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.9894 (0.8852)\tPrec@1 67.188 (70.037)\n",
            " * Prec@1 70.370\n",
            "current lr 9.70440e-02\n",
            "Epoch: [11][0/391]\tTime 5.311 (5.311)\tLoss 0.8055 (0.8055)\tPrec@1 72.656 (72.656)\n",
            "Epoch: [11][50/391]\tTime 0.261 (0.361)\tLoss 0.9563 (0.8476)\tPrec@1 70.312 (72.549)\n",
            "Epoch: [11][100/391]\tTime 0.262 (0.312)\tLoss 0.8026 (0.8402)\tPrec@1 73.438 (72.649)\n",
            "Epoch: [11][150/391]\tTime 0.263 (0.296)\tLoss 0.8774 (0.8353)\tPrec@1 68.750 (72.936)\n",
            "Epoch: [11][200/391]\tTime 0.264 (0.288)\tLoss 1.0260 (0.8431)\tPrec@1 62.500 (72.563)\n",
            "Epoch: [11][250/391]\tTime 0.263 (0.283)\tLoss 0.7185 (0.8340)\tPrec@1 78.906 (72.880)\n",
            "Epoch: [11][300/391]\tTime 0.264 (0.280)\tLoss 0.8300 (0.8328)\tPrec@1 75.000 (72.996)\n",
            "Epoch: [11][350/391]\tTime 0.263 (0.277)\tLoss 0.8783 (0.8326)\tPrec@1 71.875 (73.006)\n",
            "epoch 11 training time consumed: 107.93s\n",
            "Test: [0/79]\tTime 3.965 (3.965)\tLoss 0.8231 (0.8231)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.9785 (0.8606)\tPrec@1 67.969 (70.389)\n",
            " * Prec@1 70.600\n",
            "current lr 9.64888e-02\n",
            "Epoch: [12][0/391]\tTime 5.304 (5.304)\tLoss 0.6641 (0.6641)\tPrec@1 78.125 (78.125)\n",
            "Epoch: [12][50/391]\tTime 0.262 (0.361)\tLoss 0.8268 (0.8025)\tPrec@1 75.781 (74.893)\n",
            "Epoch: [12][100/391]\tTime 0.263 (0.312)\tLoss 0.8108 (0.7894)\tPrec@1 71.094 (75.023)\n",
            "Epoch: [12][150/391]\tTime 0.263 (0.296)\tLoss 0.8120 (0.7887)\tPrec@1 74.219 (74.907)\n",
            "Epoch: [12][200/391]\tTime 0.262 (0.288)\tLoss 0.8962 (0.7827)\tPrec@1 71.875 (75.047)\n",
            "Epoch: [12][250/391]\tTime 0.263 (0.283)\tLoss 0.7269 (0.7817)\tPrec@1 74.219 (75.056)\n",
            "Epoch: [12][300/391]\tTime 0.263 (0.279)\tLoss 0.6697 (0.7783)\tPrec@1 79.688 (75.226)\n",
            "Epoch: [12][350/391]\tTime 0.264 (0.277)\tLoss 0.9127 (0.7787)\tPrec@1 70.312 (75.154)\n",
            "epoch 12 training time consumed: 107.83s\n",
            "Test: [0/79]\tTime 4.080 (4.080)\tLoss 1.2647 (1.2647)\tPrec@1 62.500 (62.500)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.3195 (1.2784)\tPrec@1 62.500 (62.010)\n",
            " * Prec@1 61.970\n",
            "current lr 9.58877e-02\n",
            "Epoch: [13][0/391]\tTime 5.287 (5.287)\tLoss 0.7827 (0.7827)\tPrec@1 75.000 (75.000)\n",
            "Epoch: [13][50/391]\tTime 0.263 (0.360)\tLoss 0.7892 (0.7365)\tPrec@1 73.438 (75.904)\n",
            "Epoch: [13][100/391]\tTime 0.262 (0.312)\tLoss 0.6953 (0.7564)\tPrec@1 78.906 (75.418)\n",
            "Epoch: [13][150/391]\tTime 0.263 (0.296)\tLoss 0.8443 (0.7483)\tPrec@1 72.656 (75.833)\n",
            "Epoch: [13][200/391]\tTime 0.262 (0.287)\tLoss 0.6385 (0.7464)\tPrec@1 82.812 (76.077)\n",
            "Epoch: [13][250/391]\tTime 0.263 (0.283)\tLoss 0.6509 (0.7415)\tPrec@1 78.125 (76.142)\n",
            "Epoch: [13][300/391]\tTime 0.263 (0.279)\tLoss 0.7662 (0.7417)\tPrec@1 75.781 (76.100)\n",
            "Epoch: [13][350/391]\tTime 0.263 (0.277)\tLoss 0.6814 (0.7415)\tPrec@1 77.344 (76.115)\n",
            "epoch 13 training time consumed: 107.82s\n",
            "Test: [0/79]\tTime 4.076 (4.076)\tLoss 1.0884 (1.0884)\tPrec@1 67.188 (67.188)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.2787 (1.0807)\tPrec@1 69.531 (68.735)\n",
            " * Prec@1 68.750\n",
            "current lr 9.52414e-02\n",
            "Epoch: [14][0/391]\tTime 5.464 (5.464)\tLoss 0.6580 (0.6580)\tPrec@1 75.781 (75.781)\n",
            "Epoch: [14][50/391]\tTime 0.262 (0.364)\tLoss 0.7284 (0.7141)\tPrec@1 72.656 (77.145)\n",
            "Epoch: [14][100/391]\tTime 0.263 (0.314)\tLoss 0.7565 (0.7135)\tPrec@1 75.000 (77.197)\n",
            "Epoch: [14][150/391]\tTime 0.262 (0.297)\tLoss 0.7512 (0.7183)\tPrec@1 75.781 (77.018)\n",
            "Epoch: [14][200/391]\tTime 0.263 (0.288)\tLoss 0.6722 (0.7204)\tPrec@1 81.250 (76.986)\n",
            "Epoch: [14][250/391]\tTime 0.264 (0.283)\tLoss 0.6493 (0.7115)\tPrec@1 78.125 (77.285)\n",
            "Epoch: [14][300/391]\tTime 0.264 (0.280)\tLoss 0.7210 (0.7141)\tPrec@1 77.344 (77.224)\n",
            "Epoch: [14][350/391]\tTime 0.263 (0.278)\tLoss 0.7049 (0.7128)\tPrec@1 78.125 (77.290)\n",
            "epoch 14 training time consumed: 108.04s\n",
            "Test: [0/79]\tTime 4.003 (4.003)\tLoss 0.7956 (0.7956)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.086 (0.163)\tLoss 0.7697 (0.7055)\tPrec@1 74.219 (76.700)\n",
            " * Prec@1 76.950\n",
            "current lr 9.45503e-02\n",
            "Epoch: [15][0/391]\tTime 5.336 (5.336)\tLoss 0.5891 (0.5891)\tPrec@1 79.688 (79.688)\n",
            "Epoch: [15][50/391]\tTime 0.263 (0.361)\tLoss 0.5650 (0.6656)\tPrec@1 80.469 (78.922)\n",
            "Epoch: [15][100/391]\tTime 0.263 (0.312)\tLoss 0.6167 (0.6603)\tPrec@1 75.781 (79.038)\n",
            "Epoch: [15][150/391]\tTime 0.262 (0.296)\tLoss 0.6069 (0.6567)\tPrec@1 79.688 (79.149)\n",
            "Epoch: [15][200/391]\tTime 0.263 (0.288)\tLoss 0.6788 (0.6685)\tPrec@1 78.906 (78.755)\n",
            "Epoch: [15][250/391]\tTime 0.263 (0.283)\tLoss 0.8572 (0.6736)\tPrec@1 71.875 (78.614)\n",
            "Epoch: [15][300/391]\tTime 0.263 (0.279)\tLoss 0.6065 (0.6720)\tPrec@1 85.938 (78.758)\n",
            "Epoch: [15][350/391]\tTime 0.263 (0.277)\tLoss 0.6194 (0.6695)\tPrec@1 82.031 (78.868)\n",
            "epoch 15 training time consumed: 107.82s\n",
            "Test: [0/79]\tTime 4.093 (4.093)\tLoss 0.9157 (0.9157)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.1655 (1.0471)\tPrec@1 63.281 (66.988)\n",
            " * Prec@1 67.050\n",
            "current lr 9.38153e-02\n",
            "Epoch: [16][0/391]\tTime 5.326 (5.326)\tLoss 0.4251 (0.4251)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [16][50/391]\tTime 0.263 (0.361)\tLoss 0.5835 (0.6343)\tPrec@1 76.562 (79.963)\n",
            "Epoch: [16][100/391]\tTime 0.262 (0.313)\tLoss 0.5797 (0.6377)\tPrec@1 78.906 (79.827)\n",
            "Epoch: [16][150/391]\tTime 0.263 (0.296)\tLoss 0.5590 (0.6341)\tPrec@1 81.250 (79.874)\n",
            "Epoch: [16][200/391]\tTime 0.264 (0.288)\tLoss 0.6381 (0.6381)\tPrec@1 78.906 (79.707)\n",
            "Epoch: [16][250/391]\tTime 0.262 (0.283)\tLoss 0.5743 (0.6389)\tPrec@1 83.594 (79.697)\n",
            "Epoch: [16][300/391]\tTime 0.262 (0.280)\tLoss 0.6967 (0.6423)\tPrec@1 79.688 (79.633)\n",
            "Epoch: [16][350/391]\tTime 0.263 (0.277)\tLoss 0.8136 (0.6422)\tPrec@1 74.219 (79.636)\n",
            "epoch 16 training time consumed: 107.94s\n",
            "Test: [0/79]\tTime 3.953 (3.953)\tLoss 0.5334 (0.5334)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.7523 (0.6318)\tPrec@1 78.906 (79.718)\n",
            " * Prec@1 79.430\n",
            "current lr 9.30371e-02\n",
            "Epoch: [17][0/391]\tTime 5.491 (5.491)\tLoss 0.6461 (0.6461)\tPrec@1 78.125 (78.125)\n",
            "Epoch: [17][50/391]\tTime 0.263 (0.365)\tLoss 0.6945 (0.6100)\tPrec@1 73.438 (80.576)\n",
            "Epoch: [17][100/391]\tTime 0.262 (0.314)\tLoss 0.8678 (0.6219)\tPrec@1 75.000 (80.384)\n",
            "Epoch: [17][150/391]\tTime 0.263 (0.297)\tLoss 0.6618 (0.6236)\tPrec@1 81.250 (80.205)\n",
            "Epoch: [17][200/391]\tTime 0.263 (0.289)\tLoss 0.6428 (0.6260)\tPrec@1 76.562 (80.239)\n",
            "Epoch: [17][250/391]\tTime 0.264 (0.283)\tLoss 0.7558 (0.6266)\tPrec@1 75.781 (80.167)\n",
            "Epoch: [17][300/391]\tTime 0.263 (0.280)\tLoss 0.5518 (0.6280)\tPrec@1 81.250 (80.118)\n",
            "Epoch: [17][350/391]\tTime 0.263 (0.278)\tLoss 0.7694 (0.6238)\tPrec@1 78.125 (80.253)\n",
            "epoch 17 training time consumed: 108.10s\n",
            "Test: [0/79]\tTime 4.004 (4.004)\tLoss 0.7460 (0.7460)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.8447 (0.8278)\tPrec@1 78.125 (75.015)\n",
            " * Prec@1 74.770\n",
            "current lr 9.22164e-02\n",
            "Epoch: [18][0/391]\tTime 5.310 (5.310)\tLoss 0.6477 (0.6477)\tPrec@1 80.469 (80.469)\n",
            "Epoch: [18][50/391]\tTime 0.262 (0.361)\tLoss 0.7608 (0.6024)\tPrec@1 73.438 (80.821)\n",
            "Epoch: [18][100/391]\tTime 0.262 (0.312)\tLoss 0.5164 (0.5884)\tPrec@1 82.031 (81.335)\n",
            "Epoch: [18][150/391]\tTime 0.263 (0.296)\tLoss 0.7846 (0.5980)\tPrec@1 74.219 (80.981)\n",
            "Epoch: [18][200/391]\tTime 0.264 (0.288)\tLoss 0.5682 (0.5993)\tPrec@1 79.688 (80.943)\n",
            "Epoch: [18][250/391]\tTime 0.263 (0.283)\tLoss 0.5869 (0.5961)\tPrec@1 80.469 (81.057)\n",
            "Epoch: [18][300/391]\tTime 0.262 (0.280)\tLoss 0.5061 (0.5958)\tPrec@1 83.594 (81.074)\n",
            "Epoch: [18][350/391]\tTime 0.263 (0.277)\tLoss 0.6120 (0.5965)\tPrec@1 82.031 (81.139)\n",
            "epoch 18 training time consumed: 107.89s\n",
            "Test: [0/79]\tTime 4.002 (4.002)\tLoss 0.7836 (0.7836)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.8860 (0.7392)\tPrec@1 73.438 (76.700)\n",
            " * Prec@1 76.860\n",
            "current lr 9.13540e-02\n",
            "Epoch: [19][0/391]\tTime 5.315 (5.315)\tLoss 0.5924 (0.5924)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [19][50/391]\tTime 0.261 (0.361)\tLoss 0.8226 (0.6112)\tPrec@1 75.781 (81.327)\n",
            "Epoch: [19][100/391]\tTime 0.262 (0.312)\tLoss 0.5130 (0.5904)\tPrec@1 85.156 (81.776)\n",
            "Epoch: [19][150/391]\tTime 0.262 (0.296)\tLoss 0.7649 (0.5857)\tPrec@1 80.469 (81.710)\n",
            "Epoch: [19][200/391]\tTime 0.263 (0.288)\tLoss 0.3740 (0.5853)\tPrec@1 85.156 (81.817)\n",
            "Epoch: [19][250/391]\tTime 0.263 (0.283)\tLoss 0.5544 (0.5828)\tPrec@1 79.688 (81.854)\n",
            "Epoch: [19][300/391]\tTime 0.263 (0.279)\tLoss 0.5452 (0.5910)\tPrec@1 79.688 (81.515)\n",
            "Epoch: [19][350/391]\tTime 0.262 (0.277)\tLoss 0.6125 (0.5931)\tPrec@1 80.469 (81.421)\n",
            "epoch 19 training time consumed: 107.88s\n",
            "Test: [0/79]\tTime 4.026 (4.026)\tLoss 0.6012 (0.6012)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.8270 (0.6799)\tPrec@1 74.219 (78.064)\n",
            " * Prec@1 78.210\n",
            "current lr 9.04508e-02\n",
            "Epoch: [20][0/391]\tTime 5.324 (5.324)\tLoss 0.4775 (0.4775)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [20][50/391]\tTime 0.261 (0.360)\tLoss 0.5959 (0.5193)\tPrec@1 80.469 (83.686)\n",
            "Epoch: [20][100/391]\tTime 0.262 (0.311)\tLoss 0.6023 (0.5455)\tPrec@1 81.250 (82.843)\n",
            "Epoch: [20][150/391]\tTime 0.261 (0.295)\tLoss 0.5003 (0.5551)\tPrec@1 85.156 (82.393)\n",
            "Epoch: [20][200/391]\tTime 0.262 (0.287)\tLoss 0.6593 (0.5589)\tPrec@1 82.812 (82.346)\n",
            "Epoch: [20][250/391]\tTime 0.261 (0.282)\tLoss 0.4636 (0.5611)\tPrec@1 87.500 (82.280)\n",
            "Epoch: [20][300/391]\tTime 0.262 (0.278)\tLoss 0.6498 (0.5604)\tPrec@1 82.031 (82.273)\n",
            "Epoch: [20][350/391]\tTime 0.262 (0.276)\tLoss 0.7949 (0.5608)\tPrec@1 76.562 (82.254)\n",
            "epoch 20 training time consumed: 107.47s\n",
            "Test: [0/79]\tTime 4.026 (4.026)\tLoss 0.7396 (0.7396)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.088 (0.164)\tLoss 0.9114 (0.8417)\tPrec@1 77.344 (74.755)\n",
            " * Prec@1 75.110\n",
            "current lr 8.95078e-02\n",
            "Epoch: [21][0/391]\tTime 5.334 (5.334)\tLoss 0.5195 (0.5195)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [21][50/391]\tTime 0.262 (0.361)\tLoss 0.6320 (0.5405)\tPrec@1 78.906 (82.981)\n",
            "Epoch: [21][100/391]\tTime 0.264 (0.312)\tLoss 0.4821 (0.5611)\tPrec@1 86.719 (82.078)\n",
            "Epoch: [21][150/391]\tTime 0.263 (0.296)\tLoss 0.5164 (0.5620)\tPrec@1 84.375 (82.347)\n",
            "Epoch: [21][200/391]\tTime 0.262 (0.288)\tLoss 0.3170 (0.5584)\tPrec@1 91.406 (82.354)\n",
            "Epoch: [21][250/391]\tTime 0.264 (0.283)\tLoss 0.8619 (0.5640)\tPrec@1 71.875 (82.125)\n",
            "Epoch: [21][300/391]\tTime 0.262 (0.279)\tLoss 0.7165 (0.5626)\tPrec@1 81.250 (82.143)\n",
            "Epoch: [21][350/391]\tTime 0.264 (0.277)\tLoss 0.5270 (0.5583)\tPrec@1 83.594 (82.240)\n",
            "epoch 21 training time consumed: 107.86s\n",
            "Test: [0/79]\tTime 4.069 (4.069)\tLoss 0.8418 (0.8418)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.086 (0.165)\tLoss 0.8844 (0.7877)\tPrec@1 72.656 (75.031)\n",
            " * Prec@1 74.790\n",
            "current lr 8.85257e-02\n",
            "Epoch: [22][0/391]\tTime 5.297 (5.297)\tLoss 0.4852 (0.4852)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [22][50/391]\tTime 0.261 (0.361)\tLoss 0.4994 (0.5201)\tPrec@1 85.938 (83.410)\n",
            "Epoch: [22][100/391]\tTime 0.262 (0.312)\tLoss 0.5890 (0.5370)\tPrec@1 78.906 (82.766)\n",
            "Epoch: [22][150/391]\tTime 0.262 (0.296)\tLoss 0.6297 (0.5352)\tPrec@1 82.031 (82.823)\n",
            "Epoch: [22][200/391]\tTime 0.264 (0.288)\tLoss 0.5256 (0.5335)\tPrec@1 83.594 (82.952)\n",
            "Epoch: [22][250/391]\tTime 0.263 (0.283)\tLoss 0.5927 (0.5344)\tPrec@1 81.250 (82.943)\n",
            "Epoch: [22][300/391]\tTime 0.263 (0.280)\tLoss 0.5059 (0.5349)\tPrec@1 85.938 (83.031)\n",
            "Epoch: [22][350/391]\tTime 0.262 (0.277)\tLoss 0.4731 (0.5348)\tPrec@1 82.812 (83.100)\n",
            "epoch 22 training time consumed: 107.89s\n",
            "Test: [0/79]\tTime 4.165 (4.165)\tLoss 0.8389 (0.8389)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.088 (0.167)\tLoss 0.8665 (0.8328)\tPrec@1 76.562 (75.873)\n",
            " * Prec@1 75.810\n",
            "current lr 8.75056e-02\n",
            "Epoch: [23][0/391]\tTime 5.330 (5.330)\tLoss 0.5295 (0.5295)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [23][50/391]\tTime 0.262 (0.361)\tLoss 0.5051 (0.5188)\tPrec@1 85.156 (83.747)\n",
            "Epoch: [23][100/391]\tTime 0.262 (0.312)\tLoss 0.4061 (0.5283)\tPrec@1 85.938 (83.478)\n",
            "Epoch: [23][150/391]\tTime 0.263 (0.296)\tLoss 0.6698 (0.5245)\tPrec@1 77.344 (83.547)\n",
            "Epoch: [23][200/391]\tTime 0.263 (0.288)\tLoss 0.4911 (0.5282)\tPrec@1 86.719 (83.361)\n",
            "Epoch: [23][250/391]\tTime 0.264 (0.283)\tLoss 0.4406 (0.5281)\tPrec@1 86.719 (83.500)\n",
            "Epoch: [23][300/391]\tTime 0.263 (0.280)\tLoss 0.4927 (0.5237)\tPrec@1 82.812 (83.620)\n",
            "Epoch: [23][350/391]\tTime 0.264 (0.277)\tLoss 0.5828 (0.5225)\tPrec@1 83.594 (83.672)\n",
            "epoch 23 training time consumed: 107.94s\n",
            "Test: [0/79]\tTime 3.960 (3.960)\tLoss 1.0775 (1.0775)\tPrec@1 68.750 (68.750)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 1.1889 (1.0329)\tPrec@1 67.188 (71.247)\n",
            " * Prec@1 71.050\n",
            "current lr 8.64484e-02\n",
            "Epoch: [24][0/391]\tTime 5.324 (5.324)\tLoss 0.5927 (0.5927)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [24][50/391]\tTime 0.262 (0.362)\tLoss 0.4125 (0.5096)\tPrec@1 84.375 (83.931)\n",
            "Epoch: [24][100/391]\tTime 0.263 (0.313)\tLoss 0.4653 (0.4987)\tPrec@1 85.938 (84.336)\n",
            "Epoch: [24][150/391]\tTime 0.263 (0.296)\tLoss 0.5279 (0.4928)\tPrec@1 82.031 (84.608)\n",
            "Epoch: [24][200/391]\tTime 0.263 (0.288)\tLoss 0.5650 (0.4940)\tPrec@1 82.031 (84.604)\n",
            "Epoch: [24][250/391]\tTime 0.263 (0.283)\tLoss 0.6796 (0.5002)\tPrec@1 76.562 (84.431)\n",
            "Epoch: [24][300/391]\tTime 0.263 (0.280)\tLoss 0.4184 (0.5066)\tPrec@1 88.281 (84.196)\n",
            "Epoch: [24][350/391]\tTime 0.263 (0.277)\tLoss 0.6539 (0.5079)\tPrec@1 82.812 (84.192)\n",
            "epoch 24 training time consumed: 107.98s\n",
            "Test: [0/79]\tTime 4.073 (4.073)\tLoss 1.3384 (1.3384)\tPrec@1 59.375 (59.375)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.4014 (1.3840)\tPrec@1 64.844 (65.901)\n",
            " * Prec@1 65.830\n",
            "current lr 8.53553e-02\n",
            "Epoch: [25][0/391]\tTime 5.387 (5.387)\tLoss 0.5046 (0.5046)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [25][50/391]\tTime 0.263 (0.362)\tLoss 0.5557 (0.5329)\tPrec@1 81.250 (83.670)\n",
            "Epoch: [25][100/391]\tTime 0.262 (0.313)\tLoss 0.5032 (0.4975)\tPrec@1 84.375 (84.383)\n",
            "Epoch: [25][150/391]\tTime 0.262 (0.296)\tLoss 0.4469 (0.5040)\tPrec@1 85.938 (84.127)\n",
            "Epoch: [25][200/391]\tTime 0.263 (0.288)\tLoss 0.6461 (0.5073)\tPrec@1 77.344 (83.990)\n",
            "Epoch: [25][250/391]\tTime 0.262 (0.283)\tLoss 0.6613 (0.5071)\tPrec@1 79.688 (84.076)\n",
            "Epoch: [25][300/391]\tTime 0.263 (0.280)\tLoss 0.3858 (0.5105)\tPrec@1 86.719 (83.942)\n",
            "Epoch: [25][350/391]\tTime 0.263 (0.277)\tLoss 0.4238 (0.5065)\tPrec@1 89.844 (84.057)\n",
            "epoch 25 training time consumed: 107.94s\n",
            "Test: [0/79]\tTime 4.027 (4.027)\tLoss 0.6038 (0.6038)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.6774 (0.5730)\tPrec@1 81.250 (82.445)\n",
            " * Prec@1 82.360\n",
            "current lr 8.42274e-02\n",
            "Epoch: [26][0/391]\tTime 5.507 (5.507)\tLoss 0.5195 (0.5195)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [26][50/391]\tTime 0.261 (0.365)\tLoss 0.4385 (0.5097)\tPrec@1 88.281 (84.130)\n",
            "Epoch: [26][100/391]\tTime 0.262 (0.314)\tLoss 0.4341 (0.5050)\tPrec@1 87.500 (84.329)\n",
            "Epoch: [26][150/391]\tTime 0.262 (0.297)\tLoss 0.5554 (0.5043)\tPrec@1 81.250 (84.215)\n",
            "Epoch: [26][200/391]\tTime 0.263 (0.289)\tLoss 0.4700 (0.4982)\tPrec@1 88.281 (84.348)\n",
            "Epoch: [26][250/391]\tTime 0.264 (0.283)\tLoss 0.5053 (0.4994)\tPrec@1 85.156 (84.232)\n",
            "Epoch: [26][300/391]\tTime 0.264 (0.280)\tLoss 0.4830 (0.4958)\tPrec@1 86.719 (84.333)\n",
            "Epoch: [26][350/391]\tTime 0.264 (0.278)\tLoss 0.6273 (0.4951)\tPrec@1 81.250 (84.364)\n",
            "epoch 26 training time consumed: 108.12s\n",
            "Test: [0/79]\tTime 4.138 (4.138)\tLoss 0.7783 (0.7783)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.9824 (0.9243)\tPrec@1 75.000 (73.254)\n",
            " * Prec@1 73.130\n",
            "current lr 8.30656e-02\n",
            "Epoch: [27][0/391]\tTime 5.324 (5.324)\tLoss 0.5075 (0.5075)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [27][50/391]\tTime 0.262 (0.361)\tLoss 0.4091 (0.4548)\tPrec@1 85.938 (85.555)\n",
            "Epoch: [27][100/391]\tTime 0.263 (0.312)\tLoss 0.5849 (0.4608)\tPrec@1 80.469 (85.094)\n",
            "Epoch: [27][150/391]\tTime 0.262 (0.296)\tLoss 0.3849 (0.4785)\tPrec@1 88.281 (84.634)\n",
            "Epoch: [27][200/391]\tTime 0.262 (0.288)\tLoss 0.5526 (0.4805)\tPrec@1 81.250 (84.713)\n",
            "Epoch: [27][250/391]\tTime 0.263 (0.283)\tLoss 0.4998 (0.4830)\tPrec@1 85.156 (84.658)\n",
            "Epoch: [27][300/391]\tTime 0.262 (0.280)\tLoss 0.5174 (0.4835)\tPrec@1 83.594 (84.614)\n",
            "Epoch: [27][350/391]\tTime 0.263 (0.277)\tLoss 0.6349 (0.4843)\tPrec@1 80.469 (84.669)\n",
            "epoch 27 training time consumed: 107.90s\n",
            "Test: [0/79]\tTime 3.984 (3.984)\tLoss 0.7378 (0.7378)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.086 (0.163)\tLoss 0.8911 (0.8401)\tPrec@1 75.781 (73.698)\n",
            " * Prec@1 73.840\n",
            "current lr 8.18712e-02\n",
            "Epoch: [28][0/391]\tTime 5.338 (5.338)\tLoss 0.5016 (0.5016)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [28][50/391]\tTime 0.262 (0.362)\tLoss 0.4304 (0.4373)\tPrec@1 89.062 (86.244)\n",
            "Epoch: [28][100/391]\tTime 0.262 (0.312)\tLoss 0.8230 (0.4630)\tPrec@1 76.562 (85.489)\n",
            "Epoch: [28][150/391]\tTime 0.263 (0.296)\tLoss 0.3132 (0.4598)\tPrec@1 90.625 (85.560)\n",
            "Epoch: [28][200/391]\tTime 0.263 (0.288)\tLoss 0.3189 (0.4595)\tPrec@1 89.844 (85.642)\n",
            "Epoch: [28][250/391]\tTime 0.263 (0.283)\tLoss 0.4129 (0.4614)\tPrec@1 87.500 (85.601)\n",
            "Epoch: [28][300/391]\tTime 0.263 (0.280)\tLoss 0.2776 (0.4658)\tPrec@1 94.531 (85.411)\n",
            "Epoch: [28][350/391]\tTime 0.263 (0.277)\tLoss 0.6558 (0.4718)\tPrec@1 83.594 (85.199)\n",
            "epoch 28 training time consumed: 107.99s\n",
            "Test: [0/79]\tTime 4.097 (4.097)\tLoss 0.5723 (0.5723)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.8257 (0.7018)\tPrec@1 75.000 (79.688)\n",
            " * Prec@1 79.860\n",
            "current lr 8.06454e-02\n",
            "Epoch: [29][0/391]\tTime 5.353 (5.353)\tLoss 0.4983 (0.4983)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [29][50/391]\tTime 0.262 (0.362)\tLoss 0.3460 (0.4351)\tPrec@1 89.062 (85.938)\n",
            "Epoch: [29][100/391]\tTime 0.262 (0.313)\tLoss 0.5246 (0.4369)\tPrec@1 81.250 (85.791)\n",
            "Epoch: [29][150/391]\tTime 0.263 (0.296)\tLoss 0.4431 (0.4379)\tPrec@1 86.719 (85.938)\n",
            "Epoch: [29][200/391]\tTime 0.263 (0.288)\tLoss 0.4079 (0.4459)\tPrec@1 86.719 (85.778)\n",
            "Epoch: [29][250/391]\tTime 0.264 (0.283)\tLoss 0.3172 (0.4503)\tPrec@1 89.844 (85.623)\n",
            "Epoch: [29][300/391]\tTime 0.264 (0.280)\tLoss 0.6363 (0.4541)\tPrec@1 85.156 (85.579)\n",
            "Epoch: [29][350/391]\tTime 0.263 (0.277)\tLoss 0.3975 (0.4557)\tPrec@1 85.156 (85.590)\n",
            "epoch 29 training time consumed: 107.99s\n",
            "Test: [0/79]\tTime 4.140 (4.140)\tLoss 1.6150 (1.6150)\tPrec@1 53.906 (53.906)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 1.7645 (1.3998)\tPrec@1 57.812 (62.240)\n",
            " * Prec@1 62.640\n",
            "current lr 7.93893e-02\n",
            "Epoch: [30][0/391]\tTime 5.472 (5.472)\tLoss 0.6241 (0.6241)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [30][50/391]\tTime 0.263 (0.365)\tLoss 0.4227 (0.4650)\tPrec@1 83.594 (84.957)\n",
            "Epoch: [30][100/391]\tTime 0.263 (0.314)\tLoss 0.4264 (0.4546)\tPrec@1 86.719 (85.473)\n",
            "Epoch: [30][150/391]\tTime 0.262 (0.297)\tLoss 0.4197 (0.4537)\tPrec@1 87.500 (85.658)\n",
            "Epoch: [30][200/391]\tTime 0.263 (0.289)\tLoss 0.4215 (0.4556)\tPrec@1 82.031 (85.677)\n",
            "Epoch: [30][250/391]\tTime 0.263 (0.284)\tLoss 0.4437 (0.4498)\tPrec@1 85.156 (85.829)\n",
            "Epoch: [30][300/391]\tTime 0.264 (0.280)\tLoss 0.5746 (0.4455)\tPrec@1 82.031 (85.976)\n",
            "Epoch: [30][350/391]\tTime 0.263 (0.278)\tLoss 0.3568 (0.4466)\tPrec@1 88.281 (85.955)\n",
            "epoch 30 training time consumed: 108.11s\n",
            "Test: [0/79]\tTime 3.940 (3.940)\tLoss 1.1420 (1.1420)\tPrec@1 67.969 (67.969)\n",
            "Test: [50/79]\tTime 0.086 (0.162)\tLoss 1.0798 (0.9829)\tPrec@1 71.094 (73.882)\n",
            " * Prec@1 73.970\n",
            "current lr 7.81042e-02\n",
            "Epoch: [31][0/391]\tTime 5.498 (5.498)\tLoss 0.4770 (0.4770)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [31][50/391]\tTime 0.262 (0.365)\tLoss 0.5768 (0.4434)\tPrec@1 84.375 (85.983)\n",
            "Epoch: [31][100/391]\tTime 0.261 (0.314)\tLoss 0.4974 (0.4339)\tPrec@1 82.031 (86.301)\n",
            "Epoch: [31][150/391]\tTime 0.262 (0.297)\tLoss 0.3901 (0.4286)\tPrec@1 86.719 (86.310)\n",
            "Epoch: [31][200/391]\tTime 0.262 (0.289)\tLoss 0.3556 (0.4338)\tPrec@1 90.625 (86.299)\n",
            "Epoch: [31][250/391]\tTime 0.262 (0.283)\tLoss 0.5914 (0.4375)\tPrec@1 79.688 (86.205)\n",
            "Epoch: [31][300/391]\tTime 0.262 (0.280)\tLoss 0.5156 (0.4403)\tPrec@1 82.812 (86.044)\n",
            "Epoch: [31][350/391]\tTime 0.263 (0.278)\tLoss 0.4464 (0.4396)\tPrec@1 85.938 (86.033)\n",
            "epoch 31 training time consumed: 108.02s\n",
            "Test: [0/79]\tTime 4.053 (4.053)\tLoss 1.1600 (1.1600)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 1.3028 (1.0027)\tPrec@1 64.844 (72.411)\n",
            " * Prec@1 72.340\n",
            "current lr 7.67913e-02\n",
            "Epoch: [32][0/391]\tTime 5.364 (5.364)\tLoss 0.5527 (0.5527)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [32][50/391]\tTime 0.262 (0.362)\tLoss 0.3432 (0.4479)\tPrec@1 89.062 (86.075)\n",
            "Epoch: [32][100/391]\tTime 0.263 (0.313)\tLoss 0.5016 (0.4270)\tPrec@1 84.375 (86.796)\n",
            "Epoch: [32][150/391]\tTime 0.262 (0.296)\tLoss 0.3702 (0.4266)\tPrec@1 89.062 (86.719)\n",
            "Epoch: [32][200/391]\tTime 0.263 (0.288)\tLoss 0.4103 (0.4202)\tPrec@1 85.938 (86.851)\n",
            "Epoch: [32][250/391]\tTime 0.264 (0.283)\tLoss 0.3713 (0.4213)\tPrec@1 88.281 (86.837)\n",
            "Epoch: [32][300/391]\tTime 0.263 (0.280)\tLoss 0.3698 (0.4176)\tPrec@1 91.406 (86.911)\n",
            "Epoch: [32][350/391]\tTime 0.263 (0.277)\tLoss 0.3988 (0.4198)\tPrec@1 88.281 (86.910)\n",
            "epoch 32 training time consumed: 107.98s\n",
            "Test: [0/79]\tTime 4.156 (4.156)\tLoss 0.4603 (0.4603)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.087 (0.167)\tLoss 0.5175 (0.5616)\tPrec@1 79.688 (82.966)\n",
            " * Prec@1 82.650\n",
            "current lr 7.54521e-02\n",
            "Epoch: [33][0/391]\tTime 5.471 (5.471)\tLoss 0.5609 (0.5609)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [33][50/391]\tTime 0.263 (0.365)\tLoss 0.2994 (0.4034)\tPrec@1 90.625 (86.918)\n",
            "Epoch: [33][100/391]\tTime 0.263 (0.314)\tLoss 0.5308 (0.4049)\tPrec@1 83.594 (87.044)\n",
            "Epoch: [33][150/391]\tTime 0.263 (0.297)\tLoss 0.5367 (0.4027)\tPrec@1 85.156 (87.117)\n",
            "Epoch: [33][200/391]\tTime 0.262 (0.289)\tLoss 0.3883 (0.4103)\tPrec@1 87.500 (86.995)\n",
            "Epoch: [33][250/391]\tTime 0.263 (0.284)\tLoss 0.4539 (0.4144)\tPrec@1 85.938 (86.818)\n",
            "Epoch: [33][300/391]\tTime 0.262 (0.280)\tLoss 0.4966 (0.4154)\tPrec@1 82.031 (86.851)\n",
            "Epoch: [33][350/391]\tTime 0.262 (0.278)\tLoss 0.3942 (0.4187)\tPrec@1 85.156 (86.739)\n",
            "epoch 33 training time consumed: 108.06s\n",
            "Test: [0/79]\tTime 4.006 (4.006)\tLoss 0.6138 (0.6138)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.8871 (0.7994)\tPrec@1 76.562 (76.915)\n",
            " * Prec@1 77.200\n",
            "current lr 7.40877e-02\n",
            "Epoch: [34][0/391]\tTime 5.489 (5.489)\tLoss 0.4209 (0.4209)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [34][50/391]\tTime 0.263 (0.364)\tLoss 0.4493 (0.3901)\tPrec@1 83.594 (87.485)\n",
            "Epoch: [34][100/391]\tTime 0.263 (0.314)\tLoss 0.5451 (0.4078)\tPrec@1 82.812 (86.966)\n",
            "Epoch: [34][150/391]\tTime 0.262 (0.297)\tLoss 0.4326 (0.4028)\tPrec@1 85.156 (87.148)\n",
            "Epoch: [34][200/391]\tTime 0.263 (0.288)\tLoss 0.4090 (0.4070)\tPrec@1 82.031 (87.072)\n",
            "Epoch: [34][250/391]\tTime 0.262 (0.283)\tLoss 0.3661 (0.4137)\tPrec@1 85.938 (86.896)\n",
            "Epoch: [34][300/391]\tTime 0.263 (0.280)\tLoss 0.5476 (0.4117)\tPrec@1 84.375 (86.981)\n",
            "Epoch: [34][350/391]\tTime 0.263 (0.278)\tLoss 0.4306 (0.4114)\tPrec@1 87.500 (86.970)\n",
            "epoch 34 training time consumed: 108.02s\n",
            "Test: [0/79]\tTime 4.042 (4.042)\tLoss 0.4830 (0.4830)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.7385 (0.6359)\tPrec@1 78.125 (81.342)\n",
            " * Prec@1 81.280\n",
            "current lr 7.26995e-02\n",
            "Epoch: [35][0/391]\tTime 5.536 (5.536)\tLoss 0.3556 (0.3556)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [35][50/391]\tTime 0.261 (0.364)\tLoss 0.3409 (0.3784)\tPrec@1 87.500 (88.021)\n",
            "Epoch: [35][100/391]\tTime 0.262 (0.314)\tLoss 0.3673 (0.3993)\tPrec@1 86.719 (87.276)\n",
            "Epoch: [35][150/391]\tTime 0.261 (0.296)\tLoss 0.5218 (0.4009)\tPrec@1 82.031 (87.314)\n",
            "Epoch: [35][200/391]\tTime 0.262 (0.288)\tLoss 0.4209 (0.3966)\tPrec@1 87.500 (87.446)\n",
            "Epoch: [35][250/391]\tTime 0.262 (0.283)\tLoss 0.4200 (0.4005)\tPrec@1 85.156 (87.260)\n",
            "Epoch: [35][300/391]\tTime 0.261 (0.279)\tLoss 0.5287 (0.3985)\tPrec@1 85.156 (87.349)\n",
            "Epoch: [35][350/391]\tTime 0.262 (0.277)\tLoss 0.4325 (0.4010)\tPrec@1 88.281 (87.284)\n",
            "epoch 35 training time consumed: 107.68s\n",
            "Test: [0/79]\tTime 4.027 (4.027)\tLoss 0.7707 (0.7707)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.6720 (0.6703)\tPrec@1 80.469 (79.779)\n",
            " * Prec@1 79.770\n",
            "current lr 7.12890e-02\n",
            "Epoch: [36][0/391]\tTime 5.343 (5.343)\tLoss 0.4607 (0.4607)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [36][50/391]\tTime 0.262 (0.361)\tLoss 0.4863 (0.3927)\tPrec@1 86.719 (88.006)\n",
            "Epoch: [36][100/391]\tTime 0.260 (0.312)\tLoss 0.4450 (0.3864)\tPrec@1 84.375 (88.049)\n",
            "Epoch: [36][150/391]\tTime 0.262 (0.295)\tLoss 0.3655 (0.3887)\tPrec@1 89.062 (87.971)\n",
            "Epoch: [36][200/391]\tTime 0.262 (0.287)\tLoss 0.3554 (0.3878)\tPrec@1 89.062 (87.966)\n",
            "Epoch: [36][250/391]\tTime 0.261 (0.282)\tLoss 0.4126 (0.3881)\tPrec@1 86.719 (87.892)\n",
            "Epoch: [36][300/391]\tTime 0.261 (0.279)\tLoss 0.3420 (0.3924)\tPrec@1 89.062 (87.682)\n",
            "Epoch: [36][350/391]\tTime 0.261 (0.276)\tLoss 0.4403 (0.3944)\tPrec@1 86.719 (87.589)\n",
            "epoch 36 training time consumed: 107.50s\n",
            "Test: [0/79]\tTime 3.996 (3.996)\tLoss 0.4494 (0.4494)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.7400 (0.5029)\tPrec@1 75.000 (84.360)\n",
            " * Prec@1 84.270\n",
            "current lr 6.98574e-02\n",
            "Epoch: [37][0/391]\tTime 5.376 (5.376)\tLoss 0.3551 (0.3551)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [37][50/391]\tTime 0.261 (0.361)\tLoss 0.4281 (0.3672)\tPrec@1 87.500 (88.266)\n",
            "Epoch: [37][100/391]\tTime 0.262 (0.312)\tLoss 0.4113 (0.3759)\tPrec@1 89.062 (87.995)\n",
            "Epoch: [37][150/391]\tTime 0.262 (0.295)\tLoss 0.3711 (0.3744)\tPrec@1 85.156 (88.136)\n",
            "Epoch: [37][200/391]\tTime 0.264 (0.287)\tLoss 0.4224 (0.3797)\tPrec@1 86.719 (87.900)\n",
            "Epoch: [37][250/391]\tTime 0.263 (0.282)\tLoss 0.3368 (0.3834)\tPrec@1 93.750 (87.765)\n",
            "Epoch: [37][300/391]\tTime 0.262 (0.279)\tLoss 0.4397 (0.3841)\tPrec@1 87.500 (87.752)\n",
            "Epoch: [37][350/391]\tTime 0.263 (0.277)\tLoss 0.4020 (0.3874)\tPrec@1 87.500 (87.656)\n",
            "epoch 37 training time consumed: 107.76s\n",
            "Test: [0/79]\tTime 3.962 (3.962)\tLoss 0.7333 (0.7333)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.8059 (0.8013)\tPrec@1 71.875 (76.287)\n",
            " * Prec@1 75.960\n",
            "current lr 6.84062e-02\n",
            "Epoch: [38][0/391]\tTime 5.316 (5.316)\tLoss 0.3968 (0.3968)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [38][50/391]\tTime 0.262 (0.361)\tLoss 0.2997 (0.3525)\tPrec@1 89.844 (88.710)\n",
            "Epoch: [38][100/391]\tTime 0.262 (0.312)\tLoss 0.4655 (0.3633)\tPrec@1 88.281 (88.366)\n",
            "Epoch: [38][150/391]\tTime 0.263 (0.296)\tLoss 0.3305 (0.3662)\tPrec@1 89.844 (88.230)\n",
            "Epoch: [38][200/391]\tTime 0.262 (0.288)\tLoss 0.2303 (0.3654)\tPrec@1 94.531 (88.355)\n",
            "Epoch: [38][250/391]\tTime 0.262 (0.283)\tLoss 0.2589 (0.3691)\tPrec@1 91.406 (88.200)\n",
            "Epoch: [38][300/391]\tTime 0.263 (0.279)\tLoss 0.2788 (0.3748)\tPrec@1 92.188 (88.115)\n",
            "Epoch: [38][350/391]\tTime 0.275 (0.277)\tLoss 0.4897 (0.3739)\tPrec@1 84.375 (88.174)\n",
            "epoch 38 training time consumed: 107.88s\n",
            "Test: [0/79]\tTime 4.023 (4.023)\tLoss 0.8609 (0.8609)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 1.2487 (1.2140)\tPrec@1 69.531 (71.569)\n",
            " * Prec@1 71.560\n",
            "current lr 6.69369e-02\n",
            "Epoch: [39][0/391]\tTime 5.483 (5.483)\tLoss 0.2668 (0.2668)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [39][50/391]\tTime 0.262 (0.365)\tLoss 0.3022 (0.3631)\tPrec@1 90.625 (87.914)\n",
            "Epoch: [39][100/391]\tTime 0.263 (0.314)\tLoss 0.3760 (0.3676)\tPrec@1 89.062 (88.034)\n",
            "Epoch: [39][150/391]\tTime 0.263 (0.297)\tLoss 0.4525 (0.3548)\tPrec@1 85.156 (88.462)\n",
            "Epoch: [39][200/391]\tTime 0.263 (0.289)\tLoss 0.3377 (0.3583)\tPrec@1 89.062 (88.359)\n",
            "Epoch: [39][250/391]\tTime 0.264 (0.284)\tLoss 0.3137 (0.3643)\tPrec@1 85.938 (88.135)\n",
            "Epoch: [39][300/391]\tTime 0.264 (0.280)\tLoss 0.3342 (0.3653)\tPrec@1 88.281 (88.120)\n",
            "Epoch: [39][350/391]\tTime 0.263 (0.278)\tLoss 0.2941 (0.3661)\tPrec@1 89.844 (88.130)\n",
            "epoch 39 training time consumed: 108.16s\n",
            "Test: [0/79]\tTime 4.142 (4.142)\tLoss 0.5367 (0.5367)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.4872 (0.5309)\tPrec@1 81.250 (83.946)\n",
            " * Prec@1 83.630\n",
            "current lr 6.54508e-02\n",
            "Epoch: [40][0/391]\tTime 5.481 (5.481)\tLoss 0.3375 (0.3375)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [40][50/391]\tTime 0.262 (0.364)\tLoss 0.1774 (0.3281)\tPrec@1 95.312 (89.292)\n",
            "Epoch: [40][100/391]\tTime 0.263 (0.314)\tLoss 0.4644 (0.3214)\tPrec@1 84.375 (89.519)\n",
            "Epoch: [40][150/391]\tTime 0.262 (0.297)\tLoss 0.3142 (0.3364)\tPrec@1 90.625 (89.238)\n",
            "Epoch: [40][200/391]\tTime 0.263 (0.288)\tLoss 0.3860 (0.3416)\tPrec@1 86.719 (89.160)\n",
            "Epoch: [40][250/391]\tTime 0.262 (0.283)\tLoss 0.4313 (0.3446)\tPrec@1 84.375 (89.050)\n",
            "Epoch: [40][300/391]\tTime 0.263 (0.280)\tLoss 0.3948 (0.3497)\tPrec@1 85.938 (88.925)\n",
            "Epoch: [40][350/391]\tTime 0.262 (0.278)\tLoss 0.4811 (0.3506)\tPrec@1 84.375 (88.896)\n",
            "epoch 40 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.051 (4.051)\tLoss 0.4484 (0.4484)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.4596 (0.4615)\tPrec@1 85.938 (85.708)\n",
            " * Prec@1 85.100\n",
            "current lr 6.39496e-02\n",
            "Epoch: [41][0/391]\tTime 5.316 (5.316)\tLoss 0.4180 (0.4180)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [41][50/391]\tTime 0.261 (0.361)\tLoss 0.3035 (0.3297)\tPrec@1 90.625 (89.537)\n",
            "Epoch: [41][100/391]\tTime 0.263 (0.312)\tLoss 0.3821 (0.3402)\tPrec@1 85.156 (88.908)\n",
            "Epoch: [41][150/391]\tTime 0.263 (0.296)\tLoss 0.4453 (0.3435)\tPrec@1 88.281 (88.902)\n",
            "Epoch: [41][200/391]\tTime 0.264 (0.288)\tLoss 0.3101 (0.3483)\tPrec@1 89.062 (88.662)\n",
            "Epoch: [41][250/391]\tTime 0.262 (0.282)\tLoss 0.3491 (0.3442)\tPrec@1 87.500 (88.798)\n",
            "Epoch: [41][300/391]\tTime 0.263 (0.279)\tLoss 0.2131 (0.3430)\tPrec@1 93.750 (88.889)\n",
            "Epoch: [41][350/391]\tTime 0.261 (0.277)\tLoss 0.4167 (0.3436)\tPrec@1 86.719 (88.862)\n",
            "epoch 41 training time consumed: 107.65s\n",
            "Test: [0/79]\tTime 4.067 (4.067)\tLoss 0.5740 (0.5740)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.8081 (0.6349)\tPrec@1 77.344 (81.648)\n",
            " * Prec@1 81.250\n",
            "current lr 6.24345e-02\n",
            "Epoch: [42][0/391]\tTime 5.535 (5.535)\tLoss 0.2520 (0.2520)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [42][50/391]\tTime 0.262 (0.365)\tLoss 0.2959 (0.3331)\tPrec@1 90.625 (89.691)\n",
            "Epoch: [42][100/391]\tTime 0.261 (0.314)\tLoss 0.2644 (0.3195)\tPrec@1 91.406 (90.037)\n",
            "Epoch: [42][150/391]\tTime 0.262 (0.296)\tLoss 0.3252 (0.3160)\tPrec@1 89.062 (90.108)\n",
            "Epoch: [42][200/391]\tTime 0.262 (0.288)\tLoss 0.3181 (0.3250)\tPrec@1 86.719 (89.774)\n",
            "Epoch: [42][250/391]\tTime 0.261 (0.283)\tLoss 0.3992 (0.3256)\tPrec@1 87.500 (89.716)\n",
            "Epoch: [42][300/391]\tTime 0.261 (0.279)\tLoss 0.4292 (0.3300)\tPrec@1 85.156 (89.530)\n",
            "Epoch: [42][350/391]\tTime 0.262 (0.277)\tLoss 0.3009 (0.3339)\tPrec@1 88.281 (89.370)\n",
            "epoch 42 training time consumed: 107.72s\n",
            "Test: [0/79]\tTime 4.001 (4.001)\tLoss 0.8971 (0.8971)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 1.1817 (1.0118)\tPrec@1 67.969 (71.063)\n",
            " * Prec@1 70.830\n",
            "current lr 6.09072e-02\n",
            "Epoch: [43][0/391]\tTime 5.328 (5.328)\tLoss 0.2910 (0.2910)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [43][50/391]\tTime 0.261 (0.360)\tLoss 0.3558 (0.3385)\tPrec@1 92.188 (88.940)\n",
            "Epoch: [43][100/391]\tTime 0.261 (0.311)\tLoss 0.2593 (0.3290)\tPrec@1 91.406 (89.186)\n",
            "Epoch: [43][150/391]\tTime 0.262 (0.295)\tLoss 0.3184 (0.3165)\tPrec@1 89.844 (89.797)\n",
            "Epoch: [43][200/391]\tTime 0.262 (0.287)\tLoss 0.3245 (0.3190)\tPrec@1 89.844 (89.774)\n",
            "Epoch: [43][250/391]\tTime 0.263 (0.282)\tLoss 0.4793 (0.3256)\tPrec@1 85.156 (89.551)\n",
            "Epoch: [43][300/391]\tTime 0.264 (0.279)\tLoss 0.3642 (0.3273)\tPrec@1 90.625 (89.517)\n",
            "Epoch: [43][350/391]\tTime 0.264 (0.276)\tLoss 0.3809 (0.3284)\tPrec@1 89.062 (89.461)\n",
            "epoch 43 training time consumed: 107.64s\n",
            "Test: [0/79]\tTime 4.042 (4.042)\tLoss 0.4799 (0.4799)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.5769 (0.4938)\tPrec@1 82.031 (83.379)\n",
            " * Prec@1 83.720\n",
            "current lr 5.93691e-02\n",
            "Epoch: [44][0/391]\tTime 5.509 (5.509)\tLoss 0.3664 (0.3664)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [44][50/391]\tTime 0.261 (0.365)\tLoss 0.3423 (0.3243)\tPrec@1 90.625 (89.660)\n",
            "Epoch: [44][100/391]\tTime 0.262 (0.314)\tLoss 0.4079 (0.3210)\tPrec@1 90.625 (89.712)\n",
            "Epoch: [44][150/391]\tTime 0.262 (0.297)\tLoss 0.3357 (0.3173)\tPrec@1 91.406 (89.802)\n",
            "Epoch: [44][200/391]\tTime 0.262 (0.288)\tLoss 0.3322 (0.3170)\tPrec@1 87.500 (89.743)\n",
            "Epoch: [44][250/391]\tTime 0.263 (0.283)\tLoss 0.3314 (0.3205)\tPrec@1 90.625 (89.750)\n",
            "Epoch: [44][300/391]\tTime 0.263 (0.280)\tLoss 0.4258 (0.3162)\tPrec@1 87.500 (89.888)\n",
            "Epoch: [44][350/391]\tTime 0.263 (0.278)\tLoss 0.2875 (0.3201)\tPrec@1 90.625 (89.779)\n",
            "epoch 44 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.061 (4.061)\tLoss 0.7283 (0.7283)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.088 (0.165)\tLoss 0.7264 (0.6094)\tPrec@1 79.688 (82.812)\n",
            " * Prec@1 82.720\n",
            "current lr 5.78217e-02\n",
            "Epoch: [45][0/391]\tTime 5.354 (5.354)\tLoss 0.3146 (0.3146)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [45][50/391]\tTime 0.262 (0.362)\tLoss 0.1868 (0.2953)\tPrec@1 95.312 (90.303)\n",
            "Epoch: [45][100/391]\tTime 0.262 (0.313)\tLoss 0.3683 (0.2970)\tPrec@1 90.625 (90.261)\n",
            "Epoch: [45][150/391]\tTime 0.263 (0.297)\tLoss 0.3864 (0.3030)\tPrec@1 87.500 (89.983)\n",
            "Epoch: [45][200/391]\tTime 0.263 (0.288)\tLoss 0.3364 (0.3036)\tPrec@1 87.500 (89.999)\n",
            "Epoch: [45][250/391]\tTime 0.264 (0.283)\tLoss 0.3868 (0.3082)\tPrec@1 90.625 (89.937)\n",
            "Epoch: [45][300/391]\tTime 0.263 (0.280)\tLoss 0.3240 (0.3096)\tPrec@1 90.625 (89.971)\n",
            "Epoch: [45][350/391]\tTime 0.265 (0.278)\tLoss 0.2612 (0.3090)\tPrec@1 91.406 (89.997)\n",
            "epoch 45 training time consumed: 108.04s\n",
            "Test: [0/79]\tTime 4.094 (4.094)\tLoss 0.4545 (0.4545)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.086 (0.165)\tLoss 0.6372 (0.4984)\tPrec@1 82.031 (84.589)\n",
            " * Prec@1 84.070\n",
            "current lr 5.62667e-02\n",
            "Epoch: [46][0/391]\tTime 5.473 (5.473)\tLoss 0.2813 (0.2813)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [46][50/391]\tTime 0.261 (0.364)\tLoss 0.3415 (0.2821)\tPrec@1 89.062 (90.748)\n",
            "Epoch: [46][100/391]\tTime 0.262 (0.314)\tLoss 0.2923 (0.2819)\tPrec@1 90.625 (90.718)\n",
            "Epoch: [46][150/391]\tTime 0.267 (0.297)\tLoss 0.3889 (0.2918)\tPrec@1 85.938 (90.537)\n",
            "Epoch: [46][200/391]\tTime 0.262 (0.288)\tLoss 0.3061 (0.2899)\tPrec@1 94.531 (90.543)\n",
            "Epoch: [46][250/391]\tTime 0.263 (0.283)\tLoss 0.2359 (0.2931)\tPrec@1 92.969 (90.513)\n",
            "Epoch: [46][300/391]\tTime 0.263 (0.280)\tLoss 0.2972 (0.2950)\tPrec@1 89.844 (90.474)\n",
            "Epoch: [46][350/391]\tTime 0.263 (0.277)\tLoss 0.2775 (0.2984)\tPrec@1 91.406 (90.371)\n",
            "epoch 46 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.056 (4.056)\tLoss 0.3797 (0.3797)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.5959 (0.4372)\tPrec@1 77.344 (86.137)\n",
            " * Prec@1 86.160\n",
            "current lr 5.47054e-02\n",
            "Epoch: [47][0/391]\tTime 5.493 (5.493)\tLoss 0.2234 (0.2234)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [47][50/391]\tTime 0.262 (0.364)\tLoss 0.2441 (0.2788)\tPrec@1 91.406 (90.931)\n",
            "Epoch: [47][100/391]\tTime 0.263 (0.314)\tLoss 0.4278 (0.2997)\tPrec@1 85.938 (90.308)\n",
            "Epoch: [47][150/391]\tTime 0.263 (0.297)\tLoss 0.1097 (0.2915)\tPrec@1 97.656 (90.615)\n",
            "Epoch: [47][200/391]\tTime 0.263 (0.289)\tLoss 0.2407 (0.2853)\tPrec@1 93.750 (90.885)\n",
            "Epoch: [47][250/391]\tTime 0.263 (0.283)\tLoss 0.2744 (0.2856)\tPrec@1 92.188 (90.886)\n",
            "Epoch: [47][300/391]\tTime 0.263 (0.280)\tLoss 0.2868 (0.2841)\tPrec@1 88.281 (90.877)\n",
            "Epoch: [47][350/391]\tTime 0.263 (0.278)\tLoss 0.2681 (0.2863)\tPrec@1 90.625 (90.787)\n",
            "epoch 47 training time consumed: 108.05s\n",
            "Test: [0/79]\tTime 4.007 (4.007)\tLoss 0.4837 (0.4837)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.7806 (0.5395)\tPrec@1 78.906 (84.375)\n",
            " * Prec@1 84.100\n",
            "current lr 5.31395e-02\n",
            "Epoch: [48][0/391]\tTime 5.353 (5.353)\tLoss 0.2076 (0.2076)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [48][50/391]\tTime 0.261 (0.361)\tLoss 0.2499 (0.2586)\tPrec@1 90.625 (91.590)\n",
            "Epoch: [48][100/391]\tTime 0.261 (0.312)\tLoss 0.2645 (0.2643)\tPrec@1 92.188 (91.476)\n",
            "Epoch: [48][150/391]\tTime 0.262 (0.295)\tLoss 0.2261 (0.2636)\tPrec@1 92.188 (91.391)\n",
            "Epoch: [48][200/391]\tTime 0.262 (0.287)\tLoss 0.2413 (0.2704)\tPrec@1 91.406 (91.329)\n",
            "Epoch: [48][250/391]\tTime 0.262 (0.282)\tLoss 0.3600 (0.2758)\tPrec@1 87.500 (91.248)\n",
            "Epoch: [48][300/391]\tTime 0.262 (0.279)\tLoss 0.2990 (0.2757)\tPrec@1 89.062 (91.235)\n",
            "Epoch: [48][350/391]\tTime 0.261 (0.276)\tLoss 0.3172 (0.2796)\tPrec@1 88.281 (91.153)\n",
            "epoch 48 training time consumed: 107.52s\n",
            "Test: [0/79]\tTime 4.002 (4.002)\tLoss 0.5602 (0.5602)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.5298 (0.5350)\tPrec@1 85.156 (83.900)\n",
            " * Prec@1 83.930\n",
            "current lr 5.15705e-02\n",
            "Epoch: [49][0/391]\tTime 5.334 (5.334)\tLoss 0.1760 (0.1760)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [49][50/391]\tTime 0.261 (0.361)\tLoss 0.2690 (0.2623)\tPrec@1 93.750 (91.697)\n",
            "Epoch: [49][100/391]\tTime 0.261 (0.311)\tLoss 0.2275 (0.2667)\tPrec@1 90.625 (91.561)\n",
            "Epoch: [49][150/391]\tTime 0.262 (0.295)\tLoss 0.3422 (0.2665)\tPrec@1 89.844 (91.386)\n",
            "Epoch: [49][200/391]\tTime 0.261 (0.287)\tLoss 0.3256 (0.2672)\tPrec@1 91.406 (91.352)\n",
            "Epoch: [49][250/391]\tTime 0.262 (0.282)\tLoss 0.3098 (0.2708)\tPrec@1 89.844 (91.244)\n",
            "Epoch: [49][300/391]\tTime 0.262 (0.279)\tLoss 0.2497 (0.2727)\tPrec@1 92.969 (91.131)\n",
            "Epoch: [49][350/391]\tTime 0.262 (0.276)\tLoss 0.3893 (0.2749)\tPrec@1 87.500 (91.084)\n",
            "epoch 49 training time consumed: 107.52s\n",
            "Test: [0/79]\tTime 3.981 (3.981)\tLoss 0.6502 (0.6502)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.086 (0.163)\tLoss 0.6195 (0.5501)\tPrec@1 80.469 (83.578)\n",
            " * Prec@1 83.500\n",
            "current lr 5.00000e-02\n",
            "Epoch: [50][0/391]\tTime 5.318 (5.318)\tLoss 0.1700 (0.1700)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [50][50/391]\tTime 0.263 (0.361)\tLoss 0.1988 (0.2403)\tPrec@1 94.531 (92.080)\n",
            "Epoch: [50][100/391]\tTime 0.262 (0.312)\tLoss 0.2160 (0.2592)\tPrec@1 92.969 (91.561)\n",
            "Epoch: [50][150/391]\tTime 0.262 (0.296)\tLoss 0.2478 (0.2605)\tPrec@1 92.188 (91.556)\n",
            "Epoch: [50][200/391]\tTime 0.263 (0.288)\tLoss 0.1244 (0.2612)\tPrec@1 96.875 (91.433)\n",
            "Epoch: [50][250/391]\tTime 0.263 (0.283)\tLoss 0.2898 (0.2685)\tPrec@1 89.844 (91.163)\n",
            "Epoch: [50][300/391]\tTime 0.262 (0.279)\tLoss 0.2617 (0.2677)\tPrec@1 89.844 (91.227)\n",
            "Epoch: [50][350/391]\tTime 0.262 (0.277)\tLoss 0.2319 (0.2689)\tPrec@1 92.188 (91.206)\n",
            "epoch 50 training time consumed: 107.83s\n",
            "Test: [0/79]\tTime 4.064 (4.064)\tLoss 0.3899 (0.3899)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.7155 (0.4542)\tPrec@1 78.906 (85.800)\n",
            " * Prec@1 86.380\n",
            "current lr 4.84295e-02\n",
            "Epoch: [51][0/391]\tTime 5.311 (5.311)\tLoss 0.2226 (0.2226)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [51][50/391]\tTime 0.262 (0.361)\tLoss 0.3785 (0.2439)\tPrec@1 88.281 (92.142)\n",
            "Epoch: [51][100/391]\tTime 0.263 (0.312)\tLoss 0.3117 (0.2649)\tPrec@1 92.969 (91.522)\n",
            "Epoch: [51][150/391]\tTime 0.264 (0.296)\tLoss 0.1685 (0.2546)\tPrec@1 94.531 (91.970)\n",
            "Epoch: [51][200/391]\tTime 0.263 (0.288)\tLoss 0.2630 (0.2525)\tPrec@1 91.406 (92.013)\n",
            "Epoch: [51][250/391]\tTime 0.263 (0.283)\tLoss 0.3122 (0.2487)\tPrec@1 88.281 (92.113)\n",
            "Epoch: [51][300/391]\tTime 0.264 (0.280)\tLoss 0.2815 (0.2524)\tPrec@1 91.406 (91.988)\n",
            "Epoch: [51][350/391]\tTime 0.264 (0.277)\tLoss 0.2496 (0.2528)\tPrec@1 92.188 (91.949)\n",
            "epoch 51 training time consumed: 107.98s\n",
            "Test: [0/79]\tTime 4.059 (4.059)\tLoss 0.5538 (0.5538)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.9462 (0.6639)\tPrec@1 75.781 (81.572)\n",
            " * Prec@1 81.530\n",
            "current lr 4.68605e-02\n",
            "Epoch: [52][0/391]\tTime 5.373 (5.373)\tLoss 0.2469 (0.2469)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [52][50/391]\tTime 0.262 (0.362)\tLoss 0.2777 (0.2329)\tPrec@1 90.625 (92.417)\n",
            "Epoch: [52][100/391]\tTime 0.263 (0.313)\tLoss 0.2585 (0.2317)\tPrec@1 92.188 (92.567)\n",
            "Epoch: [52][150/391]\tTime 0.264 (0.297)\tLoss 0.2343 (0.2344)\tPrec@1 92.188 (92.405)\n",
            "Epoch: [52][200/391]\tTime 0.263 (0.288)\tLoss 0.2295 (0.2370)\tPrec@1 92.969 (92.347)\n",
            "Epoch: [52][250/391]\tTime 0.263 (0.283)\tLoss 0.3900 (0.2376)\tPrec@1 87.500 (92.359)\n",
            "Epoch: [52][300/391]\tTime 0.262 (0.280)\tLoss 0.4003 (0.2368)\tPrec@1 89.844 (92.369)\n",
            "Epoch: [52][350/391]\tTime 0.262 (0.277)\tLoss 0.2138 (0.2414)\tPrec@1 91.406 (92.250)\n",
            "epoch 52 training time consumed: 107.94s\n",
            "Test: [0/79]\tTime 4.025 (4.025)\tLoss 0.4369 (0.4369)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.4394 (0.4917)\tPrec@1 85.938 (85.463)\n",
            " * Prec@1 85.300\n",
            "current lr 4.52946e-02\n",
            "Epoch: [53][0/391]\tTime 5.513 (5.513)\tLoss 0.1755 (0.1755)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [53][50/391]\tTime 0.262 (0.365)\tLoss 0.2175 (0.2135)\tPrec@1 94.531 (93.229)\n",
            "Epoch: [53][100/391]\tTime 0.263 (0.314)\tLoss 0.1565 (0.2291)\tPrec@1 95.312 (92.783)\n",
            "Epoch: [53][150/391]\tTime 0.262 (0.297)\tLoss 0.1786 (0.2277)\tPrec@1 96.094 (92.824)\n",
            "Epoch: [53][200/391]\tTime 0.262 (0.289)\tLoss 0.1674 (0.2325)\tPrec@1 96.094 (92.553)\n",
            "Epoch: [53][250/391]\tTime 0.263 (0.283)\tLoss 0.2077 (0.2368)\tPrec@1 93.750 (92.452)\n",
            "Epoch: [53][300/391]\tTime 0.262 (0.280)\tLoss 0.2369 (0.2365)\tPrec@1 92.969 (92.444)\n",
            "Epoch: [53][350/391]\tTime 0.264 (0.278)\tLoss 0.3136 (0.2372)\tPrec@1 90.625 (92.410)\n",
            "epoch 53 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.091 (4.091)\tLoss 0.6740 (0.6740)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.6774 (0.7955)\tPrec@1 83.594 (80.300)\n",
            " * Prec@1 80.400\n",
            "current lr 4.37333e-02\n",
            "Epoch: [54][0/391]\tTime 5.487 (5.487)\tLoss 0.1998 (0.1998)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [54][50/391]\tTime 0.262 (0.363)\tLoss 0.1248 (0.2322)\tPrec@1 97.656 (92.770)\n",
            "Epoch: [54][100/391]\tTime 0.261 (0.313)\tLoss 0.3947 (0.2210)\tPrec@1 86.719 (92.938)\n",
            "Epoch: [54][150/391]\tTime 0.262 (0.296)\tLoss 0.2315 (0.2201)\tPrec@1 90.625 (92.984)\n",
            "Epoch: [54][200/391]\tTime 0.262 (0.288)\tLoss 0.2939 (0.2218)\tPrec@1 91.406 (92.922)\n",
            "Epoch: [54][250/391]\tTime 0.262 (0.282)\tLoss 0.2111 (0.2294)\tPrec@1 91.406 (92.633)\n",
            "Epoch: [54][300/391]\tTime 0.261 (0.279)\tLoss 0.3203 (0.2299)\tPrec@1 89.062 (92.639)\n",
            "Epoch: [54][350/391]\tTime 0.261 (0.277)\tLoss 0.2409 (0.2329)\tPrec@1 92.969 (92.532)\n",
            "epoch 54 training time consumed: 107.65s\n",
            "Test: [0/79]\tTime 4.012 (4.012)\tLoss 0.5792 (0.5792)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.5277 (0.4172)\tPrec@1 82.031 (87.286)\n",
            " * Prec@1 87.260\n",
            "current lr 4.21783e-02\n",
            "Epoch: [55][0/391]\tTime 5.370 (5.370)\tLoss 0.2193 (0.2193)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [55][50/391]\tTime 0.262 (0.361)\tLoss 0.1387 (0.2139)\tPrec@1 95.312 (93.306)\n",
            "Epoch: [55][100/391]\tTime 0.262 (0.312)\tLoss 0.1534 (0.2121)\tPrec@1 96.094 (93.340)\n",
            "Epoch: [55][150/391]\tTime 0.262 (0.295)\tLoss 0.1165 (0.2143)\tPrec@1 96.094 (93.139)\n",
            "Epoch: [55][200/391]\tTime 0.262 (0.287)\tLoss 0.2068 (0.2213)\tPrec@1 92.969 (92.942)\n",
            "Epoch: [55][250/391]\tTime 0.262 (0.282)\tLoss 0.1684 (0.2235)\tPrec@1 96.875 (92.891)\n",
            "Epoch: [55][300/391]\tTime 0.263 (0.279)\tLoss 0.2543 (0.2252)\tPrec@1 91.406 (92.886)\n",
            "Epoch: [55][350/391]\tTime 0.263 (0.276)\tLoss 0.2477 (0.2256)\tPrec@1 90.625 (92.831)\n",
            "epoch 55 training time consumed: 107.52s\n",
            "Test: [0/79]\tTime 4.028 (4.028)\tLoss 0.4746 (0.4746)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.5743 (0.4592)\tPrec@1 85.938 (86.489)\n",
            " * Prec@1 86.970\n",
            "current lr 4.06309e-02\n",
            "Epoch: [56][0/391]\tTime 5.532 (5.532)\tLoss 0.2252 (0.2252)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [56][50/391]\tTime 0.260 (0.364)\tLoss 0.2403 (0.1936)\tPrec@1 93.750 (94.041)\n",
            "Epoch: [56][100/391]\tTime 0.264 (0.314)\tLoss 0.1838 (0.1969)\tPrec@1 90.625 (93.843)\n",
            "Epoch: [56][150/391]\tTime 0.262 (0.297)\tLoss 0.2009 (0.1996)\tPrec@1 92.969 (93.626)\n",
            "Epoch: [56][200/391]\tTime 0.263 (0.288)\tLoss 0.1691 (0.2030)\tPrec@1 94.531 (93.544)\n",
            "Epoch: [56][250/391]\tTime 0.263 (0.283)\tLoss 0.2926 (0.2086)\tPrec@1 92.188 (93.327)\n",
            "Epoch: [56][300/391]\tTime 0.262 (0.280)\tLoss 0.1642 (0.2089)\tPrec@1 92.969 (93.301)\n",
            "Epoch: [56][350/391]\tTime 0.263 (0.277)\tLoss 0.1339 (0.2079)\tPrec@1 96.094 (93.340)\n",
            "epoch 56 training time consumed: 107.99s\n",
            "Test: [0/79]\tTime 4.103 (4.103)\tLoss 0.3321 (0.3321)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.086 (0.166)\tLoss 0.5739 (0.4996)\tPrec@1 84.375 (85.876)\n",
            " * Prec@1 86.100\n",
            "current lr 3.90928e-02\n",
            "Epoch: [57][0/391]\tTime 5.507 (5.507)\tLoss 0.2620 (0.2620)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [57][50/391]\tTime 0.262 (0.365)\tLoss 0.1775 (0.1899)\tPrec@1 93.750 (93.873)\n",
            "Epoch: [57][100/391]\tTime 0.263 (0.314)\tLoss 0.1482 (0.1839)\tPrec@1 94.531 (94.005)\n",
            "Epoch: [57][150/391]\tTime 0.262 (0.297)\tLoss 0.3126 (0.1959)\tPrec@1 89.844 (93.631)\n",
            "Epoch: [57][200/391]\tTime 0.263 (0.289)\tLoss 0.2261 (0.2011)\tPrec@1 92.969 (93.466)\n",
            "Epoch: [57][250/391]\tTime 0.263 (0.284)\tLoss 0.1466 (0.1988)\tPrec@1 96.094 (93.538)\n",
            "Epoch: [57][300/391]\tTime 0.263 (0.280)\tLoss 0.2206 (0.2005)\tPrec@1 92.969 (93.524)\n",
            "Epoch: [57][350/391]\tTime 0.263 (0.278)\tLoss 0.3276 (0.2010)\tPrec@1 85.938 (93.478)\n",
            "epoch 57 training time consumed: 108.10s\n",
            "Test: [0/79]\tTime 4.027 (4.027)\tLoss 0.2607 (0.2607)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.3661 (0.3061)\tPrec@1 86.719 (90.288)\n",
            " * Prec@1 90.260\n",
            "current lr 3.75655e-02\n",
            "Epoch: [58][0/391]\tTime 5.501 (5.501)\tLoss 0.1013 (0.1013)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [58][50/391]\tTime 0.261 (0.365)\tLoss 0.2689 (0.1836)\tPrec@1 92.969 (93.873)\n",
            "Epoch: [58][100/391]\tTime 0.262 (0.314)\tLoss 0.2159 (0.1857)\tPrec@1 93.750 (93.928)\n",
            "Epoch: [58][150/391]\tTime 0.262 (0.297)\tLoss 0.1757 (0.1844)\tPrec@1 93.750 (93.957)\n",
            "Epoch: [58][200/391]\tTime 0.263 (0.289)\tLoss 0.2189 (0.1876)\tPrec@1 92.969 (93.878)\n",
            "Epoch: [58][250/391]\tTime 0.263 (0.284)\tLoss 0.1869 (0.1915)\tPrec@1 93.750 (93.797)\n",
            "Epoch: [58][300/391]\tTime 0.263 (0.280)\tLoss 0.2052 (0.1917)\tPrec@1 93.750 (93.781)\n",
            "Epoch: [58][350/391]\tTime 0.264 (0.278)\tLoss 0.3478 (0.1924)\tPrec@1 88.281 (93.772)\n",
            "epoch 58 training time consumed: 108.11s\n",
            "Test: [0/79]\tTime 4.142 (4.142)\tLoss 0.6830 (0.6830)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.086 (0.166)\tLoss 0.7103 (0.5847)\tPrec@1 78.906 (84.635)\n",
            " * Prec@1 84.570\n",
            "current lr 3.60504e-02\n",
            "Epoch: [59][0/391]\tTime 5.324 (5.324)\tLoss 0.1672 (0.1672)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [59][50/391]\tTime 0.262 (0.361)\tLoss 0.2042 (0.1763)\tPrec@1 92.188 (94.072)\n",
            "Epoch: [59][100/391]\tTime 0.262 (0.312)\tLoss 0.1802 (0.1714)\tPrec@1 90.625 (94.423)\n",
            "Epoch: [59][150/391]\tTime 0.262 (0.296)\tLoss 0.1473 (0.1802)\tPrec@1 94.531 (94.143)\n",
            "Epoch: [59][200/391]\tTime 0.262 (0.288)\tLoss 0.2138 (0.1822)\tPrec@1 92.188 (94.018)\n",
            "Epoch: [59][250/391]\tTime 0.263 (0.283)\tLoss 0.1374 (0.1824)\tPrec@1 96.094 (94.058)\n",
            "Epoch: [59][300/391]\tTime 0.262 (0.279)\tLoss 0.1714 (0.1833)\tPrec@1 94.531 (94.025)\n",
            "Epoch: [59][350/391]\tTime 0.262 (0.277)\tLoss 0.2168 (0.1832)\tPrec@1 92.969 (94.030)\n",
            "epoch 59 training time consumed: 107.87s\n",
            "Test: [0/79]\tTime 4.037 (4.037)\tLoss 0.4792 (0.4792)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.5727 (0.5715)\tPrec@1 84.375 (84.589)\n",
            " * Prec@1 84.640\n",
            "current lr 3.45492e-02\n",
            "Epoch: [60][0/391]\tTime 5.488 (5.488)\tLoss 0.2482 (0.2482)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [60][50/391]\tTime 0.263 (0.365)\tLoss 0.1748 (0.1828)\tPrec@1 95.312 (93.873)\n",
            "Epoch: [60][100/391]\tTime 0.262 (0.314)\tLoss 0.1727 (0.1740)\tPrec@1 93.750 (94.307)\n",
            "Epoch: [60][150/391]\tTime 0.263 (0.297)\tLoss 0.2308 (0.1747)\tPrec@1 91.406 (94.210)\n",
            "Epoch: [60][200/391]\tTime 0.263 (0.289)\tLoss 0.1265 (0.1745)\tPrec@1 96.094 (94.236)\n",
            "Epoch: [60][250/391]\tTime 0.263 (0.284)\tLoss 0.2444 (0.1794)\tPrec@1 92.188 (94.102)\n",
            "Epoch: [60][300/391]\tTime 0.263 (0.280)\tLoss 0.1197 (0.1784)\tPrec@1 94.531 (94.108)\n",
            "Epoch: [60][350/391]\tTime 0.263 (0.278)\tLoss 0.1201 (0.1801)\tPrec@1 96.094 (94.026)\n",
            "epoch 60 training time consumed: 108.17s\n",
            "Test: [0/79]\tTime 4.130 (4.130)\tLoss 0.2931 (0.2931)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.5299 (0.4143)\tPrec@1 84.375 (88.021)\n",
            " * Prec@1 88.190\n",
            "current lr 3.30631e-02\n",
            "Epoch: [61][0/391]\tTime 5.470 (5.470)\tLoss 0.2318 (0.2318)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [61][50/391]\tTime 0.263 (0.364)\tLoss 0.2035 (0.1541)\tPrec@1 93.750 (94.930)\n",
            "Epoch: [61][100/391]\tTime 0.264 (0.314)\tLoss 0.1175 (0.1566)\tPrec@1 96.875 (94.933)\n",
            "Epoch: [61][150/391]\tTime 0.263 (0.297)\tLoss 0.2553 (0.1621)\tPrec@1 90.625 (94.754)\n",
            "Epoch: [61][200/391]\tTime 0.263 (0.289)\tLoss 0.1670 (0.1618)\tPrec@1 94.531 (94.792)\n",
            "Epoch: [61][250/391]\tTime 0.262 (0.284)\tLoss 0.1543 (0.1663)\tPrec@1 96.094 (94.674)\n",
            "Epoch: [61][300/391]\tTime 0.263 (0.280)\tLoss 0.1225 (0.1678)\tPrec@1 94.531 (94.645)\n",
            "Epoch: [61][350/391]\tTime 0.264 (0.278)\tLoss 0.1572 (0.1668)\tPrec@1 93.750 (94.627)\n",
            "epoch 61 training time consumed: 108.13s\n",
            "Test: [0/79]\tTime 4.087 (4.087)\tLoss 0.4038 (0.4038)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.4595 (0.4041)\tPrec@1 86.719 (88.297)\n",
            " * Prec@1 88.420\n",
            "current lr 3.15938e-02\n",
            "Epoch: [62][0/391]\tTime 5.349 (5.349)\tLoss 0.0523 (0.0523)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [62][50/391]\tTime 0.262 (0.362)\tLoss 0.2168 (0.1268)\tPrec@1 96.094 (96.017)\n",
            "Epoch: [62][100/391]\tTime 0.262 (0.313)\tLoss 0.1109 (0.1427)\tPrec@1 97.656 (95.537)\n",
            "Epoch: [62][150/391]\tTime 0.262 (0.296)\tLoss 0.1956 (0.1465)\tPrec@1 93.750 (95.359)\n",
            "Epoch: [62][200/391]\tTime 0.263 (0.288)\tLoss 0.2321 (0.1534)\tPrec@1 94.531 (95.134)\n",
            "Epoch: [62][250/391]\tTime 0.264 (0.283)\tLoss 0.1049 (0.1563)\tPrec@1 96.875 (94.998)\n",
            "Epoch: [62][300/391]\tTime 0.263 (0.280)\tLoss 0.1431 (0.1542)\tPrec@1 95.312 (95.053)\n",
            "Epoch: [62][350/391]\tTime 0.263 (0.277)\tLoss 0.1779 (0.1559)\tPrec@1 93.750 (94.956)\n",
            "epoch 62 training time consumed: 107.90s\n",
            "Test: [0/79]\tTime 4.044 (4.044)\tLoss 0.3383 (0.3383)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.2830 (0.3384)\tPrec@1 92.188 (90.165)\n",
            " * Prec@1 89.950\n",
            "current lr 3.01426e-02\n",
            "Epoch: [63][0/391]\tTime 5.502 (5.502)\tLoss 0.1495 (0.1495)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [63][50/391]\tTime 0.262 (0.365)\tLoss 0.2113 (0.1362)\tPrec@1 94.531 (95.650)\n",
            "Epoch: [63][100/391]\tTime 0.262 (0.314)\tLoss 0.2437 (0.1486)\tPrec@1 92.188 (95.135)\n",
            "Epoch: [63][150/391]\tTime 0.263 (0.297)\tLoss 0.1597 (0.1485)\tPrec@1 92.969 (95.219)\n",
            "Epoch: [63][200/391]\tTime 0.263 (0.289)\tLoss 0.1550 (0.1489)\tPrec@1 96.094 (95.219)\n",
            "Epoch: [63][250/391]\tTime 0.264 (0.283)\tLoss 0.1453 (0.1510)\tPrec@1 94.531 (95.154)\n",
            "Epoch: [63][300/391]\tTime 0.263 (0.280)\tLoss 0.1768 (0.1543)\tPrec@1 93.750 (95.058)\n",
            "Epoch: [63][350/391]\tTime 0.263 (0.278)\tLoss 0.1591 (0.1536)\tPrec@1 95.312 (95.083)\n",
            "epoch 63 training time consumed: 108.08s\n",
            "Test: [0/79]\tTime 3.999 (3.999)\tLoss 0.2576 (0.2576)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.3011 (0.4088)\tPrec@1 91.406 (88.006)\n",
            " * Prec@1 88.190\n",
            "current lr 2.87110e-02\n",
            "Epoch: [64][0/391]\tTime 5.478 (5.478)\tLoss 0.1517 (0.1517)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [64][50/391]\tTime 0.261 (0.364)\tLoss 0.0773 (0.1378)\tPrec@1 97.656 (95.757)\n",
            "Epoch: [64][100/391]\tTime 0.262 (0.314)\tLoss 0.1485 (0.1324)\tPrec@1 96.094 (95.877)\n",
            "Epoch: [64][150/391]\tTime 0.263 (0.297)\tLoss 0.1271 (0.1348)\tPrec@1 96.094 (95.659)\n",
            "Epoch: [64][200/391]\tTime 0.263 (0.288)\tLoss 0.1757 (0.1367)\tPrec@1 94.531 (95.592)\n",
            "Epoch: [64][250/391]\tTime 0.262 (0.283)\tLoss 0.1366 (0.1371)\tPrec@1 95.312 (95.565)\n",
            "Epoch: [64][300/391]\tTime 0.263 (0.280)\tLoss 0.1520 (0.1398)\tPrec@1 95.312 (95.494)\n",
            "Epoch: [64][350/391]\tTime 0.263 (0.278)\tLoss 0.2185 (0.1386)\tPrec@1 90.625 (95.511)\n",
            "epoch 64 training time consumed: 108.04s\n",
            "Test: [0/79]\tTime 4.033 (4.033)\tLoss 0.7185 (0.7185)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.088 (0.164)\tLoss 0.9300 (0.6599)\tPrec@1 79.688 (84.145)\n",
            " * Prec@1 84.470\n",
            "current lr 2.73005e-02\n",
            "Epoch: [65][0/391]\tTime 5.312 (5.312)\tLoss 0.0851 (0.0851)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [65][50/391]\tTime 0.264 (0.361)\tLoss 0.1100 (0.1249)\tPrec@1 95.312 (95.956)\n",
            "Epoch: [65][100/391]\tTime 0.263 (0.312)\tLoss 0.1480 (0.1244)\tPrec@1 96.094 (95.939)\n",
            "Epoch: [65][150/391]\tTime 0.263 (0.296)\tLoss 0.1365 (0.1275)\tPrec@1 95.312 (95.799)\n",
            "Epoch: [65][200/391]\tTime 0.263 (0.288)\tLoss 0.1723 (0.1276)\tPrec@1 95.312 (95.756)\n",
            "Epoch: [65][250/391]\tTime 0.263 (0.283)\tLoss 0.1477 (0.1310)\tPrec@1 94.531 (95.736)\n",
            "Epoch: [65][300/391]\tTime 0.263 (0.279)\tLoss 0.0923 (0.1301)\tPrec@1 98.438 (95.743)\n",
            "Epoch: [65][350/391]\tTime 0.262 (0.277)\tLoss 0.1937 (0.1335)\tPrec@1 92.969 (95.660)\n",
            "epoch 65 training time consumed: 107.87s\n",
            "Test: [0/79]\tTime 4.022 (4.022)\tLoss 0.5837 (0.5837)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.6855 (0.5864)\tPrec@1 82.812 (85.401)\n",
            " * Prec@1 85.520\n",
            "current lr 2.59123e-02\n",
            "Epoch: [66][0/391]\tTime 5.503 (5.503)\tLoss 0.1123 (0.1123)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [66][50/391]\tTime 0.262 (0.365)\tLoss 0.1157 (0.1157)\tPrec@1 95.312 (96.170)\n",
            "Epoch: [66][100/391]\tTime 0.263 (0.314)\tLoss 0.0668 (0.1155)\tPrec@1 96.875 (96.179)\n",
            "Epoch: [66][150/391]\tTime 0.263 (0.297)\tLoss 0.1076 (0.1157)\tPrec@1 96.875 (96.130)\n",
            "Epoch: [66][200/391]\tTime 0.261 (0.289)\tLoss 0.1702 (0.1215)\tPrec@1 94.531 (95.896)\n",
            "Epoch: [66][250/391]\tTime 0.263 (0.283)\tLoss 0.0689 (0.1204)\tPrec@1 97.656 (96.007)\n",
            "Epoch: [66][300/391]\tTime 0.261 (0.280)\tLoss 0.3504 (0.1245)\tPrec@1 86.719 (95.858)\n",
            "Epoch: [66][350/391]\tTime 0.261 (0.277)\tLoss 0.1396 (0.1251)\tPrec@1 96.875 (95.862)\n",
            "epoch 66 training time consumed: 107.86s\n",
            "Test: [0/79]\tTime 4.030 (4.030)\tLoss 0.3140 (0.3140)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.5014 (0.4655)\tPrec@1 89.844 (88.021)\n",
            " * Prec@1 88.050\n",
            "current lr 2.45479e-02\n",
            "Epoch: [67][0/391]\tTime 5.348 (5.348)\tLoss 0.0930 (0.0930)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [67][50/391]\tTime 0.261 (0.361)\tLoss 0.0515 (0.1046)\tPrec@1 99.219 (96.584)\n",
            "Epoch: [67][100/391]\tTime 0.264 (0.312)\tLoss 0.0697 (0.1046)\tPrec@1 97.656 (96.411)\n",
            "Epoch: [67][150/391]\tTime 0.261 (0.295)\tLoss 0.1883 (0.1122)\tPrec@1 92.188 (96.239)\n",
            "Epoch: [67][200/391]\tTime 0.262 (0.287)\tLoss 0.1101 (0.1154)\tPrec@1 95.312 (96.187)\n",
            "Epoch: [67][250/391]\tTime 0.262 (0.282)\tLoss 0.0932 (0.1182)\tPrec@1 96.875 (96.131)\n",
            "Epoch: [67][300/391]\tTime 0.262 (0.279)\tLoss 0.0967 (0.1174)\tPrec@1 96.875 (96.125)\n",
            "Epoch: [67][350/391]\tTime 0.262 (0.277)\tLoss 0.1159 (0.1174)\tPrec@1 96.094 (96.149)\n",
            "epoch 67 training time consumed: 107.79s\n",
            "Test: [0/79]\tTime 4.126 (4.126)\tLoss 0.2959 (0.2959)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.086 (0.166)\tLoss 0.3934 (0.3577)\tPrec@1 92.188 (90.395)\n",
            " * Prec@1 90.160\n",
            "current lr 2.32087e-02\n",
            "Epoch: [68][0/391]\tTime 5.504 (5.504)\tLoss 0.1092 (0.1092)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [68][50/391]\tTime 0.263 (0.365)\tLoss 0.1028 (0.0997)\tPrec@1 96.875 (96.952)\n",
            "Epoch: [68][100/391]\tTime 0.262 (0.314)\tLoss 0.0829 (0.1055)\tPrec@1 97.656 (96.713)\n",
            "Epoch: [68][150/391]\tTime 0.263 (0.297)\tLoss 0.0825 (0.1052)\tPrec@1 96.875 (96.735)\n",
            "Epoch: [68][200/391]\tTime 0.263 (0.289)\tLoss 0.0433 (0.1049)\tPrec@1 98.438 (96.692)\n",
            "Epoch: [68][250/391]\tTime 0.263 (0.283)\tLoss 0.0885 (0.1083)\tPrec@1 96.875 (96.558)\n",
            "Epoch: [68][300/391]\tTime 0.264 (0.280)\tLoss 0.1078 (0.1089)\tPrec@1 96.875 (96.532)\n",
            "Epoch: [68][350/391]\tTime 0.264 (0.278)\tLoss 0.0991 (0.1106)\tPrec@1 96.875 (96.443)\n",
            "epoch 68 training time consumed: 108.05s\n",
            "Test: [0/79]\tTime 4.027 (4.027)\tLoss 0.2174 (0.2174)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.3352 (0.3234)\tPrec@1 89.062 (91.023)\n",
            " * Prec@1 91.160\n",
            "current lr 2.18958e-02\n",
            "Epoch: [69][0/391]\tTime 5.539 (5.539)\tLoss 0.0509 (0.0509)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [69][50/391]\tTime 0.265 (0.365)\tLoss 0.1373 (0.0869)\tPrec@1 96.875 (97.044)\n",
            "Epoch: [69][100/391]\tTime 0.263 (0.315)\tLoss 0.0956 (0.0882)\tPrec@1 96.875 (97.107)\n",
            "Epoch: [69][150/391]\tTime 0.263 (0.297)\tLoss 0.1055 (0.0915)\tPrec@1 96.875 (96.978)\n",
            "Epoch: [69][200/391]\tTime 0.263 (0.289)\tLoss 0.2227 (0.0944)\tPrec@1 93.750 (96.937)\n",
            "Epoch: [69][250/391]\tTime 0.264 (0.284)\tLoss 0.1153 (0.0970)\tPrec@1 94.531 (96.866)\n",
            "Epoch: [69][300/391]\tTime 0.263 (0.280)\tLoss 0.1350 (0.0988)\tPrec@1 96.875 (96.828)\n",
            "Epoch: [69][350/391]\tTime 0.262 (0.278)\tLoss 0.0949 (0.1000)\tPrec@1 96.094 (96.790)\n",
            "epoch 69 training time consumed: 108.13s\n",
            "Test: [0/79]\tTime 3.988 (3.988)\tLoss 0.2386 (0.2386)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.088 (0.163)\tLoss 0.3589 (0.3499)\tPrec@1 92.969 (91.054)\n",
            " * Prec@1 90.850\n",
            "current lr 2.06107e-02\n",
            "Epoch: [70][0/391]\tTime 5.466 (5.466)\tLoss 0.1078 (0.1078)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [70][50/391]\tTime 0.263 (0.365)\tLoss 0.0893 (0.0846)\tPrec@1 97.656 (97.335)\n",
            "Epoch: [70][100/391]\tTime 0.263 (0.314)\tLoss 0.0961 (0.0824)\tPrec@1 96.875 (97.362)\n",
            "Epoch: [70][150/391]\tTime 0.263 (0.297)\tLoss 0.1159 (0.0918)\tPrec@1 96.094 (97.041)\n",
            "Epoch: [70][200/391]\tTime 0.263 (0.289)\tLoss 0.1349 (0.0924)\tPrec@1 96.094 (97.046)\n",
            "Epoch: [70][250/391]\tTime 0.264 (0.284)\tLoss 0.1336 (0.0927)\tPrec@1 96.094 (96.993)\n",
            "Epoch: [70][300/391]\tTime 0.262 (0.280)\tLoss 0.1757 (0.0940)\tPrec@1 96.094 (96.955)\n",
            "Epoch: [70][350/391]\tTime 0.263 (0.278)\tLoss 0.0940 (0.0935)\tPrec@1 96.094 (96.975)\n",
            "epoch 70 training time consumed: 108.16s\n",
            "Test: [0/79]\tTime 4.071 (4.071)\tLoss 0.4438 (0.4438)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.088 (0.165)\tLoss 0.6125 (0.4559)\tPrec@1 84.375 (88.588)\n",
            " * Prec@1 88.690\n",
            "current lr 1.93546e-02\n",
            "Epoch: [71][0/391]\tTime 5.346 (5.346)\tLoss 0.0627 (0.0627)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [71][50/391]\tTime 0.262 (0.362)\tLoss 0.2171 (0.0889)\tPrec@1 94.531 (97.135)\n",
            "Epoch: [71][100/391]\tTime 0.263 (0.313)\tLoss 0.0766 (0.0832)\tPrec@1 96.875 (97.331)\n",
            "Epoch: [71][150/391]\tTime 0.262 (0.296)\tLoss 0.0390 (0.0805)\tPrec@1 99.219 (97.449)\n",
            "Epoch: [71][200/391]\tTime 0.263 (0.288)\tLoss 0.0373 (0.0801)\tPrec@1 99.219 (97.466)\n",
            "Epoch: [71][250/391]\tTime 0.263 (0.283)\tLoss 0.0633 (0.0816)\tPrec@1 97.656 (97.398)\n",
            "Epoch: [71][300/391]\tTime 0.262 (0.280)\tLoss 0.0770 (0.0829)\tPrec@1 96.875 (97.347)\n",
            "Epoch: [71][350/391]\tTime 0.264 (0.277)\tLoss 0.0695 (0.0846)\tPrec@1 98.438 (97.273)\n",
            "epoch 71 training time consumed: 107.90s\n",
            "Test: [0/79]\tTime 4.033 (4.033)\tLoss 0.2867 (0.2867)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.086 (0.164)\tLoss 0.4992 (0.3903)\tPrec@1 84.375 (89.890)\n",
            " * Prec@1 90.080\n",
            "current lr 1.81288e-02\n",
            "Epoch: [72][0/391]\tTime 5.505 (5.505)\tLoss 0.0929 (0.0929)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [72][50/391]\tTime 0.262 (0.365)\tLoss 0.0080 (0.0739)\tPrec@1 100.000 (97.733)\n",
            "Epoch: [72][100/391]\tTime 0.262 (0.314)\tLoss 0.0617 (0.0722)\tPrec@1 98.438 (97.749)\n",
            "Epoch: [72][150/391]\tTime 0.263 (0.297)\tLoss 0.0938 (0.0723)\tPrec@1 97.656 (97.698)\n",
            "Epoch: [72][200/391]\tTime 0.263 (0.289)\tLoss 0.0279 (0.0743)\tPrec@1 99.219 (97.672)\n",
            "Epoch: [72][250/391]\tTime 0.263 (0.284)\tLoss 0.0564 (0.0771)\tPrec@1 97.656 (97.566)\n",
            "Epoch: [72][300/391]\tTime 0.264 (0.280)\tLoss 0.0792 (0.0755)\tPrec@1 96.094 (97.589)\n",
            "Epoch: [72][350/391]\tTime 0.263 (0.278)\tLoss 0.0565 (0.0776)\tPrec@1 98.438 (97.561)\n",
            "epoch 72 training time consumed: 108.21s\n",
            "Test: [0/79]\tTime 4.054 (4.054)\tLoss 0.1605 (0.1605)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.3887 (0.3343)\tPrec@1 88.281 (90.993)\n",
            " * Prec@1 90.870\n",
            "current lr 1.69344e-02\n",
            "Epoch: [73][0/391]\tTime 5.486 (5.486)\tLoss 0.1034 (0.1034)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [73][50/391]\tTime 0.264 (0.365)\tLoss 0.1083 (0.0752)\tPrec@1 96.875 (97.779)\n",
            "Epoch: [73][100/391]\tTime 0.263 (0.315)\tLoss 0.0755 (0.0707)\tPrec@1 97.656 (97.881)\n",
            "Epoch: [73][150/391]\tTime 0.263 (0.298)\tLoss 0.0863 (0.0680)\tPrec@1 97.656 (97.946)\n",
            "Epoch: [73][200/391]\tTime 0.263 (0.289)\tLoss 0.1338 (0.0684)\tPrec@1 96.875 (97.913)\n",
            "Epoch: [73][250/391]\tTime 0.264 (0.284)\tLoss 0.0306 (0.0697)\tPrec@1 99.219 (97.840)\n",
            "Epoch: [73][300/391]\tTime 0.263 (0.280)\tLoss 0.0788 (0.0705)\tPrec@1 97.656 (97.786)\n",
            "Epoch: [73][350/391]\tTime 0.263 (0.278)\tLoss 0.0352 (0.0720)\tPrec@1 98.438 (97.736)\n",
            "epoch 73 training time consumed: 108.18s\n",
            "Test: [0/79]\tTime 4.126 (4.126)\tLoss 0.4298 (0.4298)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.3106 (0.3142)\tPrec@1 90.625 (91.697)\n",
            " * Prec@1 91.820\n",
            "current lr 1.57726e-02\n",
            "Epoch: [74][0/391]\tTime 5.470 (5.470)\tLoss 0.1237 (0.1237)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [74][50/391]\tTime 0.263 (0.364)\tLoss 0.0535 (0.0606)\tPrec@1 98.438 (98.085)\n",
            "Epoch: [74][100/391]\tTime 0.261 (0.314)\tLoss 0.0676 (0.0544)\tPrec@1 99.219 (98.306)\n",
            "Epoch: [74][150/391]\tTime 0.262 (0.297)\tLoss 0.0758 (0.0575)\tPrec@1 97.656 (98.184)\n",
            "Epoch: [74][200/391]\tTime 0.262 (0.288)\tLoss 0.1076 (0.0612)\tPrec@1 96.875 (98.060)\n",
            "Epoch: [74][250/391]\tTime 0.279 (0.283)\tLoss 0.1235 (0.0632)\tPrec@1 96.875 (97.999)\n",
            "Epoch: [74][300/391]\tTime 0.263 (0.280)\tLoss 0.0440 (0.0626)\tPrec@1 98.438 (98.001)\n",
            "Epoch: [74][350/391]\tTime 0.263 (0.278)\tLoss 0.0280 (0.0618)\tPrec@1 99.219 (98.039)\n",
            "epoch 74 training time consumed: 108.02s\n",
            "Test: [0/79]\tTime 4.039 (4.039)\tLoss 0.2948 (0.2948)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.4232 (0.3427)\tPrec@1 90.625 (91.774)\n",
            " * Prec@1 91.500\n",
            "current lr 1.46447e-02\n",
            "Epoch: [75][0/391]\tTime 5.306 (5.306)\tLoss 0.0218 (0.0218)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [75][50/391]\tTime 0.263 (0.361)\tLoss 0.0410 (0.0488)\tPrec@1 99.219 (98.376)\n",
            "Epoch: [75][100/391]\tTime 0.263 (0.312)\tLoss 0.0704 (0.0541)\tPrec@1 98.438 (98.205)\n",
            "Epoch: [75][150/391]\tTime 0.262 (0.296)\tLoss 0.0674 (0.0562)\tPrec@1 98.438 (98.184)\n",
            "Epoch: [75][200/391]\tTime 0.262 (0.288)\tLoss 0.0819 (0.0567)\tPrec@1 96.875 (98.177)\n",
            "Epoch: [75][250/391]\tTime 0.262 (0.283)\tLoss 0.0827 (0.0560)\tPrec@1 98.438 (98.201)\n",
            "Epoch: [75][300/391]\tTime 0.264 (0.280)\tLoss 0.0280 (0.0563)\tPrec@1 99.219 (98.186)\n",
            "Epoch: [75][350/391]\tTime 0.263 (0.277)\tLoss 0.0882 (0.0578)\tPrec@1 96.875 (98.144)\n",
            "epoch 75 training time consumed: 107.90s\n",
            "Test: [0/79]\tTime 3.974 (3.974)\tLoss 0.3169 (0.3169)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.3804 (0.3439)\tPrec@1 88.281 (91.743)\n",
            " * Prec@1 91.720\n",
            "current lr 1.35516e-02\n",
            "Epoch: [76][0/391]\tTime 5.324 (5.324)\tLoss 0.0587 (0.0587)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [76][50/391]\tTime 0.263 (0.362)\tLoss 0.0903 (0.0514)\tPrec@1 98.438 (98.407)\n",
            "Epoch: [76][100/391]\tTime 0.263 (0.313)\tLoss 0.0318 (0.0489)\tPrec@1 97.656 (98.523)\n",
            "Epoch: [76][150/391]\tTime 0.263 (0.296)\tLoss 0.0714 (0.0489)\tPrec@1 97.656 (98.463)\n",
            "Epoch: [76][200/391]\tTime 0.264 (0.288)\tLoss 0.0144 (0.0472)\tPrec@1 100.000 (98.484)\n",
            "Epoch: [76][250/391]\tTime 0.264 (0.283)\tLoss 0.1114 (0.0480)\tPrec@1 96.875 (98.434)\n",
            "Epoch: [76][300/391]\tTime 0.276 (0.280)\tLoss 0.0451 (0.0489)\tPrec@1 98.438 (98.409)\n",
            "Epoch: [76][350/391]\tTime 0.249 (0.277)\tLoss 0.0395 (0.0495)\tPrec@1 98.438 (98.411)\n",
            "epoch 76 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.063 (4.063)\tLoss 0.0482 (0.0482)\tPrec@1 98.438 (98.438)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.4068 (0.2958)\tPrec@1 89.844 (92.708)\n",
            " * Prec@1 92.650\n",
            "current lr 1.24944e-02\n",
            "Epoch: [77][0/391]\tTime 5.538 (5.538)\tLoss 0.0444 (0.0444)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [77][50/391]\tTime 0.263 (0.366)\tLoss 0.0326 (0.0497)\tPrec@1 99.219 (98.422)\n",
            "Epoch: [77][100/391]\tTime 0.262 (0.315)\tLoss 0.0359 (0.0406)\tPrec@1 98.438 (98.762)\n",
            "Epoch: [77][150/391]\tTime 0.263 (0.298)\tLoss 0.0316 (0.0382)\tPrec@1 99.219 (98.862)\n",
            "Epoch: [77][200/391]\tTime 0.262 (0.289)\tLoss 0.0458 (0.0400)\tPrec@1 99.219 (98.799)\n",
            "Epoch: [77][250/391]\tTime 0.264 (0.284)\tLoss 0.0269 (0.0387)\tPrec@1 99.219 (98.842)\n",
            "Epoch: [77][300/391]\tTime 0.263 (0.280)\tLoss 0.0471 (0.0392)\tPrec@1 98.438 (98.829)\n",
            "Epoch: [77][350/391]\tTime 0.262 (0.278)\tLoss 0.1572 (0.0397)\tPrec@1 96.875 (98.823)\n",
            "epoch 77 training time consumed: 108.08s\n",
            "Test: [0/79]\tTime 4.037 (4.037)\tLoss 0.3188 (0.3188)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.3440 (0.3795)\tPrec@1 90.625 (91.682)\n",
            " * Prec@1 91.750\n",
            "current lr 1.14743e-02\n",
            "Epoch: [78][0/391]\tTime 5.518 (5.518)\tLoss 0.0208 (0.0208)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [78][50/391]\tTime 0.262 (0.365)\tLoss 0.0880 (0.0466)\tPrec@1 96.875 (98.468)\n",
            "Epoch: [78][100/391]\tTime 0.264 (0.314)\tLoss 0.0395 (0.0427)\tPrec@1 98.438 (98.600)\n",
            "Epoch: [78][150/391]\tTime 0.263 (0.297)\tLoss 0.0371 (0.0411)\tPrec@1 99.219 (98.624)\n",
            "Epoch: [78][200/391]\tTime 0.263 (0.289)\tLoss 0.0119 (0.0425)\tPrec@1 100.000 (98.589)\n",
            "Epoch: [78][250/391]\tTime 0.263 (0.284)\tLoss 0.0079 (0.0414)\tPrec@1 100.000 (98.640)\n",
            "Epoch: [78][300/391]\tTime 0.263 (0.280)\tLoss 0.0418 (0.0412)\tPrec@1 98.438 (98.669)\n",
            "Epoch: [78][350/391]\tTime 0.264 (0.278)\tLoss 0.0337 (0.0402)\tPrec@1 99.219 (98.700)\n",
            "epoch 78 training time consumed: 108.13s\n",
            "Test: [0/79]\tTime 4.099 (4.099)\tLoss 0.2710 (0.2710)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.3536 (0.3379)\tPrec@1 89.062 (92.172)\n",
            " * Prec@1 92.260\n",
            "current lr 1.04922e-02\n",
            "Epoch: [79][0/391]\tTime 5.350 (5.350)\tLoss 0.0243 (0.0243)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [79][50/391]\tTime 0.263 (0.362)\tLoss 0.0496 (0.0366)\tPrec@1 97.656 (98.775)\n",
            "Epoch: [79][100/391]\tTime 0.264 (0.313)\tLoss 0.0354 (0.0403)\tPrec@1 99.219 (98.700)\n",
            "Epoch: [79][150/391]\tTime 0.264 (0.297)\tLoss 0.0294 (0.0370)\tPrec@1 99.219 (98.831)\n",
            "Epoch: [79][200/391]\tTime 0.263 (0.288)\tLoss 0.0160 (0.0351)\tPrec@1 100.000 (98.884)\n",
            "Epoch: [79][250/391]\tTime 0.263 (0.283)\tLoss 0.0163 (0.0334)\tPrec@1 99.219 (98.957)\n",
            "Epoch: [79][300/391]\tTime 0.263 (0.280)\tLoss 0.0288 (0.0332)\tPrec@1 98.438 (98.949)\n",
            "Epoch: [79][350/391]\tTime 0.263 (0.278)\tLoss 0.0309 (0.0330)\tPrec@1 98.438 (98.941)\n",
            "epoch 79 training time consumed: 108.11s\n",
            "Test: [0/79]\tTime 4.018 (4.018)\tLoss 0.2125 (0.2125)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.2582 (0.2906)\tPrec@1 92.969 (92.770)\n",
            " * Prec@1 93.010\n",
            "current lr 9.54915e-03\n",
            "Epoch: [80][0/391]\tTime 5.483 (5.483)\tLoss 0.0120 (0.0120)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [80][50/391]\tTime 0.262 (0.365)\tLoss 0.0098 (0.0290)\tPrec@1 100.000 (99.127)\n",
            "Epoch: [80][100/391]\tTime 0.263 (0.314)\tLoss 0.0057 (0.0274)\tPrec@1 100.000 (99.165)\n",
            "Epoch: [80][150/391]\tTime 0.263 (0.297)\tLoss 0.0365 (0.0258)\tPrec@1 98.438 (99.270)\n",
            "Epoch: [80][200/391]\tTime 0.263 (0.289)\tLoss 0.0499 (0.0252)\tPrec@1 98.438 (99.254)\n",
            "Epoch: [80][250/391]\tTime 0.263 (0.284)\tLoss 0.0059 (0.0246)\tPrec@1 100.000 (99.269)\n",
            "Epoch: [80][300/391]\tTime 0.263 (0.280)\tLoss 0.0139 (0.0240)\tPrec@1 100.000 (99.265)\n",
            "Epoch: [80][350/391]\tTime 0.264 (0.278)\tLoss 0.0233 (0.0242)\tPrec@1 99.219 (99.254)\n",
            "epoch 80 training time consumed: 108.07s\n",
            "Test: [0/79]\tTime 4.012 (4.012)\tLoss 0.1876 (0.1876)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.088 (0.164)\tLoss 0.3560 (0.3427)\tPrec@1 92.188 (92.770)\n",
            " * Prec@1 92.960\n",
            "current lr 8.64597e-03\n",
            "Epoch: [81][0/391]\tTime 5.490 (5.490)\tLoss 0.0053 (0.0053)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [81][50/391]\tTime 0.262 (0.364)\tLoss 0.0187 (0.0209)\tPrec@1 99.219 (99.326)\n",
            "Epoch: [81][100/391]\tTime 0.263 (0.314)\tLoss 0.0284 (0.0211)\tPrec@1 99.219 (99.327)\n",
            "Epoch: [81][150/391]\tTime 0.264 (0.297)\tLoss 0.0075 (0.0222)\tPrec@1 100.000 (99.307)\n",
            "Epoch: [81][200/391]\tTime 0.263 (0.289)\tLoss 0.0067 (0.0228)\tPrec@1 100.000 (99.273)\n",
            "Epoch: [81][250/391]\tTime 0.264 (0.283)\tLoss 0.0097 (0.0219)\tPrec@1 100.000 (99.303)\n",
            "Epoch: [81][300/391]\tTime 0.263 (0.280)\tLoss 0.0048 (0.0223)\tPrec@1 100.000 (99.286)\n",
            "Epoch: [81][350/391]\tTime 0.262 (0.278)\tLoss 0.0187 (0.0230)\tPrec@1 99.219 (99.283)\n",
            "epoch 81 training time consumed: 108.07s\n",
            "Test: [0/79]\tTime 4.045 (4.045)\tLoss 0.1520 (0.1520)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.086 (0.165)\tLoss 0.3823 (0.3078)\tPrec@1 89.062 (93.643)\n",
            " * Prec@1 93.710\n",
            "current lr 7.78360e-03\n",
            "Epoch: [82][0/391]\tTime 5.487 (5.487)\tLoss 0.0033 (0.0033)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [82][50/391]\tTime 0.262 (0.365)\tLoss 0.0547 (0.0191)\tPrec@1 99.219 (99.403)\n",
            "Epoch: [82][100/391]\tTime 0.264 (0.314)\tLoss 0.0374 (0.0167)\tPrec@1 99.219 (99.482)\n",
            "Epoch: [82][150/391]\tTime 0.263 (0.297)\tLoss 0.0155 (0.0170)\tPrec@1 99.219 (99.441)\n",
            "Epoch: [82][200/391]\tTime 0.263 (0.289)\tLoss 0.0031 (0.0155)\tPrec@1 100.000 (99.499)\n",
            "Epoch: [82][250/391]\tTime 0.264 (0.284)\tLoss 0.0325 (0.0145)\tPrec@1 98.438 (99.530)\n",
            "Epoch: [82][300/391]\tTime 0.264 (0.280)\tLoss 0.0131 (0.0148)\tPrec@1 99.219 (99.528)\n",
            "Epoch: [82][350/391]\tTime 0.263 (0.278)\tLoss 0.0108 (0.0153)\tPrec@1 99.219 (99.504)\n",
            "epoch 82 training time consumed: 108.20s\n",
            "Test: [0/79]\tTime 4.039 (4.039)\tLoss 0.2036 (0.2036)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.088 (0.164)\tLoss 0.3427 (0.3036)\tPrec@1 94.531 (93.811)\n",
            " * Prec@1 93.640\n",
            "current lr 6.96290e-03\n",
            "Epoch: [83][0/391]\tTime 5.496 (5.496)\tLoss 0.0018 (0.0018)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [83][50/391]\tTime 0.262 (0.365)\tLoss 0.0058 (0.0146)\tPrec@1 100.000 (99.586)\n",
            "Epoch: [83][100/391]\tTime 0.263 (0.314)\tLoss 0.0214 (0.0142)\tPrec@1 99.219 (99.598)\n",
            "Epoch: [83][150/391]\tTime 0.264 (0.297)\tLoss 0.0265 (0.0155)\tPrec@1 98.438 (99.550)\n",
            "Epoch: [83][200/391]\tTime 0.264 (0.289)\tLoss 0.0156 (0.0161)\tPrec@1 99.219 (99.483)\n",
            "Epoch: [83][250/391]\tTime 0.264 (0.284)\tLoss 0.0069 (0.0150)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [83][300/391]\tTime 0.264 (0.280)\tLoss 0.0033 (0.0151)\tPrec@1 100.000 (99.517)\n",
            "Epoch: [83][350/391]\tTime 0.263 (0.278)\tLoss 0.0011 (0.0145)\tPrec@1 100.000 (99.541)\n",
            "epoch 83 training time consumed: 108.14s\n",
            "Test: [0/79]\tTime 4.178 (4.178)\tLoss 0.1227 (0.1227)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.167)\tLoss 0.2928 (0.2974)\tPrec@1 92.969 (93.919)\n",
            " * Prec@1 94.020\n",
            "current lr 6.18467e-03\n",
            "Epoch: [84][0/391]\tTime 5.480 (5.480)\tLoss 0.0032 (0.0032)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [84][50/391]\tTime 0.261 (0.364)\tLoss 0.0018 (0.0105)\tPrec@1 100.000 (99.709)\n",
            "Epoch: [84][100/391]\tTime 0.263 (0.314)\tLoss 0.0484 (0.0115)\tPrec@1 99.219 (99.667)\n",
            "Epoch: [84][150/391]\tTime 0.263 (0.297)\tLoss 0.0040 (0.0120)\tPrec@1 100.000 (99.653)\n",
            "Epoch: [84][200/391]\tTime 0.263 (0.289)\tLoss 0.0024 (0.0108)\tPrec@1 100.000 (99.685)\n",
            "Epoch: [84][250/391]\tTime 0.264 (0.284)\tLoss 0.0125 (0.0101)\tPrec@1 99.219 (99.698)\n",
            "Epoch: [84][300/391]\tTime 0.263 (0.280)\tLoss 0.0021 (0.0096)\tPrec@1 100.000 (99.717)\n",
            "Epoch: [84][350/391]\tTime 0.264 (0.278)\tLoss 0.0054 (0.0094)\tPrec@1 100.000 (99.717)\n",
            "epoch 84 training time consumed: 108.12s\n",
            "Test: [0/79]\tTime 4.076 (4.076)\tLoss 0.1397 (0.1397)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.086 (0.165)\tLoss 0.3220 (0.3106)\tPrec@1 92.969 (94.056)\n",
            " * Prec@1 94.080\n",
            "current lr 5.44967e-03\n",
            "Epoch: [85][0/391]\tTime 5.372 (5.372)\tLoss 0.0026 (0.0026)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [85][50/391]\tTime 0.263 (0.363)\tLoss 0.0054 (0.0075)\tPrec@1 100.000 (99.786)\n",
            "Epoch: [85][100/391]\tTime 0.264 (0.313)\tLoss 0.0018 (0.0086)\tPrec@1 100.000 (99.729)\n",
            "Epoch: [85][150/391]\tTime 0.262 (0.297)\tLoss 0.0021 (0.0089)\tPrec@1 100.000 (99.731)\n",
            "Epoch: [85][200/391]\tTime 0.263 (0.289)\tLoss 0.0133 (0.0093)\tPrec@1 99.219 (99.708)\n",
            "Epoch: [85][250/391]\tTime 0.263 (0.284)\tLoss 0.0023 (0.0090)\tPrec@1 100.000 (99.711)\n",
            "Epoch: [85][300/391]\tTime 0.265 (0.280)\tLoss 0.0226 (0.0087)\tPrec@1 99.219 (99.720)\n",
            "Epoch: [85][350/391]\tTime 0.264 (0.278)\tLoss 0.0007 (0.0085)\tPrec@1 100.000 (99.728)\n",
            "epoch 85 training time consumed: 108.17s\n",
            "Test: [0/79]\tTime 4.150 (4.150)\tLoss 0.0871 (0.0871)\tPrec@1 98.438 (98.438)\n",
            "Test: [50/79]\tTime 0.087 (0.167)\tLoss 0.1802 (0.2804)\tPrec@1 94.531 (94.638)\n",
            " * Prec@1 94.590\n",
            "current lr 4.75865e-03\n",
            "Epoch: [86][0/391]\tTime 5.486 (5.486)\tLoss 0.0005 (0.0005)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [86][50/391]\tTime 0.264 (0.365)\tLoss 0.0034 (0.0039)\tPrec@1 100.000 (99.923)\n",
            "Epoch: [86][100/391]\tTime 0.263 (0.314)\tLoss 0.0214 (0.0052)\tPrec@1 99.219 (99.876)\n",
            "Epoch: [86][150/391]\tTime 0.263 (0.297)\tLoss 0.0011 (0.0051)\tPrec@1 100.000 (99.865)\n",
            "Epoch: [86][200/391]\tTime 0.263 (0.289)\tLoss 0.0012 (0.0051)\tPrec@1 100.000 (99.876)\n",
            "Epoch: [86][250/391]\tTime 0.262 (0.284)\tLoss 0.0018 (0.0051)\tPrec@1 100.000 (99.863)\n",
            "Epoch: [86][300/391]\tTime 0.265 (0.280)\tLoss 0.0030 (0.0055)\tPrec@1 100.000 (99.849)\n",
            "Epoch: [86][350/391]\tTime 0.263 (0.278)\tLoss 0.0072 (0.0055)\tPrec@1 100.000 (99.844)\n",
            "epoch 86 training time consumed: 108.09s\n",
            "Test: [0/79]\tTime 4.205 (4.205)\tLoss 0.0738 (0.0738)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.168)\tLoss 0.2176 (0.2795)\tPrec@1 93.750 (94.577)\n",
            " * Prec@1 94.600\n",
            "current lr 4.11227e-03\n",
            "Epoch: [87][0/391]\tTime 5.479 (5.479)\tLoss 0.0035 (0.0035)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [87][50/391]\tTime 0.262 (0.364)\tLoss 0.0018 (0.0038)\tPrec@1 100.000 (99.893)\n",
            "Epoch: [87][100/391]\tTime 0.262 (0.314)\tLoss 0.0009 (0.0046)\tPrec@1 100.000 (99.869)\n",
            "Epoch: [87][150/391]\tTime 0.264 (0.297)\tLoss 0.0047 (0.0047)\tPrec@1 100.000 (99.871)\n",
            "Epoch: [87][200/391]\tTime 0.265 (0.289)\tLoss 0.0763 (0.0045)\tPrec@1 98.438 (99.887)\n",
            "Epoch: [87][250/391]\tTime 0.263 (0.283)\tLoss 0.0011 (0.0044)\tPrec@1 100.000 (99.897)\n",
            "Epoch: [87][300/391]\tTime 0.263 (0.280)\tLoss 0.0073 (0.0043)\tPrec@1 100.000 (99.901)\n",
            "Epoch: [87][350/391]\tTime 0.263 (0.278)\tLoss 0.0009 (0.0042)\tPrec@1 100.000 (99.898)\n",
            "epoch 87 training time consumed: 108.07s\n",
            "Test: [0/79]\tTime 4.006 (4.006)\tLoss 0.1679 (0.1679)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.088 (0.164)\tLoss 0.1888 (0.2838)\tPrec@1 93.750 (94.424)\n",
            " * Prec@1 94.610\n",
            "current lr 3.51118e-03\n",
            "Epoch: [88][0/391]\tTime 5.485 (5.485)\tLoss 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [88][50/391]\tTime 0.262 (0.365)\tLoss 0.0006 (0.0030)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [88][100/391]\tTime 0.262 (0.314)\tLoss 0.0057 (0.0034)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [88][150/391]\tTime 0.264 (0.297)\tLoss 0.0160 (0.0040)\tPrec@1 99.219 (99.912)\n",
            "Epoch: [88][200/391]\tTime 0.263 (0.289)\tLoss 0.0005 (0.0038)\tPrec@1 100.000 (99.914)\n",
            "Epoch: [88][250/391]\tTime 0.263 (0.284)\tLoss 0.0005 (0.0036)\tPrec@1 100.000 (99.922)\n",
            "Epoch: [88][300/391]\tTime 0.263 (0.280)\tLoss 0.0012 (0.0036)\tPrec@1 100.000 (99.917)\n",
            "Epoch: [88][350/391]\tTime 0.264 (0.278)\tLoss 0.0005 (0.0035)\tPrec@1 100.000 (99.913)\n",
            "epoch 88 training time consumed: 108.17s\n",
            "Test: [0/79]\tTime 4.129 (4.129)\tLoss 0.1050 (0.1050)\tPrec@1 97.656 (97.656)\n",
            "Test: [50/79]\tTime 0.086 (0.166)\tLoss 0.2580 (0.2743)\tPrec@1 93.750 (94.868)\n",
            " * Prec@1 94.960\n",
            "current lr 2.95596e-03\n",
            "Epoch: [89][0/391]\tTime 5.464 (5.464)\tLoss 0.0207 (0.0207)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [89][50/391]\tTime 0.262 (0.364)\tLoss 0.0182 (0.0040)\tPrec@1 99.219 (99.893)\n",
            "Epoch: [89][100/391]\tTime 0.263 (0.314)\tLoss 0.0556 (0.0036)\tPrec@1 99.219 (99.915)\n",
            "Epoch: [89][150/391]\tTime 0.263 (0.297)\tLoss 0.0043 (0.0037)\tPrec@1 100.000 (99.907)\n",
            "Epoch: [89][200/391]\tTime 0.263 (0.289)\tLoss 0.0166 (0.0036)\tPrec@1 99.219 (99.918)\n",
            "Epoch: [89][250/391]\tTime 0.262 (0.284)\tLoss 0.0042 (0.0033)\tPrec@1 100.000 (99.932)\n",
            "Epoch: [89][300/391]\tTime 0.264 (0.280)\tLoss 0.0363 (0.0036)\tPrec@1 99.219 (99.922)\n",
            "Epoch: [89][350/391]\tTime 0.263 (0.278)\tLoss 0.0011 (0.0039)\tPrec@1 100.000 (99.911)\n",
            "epoch 89 training time consumed: 108.15s\n",
            "Test: [0/79]\tTime 4.070 (4.070)\tLoss 0.1459 (0.1459)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.088 (0.165)\tLoss 0.2125 (0.2703)\tPrec@1 93.750 (94.776)\n",
            " * Prec@1 94.890\n",
            "current lr 2.44717e-03\n",
            "Epoch: [90][0/391]\tTime 5.563 (5.563)\tLoss 0.0055 (0.0055)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [90][50/391]\tTime 0.262 (0.366)\tLoss 0.0025 (0.0030)\tPrec@1 100.000 (99.939)\n",
            "Epoch: [90][100/391]\tTime 0.262 (0.315)\tLoss 0.0006 (0.0027)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [90][150/391]\tTime 0.263 (0.297)\tLoss 0.0084 (0.0024)\tPrec@1 99.219 (99.943)\n",
            "Epoch: [90][200/391]\tTime 0.263 (0.289)\tLoss 0.0014 (0.0023)\tPrec@1 100.000 (99.949)\n",
            "Epoch: [90][250/391]\tTime 0.263 (0.284)\tLoss 0.0008 (0.0022)\tPrec@1 100.000 (99.956)\n",
            "Epoch: [90][300/391]\tTime 0.263 (0.280)\tLoss 0.0011 (0.0023)\tPrec@1 100.000 (99.951)\n",
            "Epoch: [90][350/391]\tTime 0.262 (0.278)\tLoss 0.0213 (0.0025)\tPrec@1 99.219 (99.942)\n",
            "epoch 90 training time consumed: 108.14s\n",
            "Test: [0/79]\tTime 5.199 (5.199)\tLoss 0.0897 (0.0897)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.187)\tLoss 0.2000 (0.2739)\tPrec@1 94.531 (94.838)\n",
            " * Prec@1 94.840\n",
            "current lr 1.98532e-03\n",
            "Epoch: [91][0/391]\tTime 5.475 (5.475)\tLoss 0.0134 (0.0134)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [91][50/391]\tTime 0.263 (0.365)\tLoss 0.0033 (0.0028)\tPrec@1 100.000 (99.923)\n",
            "Epoch: [91][100/391]\tTime 0.262 (0.314)\tLoss 0.0004 (0.0027)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [91][150/391]\tTime 0.263 (0.297)\tLoss 0.0027 (0.0029)\tPrec@1 100.000 (99.933)\n",
            "Epoch: [91][200/391]\tTime 0.263 (0.289)\tLoss 0.0013 (0.0028)\tPrec@1 100.000 (99.942)\n",
            "Epoch: [91][250/391]\tTime 0.264 (0.284)\tLoss 0.0003 (0.0025)\tPrec@1 100.000 (99.953)\n",
            "Epoch: [91][300/391]\tTime 0.264 (0.281)\tLoss 0.0004 (0.0024)\tPrec@1 100.000 (99.951)\n",
            "Epoch: [91][350/391]\tTime 0.268 (0.278)\tLoss 0.0003 (0.0023)\tPrec@1 100.000 (99.955)\n",
            "epoch 91 training time consumed: 108.25s\n",
            "Test: [0/79]\tTime 3.998 (3.998)\tLoss 0.1043 (0.1043)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.1909 (0.2672)\tPrec@1 95.312 (94.991)\n",
            " * Prec@1 95.020\n",
            "current lr 1.57084e-03\n",
            "Epoch: [92][0/391]\tTime 5.349 (5.349)\tLoss 0.0009 (0.0009)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [92][50/391]\tTime 0.263 (0.362)\tLoss 0.0035 (0.0015)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [92][100/391]\tTime 0.263 (0.313)\tLoss 0.0003 (0.0017)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [92][150/391]\tTime 0.264 (0.296)\tLoss 0.0011 (0.0018)\tPrec@1 100.000 (99.974)\n",
            "Epoch: [92][200/391]\tTime 0.263 (0.288)\tLoss 0.0004 (0.0018)\tPrec@1 100.000 (99.973)\n",
            "Epoch: [92][250/391]\tTime 0.264 (0.283)\tLoss 0.0003 (0.0018)\tPrec@1 100.000 (99.972)\n",
            "Epoch: [92][300/391]\tTime 0.263 (0.280)\tLoss 0.0015 (0.0019)\tPrec@1 100.000 (99.971)\n",
            "Epoch: [92][350/391]\tTime 0.263 (0.277)\tLoss 0.0004 (0.0019)\tPrec@1 100.000 (99.973)\n",
            "epoch 92 training time consumed: 108.01s\n",
            "Test: [0/79]\tTime 4.114 (4.114)\tLoss 0.1060 (0.1060)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.1832 (0.2696)\tPrec@1 94.531 (95.037)\n",
            " * Prec@1 95.080\n",
            "current lr 1.20416e-03\n",
            "Epoch: [93][0/391]\tTime 5.477 (5.477)\tLoss 0.0004 (0.0004)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [93][50/391]\tTime 0.262 (0.364)\tLoss 0.0047 (0.0026)\tPrec@1 100.000 (99.908)\n",
            "Epoch: [93][100/391]\tTime 0.263 (0.314)\tLoss 0.0043 (0.0023)\tPrec@1 100.000 (99.923)\n",
            "Epoch: [93][150/391]\tTime 0.263 (0.297)\tLoss 0.0013 (0.0024)\tPrec@1 100.000 (99.928)\n",
            "Epoch: [93][200/391]\tTime 0.262 (0.289)\tLoss 0.0004 (0.0023)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [93][250/391]\tTime 0.264 (0.283)\tLoss 0.0003 (0.0023)\tPrec@1 100.000 (99.938)\n",
            "Epoch: [93][300/391]\tTime 0.264 (0.280)\tLoss 0.0044 (0.0022)\tPrec@1 100.000 (99.943)\n",
            "Epoch: [93][350/391]\tTime 0.263 (0.278)\tLoss 0.0004 (0.0023)\tPrec@1 100.000 (99.944)\n",
            "epoch 93 training time consumed: 108.12s\n",
            "Test: [0/79]\tTime 3.933 (3.933)\tLoss 0.0778 (0.0778)\tPrec@1 97.656 (97.656)\n",
            "Test: [50/79]\tTime 0.087 (0.162)\tLoss 0.2297 (0.2662)\tPrec@1 93.750 (95.067)\n",
            " * Prec@1 95.100\n",
            "current lr 8.85637e-04\n",
            "Epoch: [94][0/391]\tTime 5.495 (5.495)\tLoss 0.0013 (0.0013)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [94][50/391]\tTime 0.263 (0.365)\tLoss 0.0036 (0.0034)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [94][100/391]\tTime 0.263 (0.314)\tLoss 0.0003 (0.0023)\tPrec@1 100.000 (99.969)\n",
            "Epoch: [94][150/391]\tTime 0.262 (0.297)\tLoss 0.0016 (0.0021)\tPrec@1 100.000 (99.964)\n",
            "Epoch: [94][200/391]\tTime 0.264 (0.289)\tLoss 0.0011 (0.0020)\tPrec@1 100.000 (99.965)\n",
            "Epoch: [94][250/391]\tTime 0.264 (0.284)\tLoss 0.0012 (0.0020)\tPrec@1 100.000 (99.966)\n",
            "Epoch: [94][300/391]\tTime 0.264 (0.280)\tLoss 0.0008 (0.0019)\tPrec@1 100.000 (99.966)\n",
            "Epoch: [94][350/391]\tTime 0.263 (0.278)\tLoss 0.0017 (0.0019)\tPrec@1 100.000 (99.964)\n",
            "epoch 94 training time consumed: 108.19s\n",
            "Test: [0/79]\tTime 4.120 (4.120)\tLoss 0.0782 (0.0782)\tPrec@1 97.656 (97.656)\n",
            "Test: [50/79]\tTime 0.087 (0.166)\tLoss 0.2244 (0.2648)\tPrec@1 94.531 (95.067)\n",
            " * Prec@1 95.040\n",
            "current lr 6.15583e-04\n",
            "Epoch: [95][0/391]\tTime 5.473 (5.473)\tLoss 0.0004 (0.0004)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [95][50/391]\tTime 0.263 (0.365)\tLoss 0.0005 (0.0012)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [95][100/391]\tTime 0.264 (0.315)\tLoss 0.0004 (0.0014)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [95][150/391]\tTime 0.263 (0.298)\tLoss 0.0004 (0.0013)\tPrec@1 100.000 (99.990)\n",
            "Epoch: [95][200/391]\tTime 0.262 (0.289)\tLoss 0.0005 (0.0013)\tPrec@1 100.000 (99.992)\n",
            "Epoch: [95][250/391]\tTime 0.263 (0.284)\tLoss 0.0008 (0.0015)\tPrec@1 100.000 (99.981)\n",
            "Epoch: [95][300/391]\tTime 0.263 (0.281)\tLoss 0.0200 (0.0016)\tPrec@1 99.219 (99.979)\n",
            "Epoch: [95][350/391]\tTime 0.263 (0.278)\tLoss 0.0006 (0.0015)\tPrec@1 100.000 (99.978)\n",
            "epoch 95 training time consumed: 108.25s\n",
            "Test: [0/79]\tTime 4.033 (4.033)\tLoss 0.0688 (0.0688)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.2138 (0.2667)\tPrec@1 94.531 (94.975)\n",
            " * Prec@1 94.980\n",
            "current lr 3.94265e-04\n",
            "Epoch: [96][0/391]\tTime 5.504 (5.504)\tLoss 0.0013 (0.0013)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [96][50/391]\tTime 0.262 (0.365)\tLoss 0.0009 (0.0012)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [96][100/391]\tTime 0.262 (0.314)\tLoss 0.0019 (0.0014)\tPrec@1 100.000 (99.969)\n",
            "Epoch: [96][150/391]\tTime 0.263 (0.297)\tLoss 0.0009 (0.0013)\tPrec@1 100.000 (99.974)\n",
            "Epoch: [96][200/391]\tTime 0.263 (0.289)\tLoss 0.0014 (0.0015)\tPrec@1 100.000 (99.973)\n",
            "Epoch: [96][250/391]\tTime 0.264 (0.284)\tLoss 0.0011 (0.0015)\tPrec@1 100.000 (99.975)\n",
            "Epoch: [96][300/391]\tTime 0.262 (0.280)\tLoss 0.0007 (0.0015)\tPrec@1 100.000 (99.969)\n",
            "Epoch: [96][350/391]\tTime 0.264 (0.278)\tLoss 0.0394 (0.0017)\tPrec@1 99.219 (99.964)\n",
            "epoch 96 training time consumed: 108.19s\n",
            "Test: [0/79]\tTime 4.072 (4.072)\tLoss 0.0972 (0.0972)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.165)\tLoss 0.1747 (0.2649)\tPrec@1 95.312 (95.129)\n",
            " * Prec@1 95.110\n",
            "current lr 2.21902e-04\n",
            "Epoch: [97][0/391]\tTime 5.319 (5.319)\tLoss 0.0012 (0.0012)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [97][50/391]\tTime 0.263 (0.361)\tLoss 0.0005 (0.0011)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [97][100/391]\tTime 0.263 (0.313)\tLoss 0.0006 (0.0010)\tPrec@1 100.000 (99.992)\n",
            "Epoch: [97][150/391]\tTime 0.264 (0.296)\tLoss 0.0018 (0.0012)\tPrec@1 100.000 (99.984)\n",
            "Epoch: [97][200/391]\tTime 0.265 (0.288)\tLoss 0.0009 (0.0014)\tPrec@1 100.000 (99.977)\n",
            "Epoch: [97][250/391]\tTime 0.263 (0.283)\tLoss 0.0015 (0.0013)\tPrec@1 100.000 (99.978)\n",
            "Epoch: [97][300/391]\tTime 0.263 (0.280)\tLoss 0.0007 (0.0014)\tPrec@1 100.000 (99.977)\n",
            "Epoch: [97][350/391]\tTime 0.263 (0.278)\tLoss 0.0003 (0.0013)\tPrec@1 100.000 (99.980)\n",
            "epoch 97 training time consumed: 108.14s\n",
            "Test: [0/79]\tTime 3.981 (3.981)\tLoss 0.1010 (0.1010)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.163)\tLoss 0.1837 (0.2663)\tPrec@1 94.531 (94.991)\n",
            " * Prec@1 95.000\n",
            "current lr 9.86636e-05\n",
            "Epoch: [98][0/391]\tTime 5.487 (5.487)\tLoss 0.0006 (0.0006)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [98][50/391]\tTime 0.263 (0.365)\tLoss 0.0004 (0.0012)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [98][100/391]\tTime 0.263 (0.315)\tLoss 0.0004 (0.0013)\tPrec@1 100.000 (99.985)\n",
            "Epoch: [98][150/391]\tTime 0.264 (0.298)\tLoss 0.0006 (0.0012)\tPrec@1 100.000 (99.990)\n",
            "Epoch: [98][200/391]\tTime 0.263 (0.289)\tLoss 0.0030 (0.0013)\tPrec@1 100.000 (99.984)\n",
            "Epoch: [98][250/391]\tTime 0.263 (0.284)\tLoss 0.0034 (0.0012)\tPrec@1 100.000 (99.988)\n",
            "Epoch: [98][300/391]\tTime 0.264 (0.281)\tLoss 0.0003 (0.0013)\tPrec@1 100.000 (99.979)\n",
            "Epoch: [98][350/391]\tTime 0.264 (0.278)\tLoss 0.0012 (0.0013)\tPrec@1 100.000 (99.980)\n",
            "epoch 98 training time consumed: 108.43s\n",
            "Test: [0/79]\tTime 4.157 (4.157)\tLoss 0.1015 (0.1015)\tPrec@1 97.656 (97.656)\n",
            "Test: [50/79]\tTime 0.087 (0.167)\tLoss 0.1660 (0.2664)\tPrec@1 95.312 (95.129)\n",
            " * Prec@1 95.100\n",
            "current lr 2.46720e-05\n",
            "Epoch: [99][0/391]\tTime 5.561 (5.561)\tLoss 0.0015 (0.0015)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [99][50/391]\tTime 0.261 (0.368)\tLoss 0.0007 (0.0023)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [99][100/391]\tTime 0.266 (0.316)\tLoss 0.0140 (0.0022)\tPrec@1 100.000 (99.954)\n",
            "Epoch: [99][150/391]\tTime 0.263 (0.299)\tLoss 0.0003 (0.0018)\tPrec@1 100.000 (99.964)\n",
            "Epoch: [99][200/391]\tTime 0.263 (0.290)\tLoss 0.0003 (0.0017)\tPrec@1 100.000 (99.961)\n",
            "Epoch: [99][250/391]\tTime 0.271 (0.284)\tLoss 0.0002 (0.0017)\tPrec@1 100.000 (99.963)\n",
            "Epoch: [99][300/391]\tTime 0.264 (0.281)\tLoss 0.0006 (0.0017)\tPrec@1 100.000 (99.961)\n",
            "Epoch: [99][350/391]\tTime 0.262 (0.278)\tLoss 0.0004 (0.0017)\tPrec@1 100.000 (99.964)\n",
            "epoch 99 training time consumed: 108.26s\n",
            "Test: [0/79]\tTime 4.032 (4.032)\tLoss 0.0854 (0.0854)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.087 (0.164)\tLoss 0.1772 (0.2676)\tPrec@1 94.531 (95.037)\n",
            " * Prec@1 95.050\n"
          ]
        }
      ],
      "source": [
        "print(\"cuda:\", torch.cuda.is_available())\n",
        "best_prec1 = 0\n",
        "model = torch.nn.DataParallel(model)\n",
        "model.cuda()\n",
        "\n",
        "# CrossEntropyLoss with smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing).cuda()\n",
        "# Choose optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
        "                                  momentum=config.momentum,\n",
        "                                  weight_decay=config.weight_decay)\n",
        "# learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
        "\n",
        "for epoch in range(config.start_epoch, config.epochs):\n",
        "    # train for one epoch\n",
        "    # print current learning rate\n",
        "    print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "    # train\n",
        "    train(train_loader, model, criterion, optimizer, epoch, writer)\n",
        "    # learning rate decay\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on validation set\n",
        "    prec1 = validate(val_loader, model, criterion, epoch, writer)\n",
        "\n",
        "    # remember best acc and save checkpoint\n",
        "    is_best = prec1 > best_prec1\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), \"best.pth\")\n",
        "        best_prec1 = prec1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95.11\n"
          ]
        }
      ],
      "source": [
        "print(best_prec1)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/vgg_nomaxpool\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2337266e20af4655a6cb20e52efb4b07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b20b61f9e214f8b8951c16a5c7e1693": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49338d75a7094f61880c852c0a484a25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc7f3a8141140918fd8cb6999ecc95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a9320e65b7a4c649a47061f03413b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c08f673139c84571abb66ad4b6922449",
              "IPY_MODEL_c6790688d326402ba083d5c22ed8360a",
              "IPY_MODEL_e1126fe3e80745f88d650d7e5d47e26a"
            ],
            "layout": "IPY_MODEL_2b20b61f9e214f8b8951c16a5c7e1693"
          }
        },
        "8d0c08e4da9d425683bea661d71dc1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "977834f92b1642549d77a2148ceba40e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae60e27f93a64ed2892d2f064508f96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c08f673139c84571abb66ad4b6922449": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_977834f92b1642549d77a2148ceba40e",
            "placeholder": "​",
            "style": "IPY_MODEL_8d0c08e4da9d425683bea661d71dc1c7",
            "value": "100%"
          }
        },
        "c6790688d326402ba083d5c22ed8360a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2337266e20af4655a6cb20e52efb4b07",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae60e27f93a64ed2892d2f064508f96e",
            "value": 170498071
          }
        },
        "e1126fe3e80745f88d650d7e5d47e26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49338d75a7094f61880c852c0a484a25",
            "placeholder": "​",
            "style": "IPY_MODEL_5fc7f3a8141140918fd8cb6999ecc95a",
            "value": " 170498071/170498071 [00:05&lt;00:00, 32156324.89it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
